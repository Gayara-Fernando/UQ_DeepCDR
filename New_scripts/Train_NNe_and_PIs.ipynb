{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba46773-9b8f-48d8-9df7-eafaaf079b91",
   "metadata": {},
   "source": [
    "Here we will construct the CIs from the multiple trained bootstrap models. We have trained 10 bootstraps for now, we will use these to get the required CIs as we have done earlier for a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99614bf9-c29d-40a3-957a-61b092aae0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:17:44.874091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 11:17:45.615816: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# These are the ones that is already on the train script\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import Dict, Union\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "# generator related imports\n",
    "from New_data_generator_with_tf import DataGenerator, batch_predict\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# [Req] IMPROVE imports\n",
    "# notice that the improvelibs are in the folder that is a level above, but in the same parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'IMPROVE')))\n",
    "from improvelib.applications.drug_response_prediction.config import DRPTrainConfig\n",
    "from improvelib.utils import str2bool\n",
    "import improvelib.utils as frm\n",
    "from improvelib.metrics import compute_metrics\n",
    "\n",
    "# Model-specific imports\n",
    "from model_params_def import train_params # [Req]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf40477-e5ef-44a2-97c9-b374728ca106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we compute the model misspecification variance (estimate) using the already trained models using the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae03f33-2295-458d-be89-b22bcb4498c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computing the PIs we also need to estimate the variance of the errors. Once we compute the above, we can follow the steps in the paper \"Constructing Optimal Predictio Intervals by Using Neural Networks and Bootstrap Method\" (we follow their section II and not their proposed method as it seems a little complex to implement)by Khosravi et al. in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b887ce-7070-4841-adaf-e49b017663ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with the first part stated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b91d7a0-6277-4f35-9a48-ba36dd3247f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the predictions first for the train data - let's get those and later think about what we need next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f5afd2-81d7-4ad3-8255-9f80c7f53d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to arrange our train and validation data for this exercise too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0031c18-1f32-4120-9ac8-6d40bbf9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory where preprocessed data is stored\n",
    "data_dir = 'exp_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8fcd5a-4b83-4457-9744-68a8bf7cc6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:18:00.190880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 11:18:02.076893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30960 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-04-22 11:18:02,957:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-04-22 11:18:03,205:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-04-22 11:18:03,415:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "cancer_gen_expr_model = tf.keras.models.load_model(os.path.join(data_dir,\"cancer_gen_expr_model\"))\n",
    "cancer_gen_mut_model = tf.keras.models.load_model(os.path.join(data_dir, \"cancer_gen_mut_model\"))\n",
    "cancer_dna_methy_model = tf.keras.models.load_model(os.path.join(data_dir, \"cancer_dna_methy_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52fd524-cab9-44b2-8cbf-ad8f32d9af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_gen_expr_model.trainable = False\n",
    "cancer_gen_mut_model.trainable = False\n",
    "cancer_dna_methy_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a20cf23-1619-4061-94be-06b72083a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"drug_features.pickle\"),\"rb\") as f:\n",
    "        dict_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd690c54-37ed-4eba-a7d4-5762a5299406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"norm_adj_mat.pickle\"),\"rb\") as f:\n",
    "        dict_adj_mat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f6413c-e84b-4fc4-aed1-857120e50ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep = pd.read_csv(os.path.join(data_dir, \"train_y_data.csv\"))\n",
    "valid_keep = pd.read_csv(os.path.join(data_dir, \"val_y_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73786ad7-6296-4c1b-9ae3-aa3a84c0c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7616, 3), (952, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep.shape, valid_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c18113a-3b52-4560-ba3c-52d482d42c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]\n",
    "valid_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd45df5-654a-4a3d-ac69-f186445cde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_drug = valid_keep[\"Drug_ID\"].unique()[-1]\n",
    "samp_ach = np.array(valid_keep[\"Cell_Line\"].unique()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b23d6a-091a-4195-87e7-7b0618e381ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug_1326\n",
      "ACH-000828\n"
     ]
    }
   ],
   "source": [
    "print(samp_drug)\n",
    "print(samp_ach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24b4c61-cef0-40f5-b5c0-43d8a268a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gcn_feats = []\n",
    "train_adj_list = []\n",
    "for drug_id in train_keep[\"Drug_ID\"].values:\n",
    "    train_gcn_feats.append(dict_features[drug_id])\n",
    "    train_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e3f2fa-93b7-44fd-9f65-b123efdf12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616, 7616)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_gcn_feats), len(train_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4de18330-5614-41f8-8508-7b621b7a9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gcn_feats = []\n",
    "valid_adj_list = []\n",
    "for drug_id in valid_keep[\"Drug_ID\"].values:\n",
    "    valid_gcn_feats.append(dict_features[drug_id])\n",
    "    valid_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26c5b59-fc90-4c16-b368-212608e8572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952, 952)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_gcn_feats), len(valid_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bba16bb-9ef7-4857-b7bf-c5c39e9fad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 524 ms, sys: 789 ms, total: 1.31 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce the values to float16\n",
    "train_gcn_feats = np.array(train_gcn_feats).astype(\"float32\")\n",
    "valid_gcn_feats = np.array(valid_gcn_feats).astype(\"float32\")\n",
    "\n",
    "train_adj_list = np.array(train_adj_list).astype(\"float32\")\n",
    "valid_adj_list = np.array(valid_adj_list).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81909e7e-1c8c-4d43-8c3e-8fd17ee81709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators for both train and validation datasets. Let's use the train one now, and later think how we can use the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3208b84-9691-4a94-8d68-5b3cf7ebf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(train_gcn_feats, train_adj_list, train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fcce2a4-30e1-4845-b70c-91c38d3480e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = DataGenerator(valid_gcn_feats, valid_adj_list, valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "372954e9-acde-4ffd-bf97-cf3412198d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, I think now once the model is loaded, we can just get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72759aaf-027e-4f46-80ba-90796d997714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 11:18:10.224025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "CPU times: user 2min 59s, sys: 13 s, total: 3min 12s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I don't think we need a function for this, we should be able to just get it done in a for loop - we will also capture the predeictions for the validation data.\n",
    "\n",
    "# location of the models\n",
    "folder_path = 'bootstrap_results_all'\n",
    "\n",
    "# name of the trained model\n",
    "model_nm = 'DeepCDR_model'\n",
    "\n",
    "all_train_predictions = []\n",
    "all_valid_predictions = []\n",
    "train_true = []\n",
    "valid_true = []\n",
    "# number of boostraps\n",
    "B = 10\n",
    "\n",
    "# start the for loop\n",
    "for i in range(1, B + 1):\n",
    "    # create the folder\n",
    "    folder_nm = 'bootstrap_' + str(i)\n",
    "    # joined path\n",
    "    folder_loc = os.path.join(folder_path, folder_nm, model_nm)\n",
    "    # load the model?\n",
    "    model = tf.keras.models.load_model(folder_loc)\n",
    "    # get the predictions on the train data\n",
    "    y_train_preds, y_train_true = batch_predict(model, train_data_gen)\n",
    "    y_val_preds, y_val_true = batch_predict(model, val_data_gen)\n",
    "    all_train_predictions.append(y_train_preds)\n",
    "    all_valid_predictions.append(y_val_preds)\n",
    "    train_true.append(y_train_true)\n",
    "    valid_true.append(y_val_true)\n",
    "\n",
    "# Notice that we can add some lists to capture the predictions on the validation data as well, eventhough we have these stored, as this will save time - and I think we also need these on the test data inorder the compute the evaluation metrics, like the coverages and the widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6170062-26b8-4df6-86db-1b9c6a0fb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_preds_array = np.array(all_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f23f8569-461c-42cb-8609-c3c5cb09d655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91058ecd-ae01-4ee3-8e11-1cfed56b2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_trues = np.array(train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "935216ba-d717-42a3-9fac-2b905ec4f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a771191a-77f1-4993-8922-08fccdb11ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_true_mean = np.mean(all_train_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1535eb63-36a4-41a6-af0f-d8cc2836b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "691104d7-d389-4149-93cc-f70e6c3f6f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00b58b1c-1f72-47e6-bb7c-95a5a6b3e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db441cb5-6915-45db-b585-f8b35b4b7f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f38fbc9e-7c63-4b8b-b80a-bc7307807625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean?\n",
    "np.mean(np.round(all_train_true_mean, 8) == np.round(np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a938d2e-bcff-4a9b-a376-5456945798a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed the y true values match - sanity check complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cace187f-d4e5-49a1-ae2b-4f9f77c8942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool, so what next?\n",
    "\n",
    "# I think computing the bootstrap quantities, the means and the variance of the predictions. Then we can work on training the NNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f437200-e310-4936-b432-d086cbf8e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first compute the bootstrap means\n",
    "\n",
    "train_bts_mean = np.mean(all_train_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e7868e3-dfcb-4fe1-bcd6-913d4d2c8b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e385854-18bf-40ad-b5c6-009b6bbb4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same measure above for the validation data, as they are required for the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c72fba0-e079-4658-b242-4684859700a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_preds_array = np.array(all_valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1042566c-ddec-4554-9321-2090f82aa957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 952)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40ecd957-39a2-4c7c-b7a8-7c17938ff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_trues = np.array(valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd2c535a-9229-4fb4-9bf6-b0058d094d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 952)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b446d5f-6a5a-4100-8285-db2771b6dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_true_mean = np.mean(all_valid_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "229b2049-a1aa-4d1a-8e42-8bbe37258682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e03ec80-b55f-42e0-addc-d5638a13b556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5071, 0.6525, 0.8944, 0.8691, 0.8527, 0.7833, 0.977 , 0.3598,\n",
       "       0.8045, 0.6042, 0.4953, 0.7821, 0.7977, 0.7289, 0.6565, 0.9151,\n",
       "       0.8609, 0.8073, 0.4476, 0.8136, 0.8707, 0.9311, 0.8245, 0.8403,\n",
       "       0.913 , 0.6812, 0.869 , 0.6503, 0.6457, 0.5061, 0.4772, 0.5123,\n",
       "       0.913 , 0.8945, 0.8858, 0.8849, 0.8343, 0.8553, 0.8076, 0.8347,\n",
       "       0.5389, 0.6081, 0.9519, 0.467 , 0.7841, 0.8478, 0.8789, 0.4217,\n",
       "       0.8834, 0.9832, 0.7933, 0.3334, 0.6786, 0.8028, 0.8078, 0.6597,\n",
       "       0.8299, 0.7734, 0.9431, 0.815 , 0.9919, 0.7359, 0.8435, 0.8259,\n",
       "       0.9377, 0.8228, 0.8317, 0.789 , 0.4609, 0.8475, 0.8412, 0.8198,\n",
       "       0.9699, 1.    , 0.6977, 0.7845, 0.8114, 0.3618, 0.7679, 0.9478,\n",
       "       0.8955, 0.7926, 0.8577, 0.7597, 0.6948, 0.7047, 0.7249, 0.3023,\n",
       "       0.8096, 0.5337, 0.558 , 0.9474, 0.7133, 0.9376, 0.9154, 0.9816,\n",
       "       0.931 , 0.6853, 0.639 , 0.8035, 0.9536, 0.9689, 0.7831, 0.4605,\n",
       "       0.7896, 0.836 , 0.9121, 0.8031, 0.8777, 0.8264, 0.8804, 0.8472,\n",
       "       0.7336, 0.8284, 0.4648, 0.81  , 0.6727, 0.6154, 0.7974, 0.8679,\n",
       "       0.9734, 0.8365, 0.6812, 0.8129, 0.9482, 0.5973, 0.9694, 0.6143,\n",
       "       0.8502, 0.9736, 0.862 , 1.    , 0.8367, 0.9431, 0.7394, 0.696 ,\n",
       "       0.7255, 0.9754, 0.7716, 0.652 , 0.5451, 0.9738, 0.7971, 0.7876,\n",
       "       0.8961, 0.8053, 0.677 , 0.8305, 0.8847, 0.1263, 0.6077, 0.9573,\n",
       "       0.898 , 0.9271, 0.8659, 0.98  , 0.8223, 0.8491, 0.8435, 0.8253,\n",
       "       0.6143, 0.7065, 0.9511, 0.8032, 0.6085, 0.9534, 0.7511, 0.7292,\n",
       "       0.9451, 0.9363, 0.6127, 0.8282, 0.8049, 0.8274, 0.5607, 0.9629,\n",
       "       0.8301, 0.4704, 0.826 , 0.7712, 0.9028, 0.951 , 0.9494, 0.9772,\n",
       "       0.8543, 0.5883, 0.9137, 0.8504, 0.8922, 0.9129, 0.9026, 0.8254,\n",
       "       0.9791, 0.9558, 0.6034, 0.6608, 0.7294, 0.7174, 0.7406, 0.8564,\n",
       "       0.3789, 0.4984, 0.8793, 0.7341, 0.8612, 0.9187, 0.7486, 0.8466,\n",
       "       0.445 , 0.7642, 0.9457, 0.8261, 0.2664, 0.8379, 0.8173, 0.7759,\n",
       "       0.8476, 0.6629, 0.9609, 0.9342, 0.9096, 0.8361, 0.8238, 0.4311,\n",
       "       0.9631, 0.7825, 0.5996, 0.7967, 0.8053, 0.9782, 0.8283, 0.9431,\n",
       "       0.543 , 0.691 , 0.818 , 0.7001, 0.603 , 0.9004, 0.9724, 0.9638,\n",
       "       0.9426, 0.7796, 0.7331, 0.7472, 0.8053, 0.8342, 0.8336, 0.971 ,\n",
       "       0.7693, 0.8345, 0.858 , 0.8998, 0.8945, 0.8285, 0.8099, 0.942 ,\n",
       "       0.5069, 0.7785, 0.7994, 0.826 , 0.4835, 0.8551, 0.5133, 0.9084,\n",
       "       0.6393, 0.9711, 0.8192, 0.8816, 0.8602, 0.9363, 0.7663, 0.9136,\n",
       "       0.8235, 0.8317, 0.8121, 0.8292, 0.8114, 0.8336, 0.9321, 0.8626,\n",
       "       0.8814, 0.8084, 0.9729, 0.947 , 0.2994, 0.5939, 0.9596, 0.8277,\n",
       "       0.3621, 0.9144, 0.7672, 0.9608, 0.6717, 0.8532, 0.8116, 0.7804,\n",
       "       0.8368, 0.9423, 0.969 , 0.8382, 0.8939, 0.862 , 0.8536, 0.8487,\n",
       "       0.7735, 0.8336, 0.4848, 0.8928, 0.9309, 0.8875, 0.9437, 0.8439,\n",
       "       0.9762, 0.8216, 0.9425, 0.8224, 0.4698, 0.7181, 0.8172, 0.9322,\n",
       "       0.702 , 0.9186, 0.8635, 0.9907, 0.8744, 0.8223, 0.6693, 0.6518,\n",
       "       0.6051, 0.8535, 0.8426, 0.8533, 0.7237, 0.8897, 0.8609, 0.8458,\n",
       "       0.9431, 0.4931, 0.8317, 0.7612, 0.8379, 0.8073, 0.9605, 0.8575,\n",
       "       0.7611, 0.7406, 0.9842, 0.8364, 0.914 , 0.7308, 0.5361, 0.8392,\n",
       "       0.7604, 0.7898, 0.7981, 0.2219, 0.9413, 0.8099, 0.9137, 0.1322,\n",
       "       0.6843, 0.5959, 0.6493, 0.7427, 0.7805, 0.9052, 0.85  , 0.6295,\n",
       "       0.8889, 0.8814, 0.8022, 0.8036, 0.9377, 0.8477, 0.801 , 0.6065,\n",
       "       0.6864, 0.4006, 0.4165, 0.8371, 0.9647, 0.732 , 0.7758, 0.9616,\n",
       "       0.7649, 0.7367, 0.9032, 0.7187, 0.8929, 1.    , 0.766 , 0.8005,\n",
       "       0.8179, 0.2762, 0.8371, 0.83  , 0.8616, 0.7949, 0.7685, 0.4635,\n",
       "       0.7063, 0.869 , 0.7935, 0.8493, 0.7754, 0.9604, 0.8295, 0.5925,\n",
       "       1.    , 0.7141, 0.7391, 0.7991, 0.6758, 0.6975, 0.9634, 0.8716,\n",
       "       0.8824, 0.7855, 0.9864, 0.8788, 0.8265, 0.8505, 0.849 , 0.7488,\n",
       "       0.9076, 0.8561, 0.7953, 0.8   , 0.6973, 0.8532, 0.839 , 0.8693,\n",
       "       0.9264, 0.8455, 0.8228, 0.991 , 0.6212, 0.9687, 0.9146, 0.8371,\n",
       "       0.7502, 0.4586, 0.9654, 0.8849, 0.7539, 0.9303, 0.5145, 0.8553,\n",
       "       0.9485, 0.857 , 0.6734, 0.7888, 0.7647, 0.6057, 0.6144, 0.8715,\n",
       "       0.9593, 0.8745, 0.9825, 0.8067, 0.7488, 0.6194, 0.9839, 0.9278,\n",
       "       0.8567, 0.935 , 0.8573, 0.9211, 0.8474, 0.9213, 0.8707, 0.8781,\n",
       "       0.524 , 0.926 , 0.9326, 0.8653, 0.7844, 0.7885, 0.6834, 0.7537,\n",
       "       0.7547, 0.8201, 0.8751, 0.9631, 0.7276, 0.8172, 0.7611, 0.8995,\n",
       "       0.8162, 1.    , 0.8779, 0.6979, 0.6332, 0.7855, 0.9685, 0.4766,\n",
       "       0.8886, 0.8789, 0.7884, 0.7703, 0.9645, 0.7517, 0.8849, 0.6858,\n",
       "       0.97  , 0.9254, 0.9639, 0.6205, 0.8122, 0.3281, 0.6907, 0.752 ,\n",
       "       0.7572, 0.4227, 0.9276, 0.9312, 0.8219, 0.8607, 0.9169, 0.5576,\n",
       "       0.9906, 0.7074, 0.96  , 0.8598, 0.4622, 0.947 , 0.8947, 0.82  ,\n",
       "       0.9578, 0.8584, 0.9049, 0.928 , 0.8673, 0.9576, 0.5601, 0.8944,\n",
       "       0.8826, 0.8598, 0.7672, 0.9905, 0.8276, 0.5206, 0.6252, 0.9187,\n",
       "       0.9553, 0.8979, 0.8508, 0.8814, 0.7053, 0.9728, 0.986 , 0.8224,\n",
       "       0.4454, 0.8862, 0.4787, 0.6587, 0.3936, 0.8558, 0.971 , 0.8129,\n",
       "       0.492 , 0.9836, 0.5267, 0.7794, 0.8998, 0.6339, 0.8905, 0.9781,\n",
       "       0.8868, 0.839 , 0.752 , 0.8201, 0.8317, 0.9146, 0.85  , 0.7615,\n",
       "       0.8349, 0.6176, 0.8116, 0.4638, 0.6725, 0.7923, 0.8103, 0.7979,\n",
       "       0.8434, 0.8129, 0.5165, 0.9519, 0.8004, 0.5484, 0.5575, 0.8856,\n",
       "       0.495 , 0.9784, 0.8425, 0.7909, 0.8914, 0.9162, 0.7185, 0.3606,\n",
       "       0.8459, 0.8478, 0.838 , 0.7518, 0.7946, 0.8314, 0.794 , 0.9343,\n",
       "       0.6345, 0.9714, 0.9776, 0.8055, 0.8468, 0.839 , 0.8957, 0.9123,\n",
       "       0.8267, 0.8172, 0.9586, 0.8301, 0.9403, 0.6539, 0.8381, 0.876 ,\n",
       "       0.9865, 0.8284, 0.8691, 0.8118, 0.9663, 0.8569, 0.837 , 0.8241,\n",
       "       0.9702, 0.9687, 0.7049, 0.5329, 0.771 , 0.7072, 0.7441, 0.8497,\n",
       "       0.9142, 0.8135, 0.7129, 0.8343, 0.493 , 0.4611, 0.9686, 0.931 ,\n",
       "       0.6926, 0.9088, 0.5024, 0.8891, 0.8472, 0.8336, 0.9359, 0.4896,\n",
       "       0.5681, 0.8453, 0.9517, 0.972 , 0.8914, 0.5275, 0.8317, 0.9872,\n",
       "       0.7911, 0.5532, 0.7925, 0.9481, 0.8042, 0.8472, 0.8238, 0.8099,\n",
       "       0.6646, 0.5526, 0.7092, 0.8743, 0.897 , 0.8233, 0.5433, 0.8164,\n",
       "       1.    , 0.9282, 0.8306, 0.601 , 0.6061, 0.9714, 0.8618, 0.4595,\n",
       "       0.8484, 0.9499, 0.9618, 0.8188, 0.5149, 0.7545, 0.965 , 0.8165,\n",
       "       0.9245, 0.9855, 0.5892, 0.9571, 0.518 , 0.5141, 0.9866, 0.3936,\n",
       "       0.6322, 0.9322, 0.9486, 0.5101, 0.5274, 0.9283, 0.9928, 0.8372,\n",
       "       0.4825, 0.8069, 0.8942, 0.4294, 0.8618, 0.9096, 0.8718, 0.861 ,\n",
       "       0.7309, 0.7883, 0.7567, 0.8729, 0.8551, 0.8512, 0.9568, 0.3754,\n",
       "       0.9562, 0.7886, 0.8835, 0.9555, 0.7949, 0.8194, 0.7077, 0.7346,\n",
       "       0.562 , 0.9642, 0.9736, 0.7705, 0.5671, 0.6538, 0.8566, 0.7172,\n",
       "       0.8079, 0.8123, 0.9227, 0.7832, 0.721 , 0.4854, 0.8187, 0.9381,\n",
       "       0.9439, 0.5646, 0.5245, 0.8731, 0.9236, 0.8446, 0.9516, 0.5808,\n",
       "       0.9794, 0.657 , 0.4682, 0.6697, 0.7581, 0.8563, 0.9307, 0.8194,\n",
       "       0.9036, 0.839 , 0.8253, 0.8255, 0.9265, 0.8284, 0.8389, 0.8984,\n",
       "       0.8752, 0.8197, 0.9651, 0.8223, 0.9321, 0.4978, 0.8268, 0.8114,\n",
       "       0.6851, 0.9835, 0.8445, 0.7807, 0.79  , 0.693 , 0.7513, 0.7694,\n",
       "       0.3975, 0.9864, 0.9544, 0.8108, 0.8894, 1.    , 0.9602, 0.4342,\n",
       "       0.81  , 0.8682, 0.921 , 0.8256, 0.8352, 0.9382, 0.9571, 0.4904,\n",
       "       0.9056, 0.8875, 0.8953, 0.8405, 0.8277, 0.9627, 0.6909, 0.9364,\n",
       "       0.923 , 0.9899, 0.5258, 0.9239, 0.9378, 0.5884, 0.7519, 0.9126,\n",
       "       0.7879, 0.8172, 0.8629, 0.7746, 0.5632, 0.9632, 0.7737, 0.7781,\n",
       "       0.7013, 0.9388, 0.7819, 0.8731, 0.4331, 0.4948, 0.4142, 0.8446,\n",
       "       0.98  , 0.7081, 0.8869, 0.8061, 0.9453, 0.8528, 0.8872, 0.7299,\n",
       "       0.8858, 0.9054, 0.4229, 0.3724, 0.8066, 0.5116, 0.598 , 0.793 ,\n",
       "       0.816 , 0.5583, 0.9582, 0.9756, 0.908 , 0.4757, 0.7647, 0.4421,\n",
       "       0.9129, 0.694 , 0.9084, 0.9481, 0.7319, 0.7938, 0.7653, 0.8994,\n",
       "       0.4537, 0.8361, 0.9713, 0.8423, 0.8923, 0.8752, 0.7658, 0.8542,\n",
       "       0.5839, 0.9454, 0.5835, 0.6785, 0.4188, 0.6557, 0.8805, 0.9674,\n",
       "       0.926 , 0.9639, 0.9537, 0.9736, 0.9476, 0.9626, 0.7292, 0.7622,\n",
       "       0.4743, 0.5856, 0.8332, 0.816 , 0.8572, 0.8329, 0.6011, 0.8687,\n",
       "       0.9156, 0.8611, 0.5454, 0.85  , 0.9416, 0.8971, 0.965 , 0.8486,\n",
       "       0.8326, 0.8714, 0.801 , 0.8504, 0.9786, 0.8835, 0.885 , 0.8774,\n",
       "       0.6337, 0.7346, 0.5496, 0.8417, 0.5541, 0.9637, 0.8557, 0.838 ,\n",
       "       0.753 , 0.7203, 0.9391, 0.8126, 0.8251, 0.9271, 0.9147, 0.8937,\n",
       "       0.9793, 0.8286, 0.949 , 0.5109, 0.8692, 0.4825, 0.9528, 0.9149,\n",
       "       0.7944, 0.815 , 0.4973, 0.8809, 0.8193, 0.9545, 0.9572, 0.9677])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee8069d3-21eb-406f-86e8-ef9193a33b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5071, 0.6525, 0.8944, 0.8691, 0.8527, 0.7833, 0.977 , 0.3598,\n",
       "       0.8045, 0.6042, 0.4953, 0.7821, 0.7977, 0.7289, 0.6565, 0.9151,\n",
       "       0.8609, 0.8073, 0.4476, 0.8136, 0.8707, 0.9311, 0.8245, 0.8403,\n",
       "       0.913 , 0.6812, 0.869 , 0.6503, 0.6457, 0.5061, 0.4772, 0.5123,\n",
       "       0.913 , 0.8945, 0.8858, 0.8849, 0.8343, 0.8553, 0.8076, 0.8347,\n",
       "       0.5389, 0.6081, 0.9519, 0.467 , 0.7841, 0.8478, 0.8789, 0.4217,\n",
       "       0.8834, 0.9832, 0.7933, 0.3334, 0.6786, 0.8028, 0.8078, 0.6597,\n",
       "       0.8299, 0.7734, 0.9431, 0.815 , 0.9919, 0.7359, 0.8435, 0.8259,\n",
       "       0.9377, 0.8228, 0.8317, 0.789 , 0.4609, 0.8475, 0.8412, 0.8198,\n",
       "       0.9699, 1.    , 0.6977, 0.7845, 0.8114, 0.3618, 0.7679, 0.9478,\n",
       "       0.8955, 0.7926, 0.8577, 0.7597, 0.6948, 0.7047, 0.7249, 0.3023,\n",
       "       0.8096, 0.5337, 0.558 , 0.9474, 0.7133, 0.9376, 0.9154, 0.9816,\n",
       "       0.931 , 0.6853, 0.639 , 0.8035, 0.9536, 0.9689, 0.7831, 0.4605,\n",
       "       0.7896, 0.836 , 0.9121, 0.8031, 0.8777, 0.8264, 0.8804, 0.8472,\n",
       "       0.7336, 0.8284, 0.4648, 0.81  , 0.6727, 0.6154, 0.7974, 0.8679,\n",
       "       0.9734, 0.8365, 0.6812, 0.8129, 0.9482, 0.5973, 0.9694, 0.6143,\n",
       "       0.8502, 0.9736, 0.862 , 1.    , 0.8367, 0.9431, 0.7394, 0.696 ,\n",
       "       0.7255, 0.9754, 0.7716, 0.652 , 0.5451, 0.9738, 0.7971, 0.7876,\n",
       "       0.8961, 0.8053, 0.677 , 0.8305, 0.8847, 0.1263, 0.6077, 0.9573,\n",
       "       0.898 , 0.9271, 0.8659, 0.98  , 0.8223, 0.8491, 0.8435, 0.8253,\n",
       "       0.6143, 0.7065, 0.9511, 0.8032, 0.6085, 0.9534, 0.7511, 0.7292,\n",
       "       0.9451, 0.9363, 0.6127, 0.8282, 0.8049, 0.8274, 0.5607, 0.9629,\n",
       "       0.8301, 0.4704, 0.826 , 0.7712, 0.9028, 0.951 , 0.9494, 0.9772,\n",
       "       0.8543, 0.5883, 0.9137, 0.8504, 0.8922, 0.9129, 0.9026, 0.8254,\n",
       "       0.9791, 0.9558, 0.6034, 0.6608, 0.7294, 0.7174, 0.7406, 0.8564,\n",
       "       0.3789, 0.4984, 0.8793, 0.7341, 0.8612, 0.9187, 0.7486, 0.8466,\n",
       "       0.445 , 0.7642, 0.9457, 0.8261, 0.2664, 0.8379, 0.8173, 0.7759,\n",
       "       0.8476, 0.6629, 0.9609, 0.9342, 0.9096, 0.8361, 0.8238, 0.4311,\n",
       "       0.9631, 0.7825, 0.5996, 0.7967, 0.8053, 0.9782, 0.8283, 0.9431,\n",
       "       0.543 , 0.691 , 0.818 , 0.7001, 0.603 , 0.9004, 0.9724, 0.9638,\n",
       "       0.9426, 0.7796, 0.7331, 0.7472, 0.8053, 0.8342, 0.8336, 0.971 ,\n",
       "       0.7693, 0.8345, 0.858 , 0.8998, 0.8945, 0.8285, 0.8099, 0.942 ,\n",
       "       0.5069, 0.7785, 0.7994, 0.826 , 0.4835, 0.8551, 0.5133, 0.9084,\n",
       "       0.6393, 0.9711, 0.8192, 0.8816, 0.8602, 0.9363, 0.7663, 0.9136,\n",
       "       0.8235, 0.8317, 0.8121, 0.8292, 0.8114, 0.8336, 0.9321, 0.8626,\n",
       "       0.8814, 0.8084, 0.9729, 0.947 , 0.2994, 0.5939, 0.9596, 0.8277,\n",
       "       0.3621, 0.9144, 0.7672, 0.9608, 0.6717, 0.8532, 0.8116, 0.7804,\n",
       "       0.8368, 0.9423, 0.969 , 0.8382, 0.8939, 0.862 , 0.8536, 0.8487,\n",
       "       0.7735, 0.8336, 0.4848, 0.8928, 0.9309, 0.8875, 0.9437, 0.8439,\n",
       "       0.9762, 0.8216, 0.9425, 0.8224, 0.4698, 0.7181, 0.8172, 0.9322,\n",
       "       0.702 , 0.9186, 0.8635, 0.9907, 0.8744, 0.8223, 0.6693, 0.6518,\n",
       "       0.6051, 0.8535, 0.8426, 0.8533, 0.7237, 0.8897, 0.8609, 0.8458,\n",
       "       0.9431, 0.4931, 0.8317, 0.7612, 0.8379, 0.8073, 0.9605, 0.8575,\n",
       "       0.7611, 0.7406, 0.9842, 0.8364, 0.914 , 0.7308, 0.5361, 0.8392,\n",
       "       0.7604, 0.7898, 0.7981, 0.2219, 0.9413, 0.8099, 0.9137, 0.1322,\n",
       "       0.6843, 0.5959, 0.6493, 0.7427, 0.7805, 0.9052, 0.85  , 0.6295,\n",
       "       0.8889, 0.8814, 0.8022, 0.8036, 0.9377, 0.8477, 0.801 , 0.6065,\n",
       "       0.6864, 0.4006, 0.4165, 0.8371, 0.9647, 0.732 , 0.7758, 0.9616,\n",
       "       0.7649, 0.7367, 0.9032, 0.7187, 0.8929, 1.    , 0.766 , 0.8005,\n",
       "       0.8179, 0.2762, 0.8371, 0.83  , 0.8616, 0.7949, 0.7685, 0.4635,\n",
       "       0.7063, 0.869 , 0.7935, 0.8493, 0.7754, 0.9604, 0.8295, 0.5925,\n",
       "       1.    , 0.7141, 0.7391, 0.7991, 0.6758, 0.6975, 0.9634, 0.8716,\n",
       "       0.8824, 0.7855, 0.9864, 0.8788, 0.8265, 0.8505, 0.849 , 0.7488,\n",
       "       0.9076, 0.8561, 0.7953, 0.8   , 0.6973, 0.8532, 0.839 , 0.8693,\n",
       "       0.9264, 0.8455, 0.8228, 0.991 , 0.6212, 0.9687, 0.9146, 0.8371,\n",
       "       0.7502, 0.4586, 0.9654, 0.8849, 0.7539, 0.9303, 0.5145, 0.8553,\n",
       "       0.9485, 0.857 , 0.6734, 0.7888, 0.7647, 0.6057, 0.6144, 0.8715,\n",
       "       0.9593, 0.8745, 0.9825, 0.8067, 0.7488, 0.6194, 0.9839, 0.9278,\n",
       "       0.8567, 0.935 , 0.8573, 0.9211, 0.8474, 0.9213, 0.8707, 0.8781,\n",
       "       0.524 , 0.926 , 0.9326, 0.8653, 0.7844, 0.7885, 0.6834, 0.7537,\n",
       "       0.7547, 0.8201, 0.8751, 0.9631, 0.7276, 0.8172, 0.7611, 0.8995,\n",
       "       0.8162, 1.    , 0.8779, 0.6979, 0.6332, 0.7855, 0.9685, 0.4766,\n",
       "       0.8886, 0.8789, 0.7884, 0.7703, 0.9645, 0.7517, 0.8849, 0.6858,\n",
       "       0.97  , 0.9254, 0.9639, 0.6205, 0.8122, 0.3281, 0.6907, 0.752 ,\n",
       "       0.7572, 0.4227, 0.9276, 0.9312, 0.8219, 0.8607, 0.9169, 0.5576,\n",
       "       0.9906, 0.7074, 0.96  , 0.8598, 0.4622, 0.947 , 0.8947, 0.82  ,\n",
       "       0.9578, 0.8584, 0.9049, 0.928 , 0.8673, 0.9576, 0.5601, 0.8944,\n",
       "       0.8826, 0.8598, 0.7672, 0.9905, 0.8276, 0.5206, 0.6252, 0.9187,\n",
       "       0.9553, 0.8979, 0.8508, 0.8814, 0.7053, 0.9728, 0.986 , 0.8224,\n",
       "       0.4454, 0.8862, 0.4787, 0.6587, 0.3936, 0.8558, 0.971 , 0.8129,\n",
       "       0.492 , 0.9836, 0.5267, 0.7794, 0.8998, 0.6339, 0.8905, 0.9781,\n",
       "       0.8868, 0.839 , 0.752 , 0.8201, 0.8317, 0.9146, 0.85  , 0.7615,\n",
       "       0.8349, 0.6176, 0.8116, 0.4638, 0.6725, 0.7923, 0.8103, 0.7979,\n",
       "       0.8434, 0.8129, 0.5165, 0.9519, 0.8004, 0.5484, 0.5575, 0.8856,\n",
       "       0.495 , 0.9784, 0.8425, 0.7909, 0.8914, 0.9162, 0.7185, 0.3606,\n",
       "       0.8459, 0.8478, 0.838 , 0.7518, 0.7946, 0.8314, 0.794 , 0.9343,\n",
       "       0.6345, 0.9714, 0.9776, 0.8055, 0.8468, 0.839 , 0.8957, 0.9123,\n",
       "       0.8267, 0.8172, 0.9586, 0.8301, 0.9403, 0.6539, 0.8381, 0.876 ,\n",
       "       0.9865, 0.8284, 0.8691, 0.8118, 0.9663, 0.8569, 0.837 , 0.8241,\n",
       "       0.9702, 0.9687, 0.7049, 0.5329, 0.771 , 0.7072, 0.7441, 0.8497,\n",
       "       0.9142, 0.8135, 0.7129, 0.8343, 0.493 , 0.4611, 0.9686, 0.931 ,\n",
       "       0.6926, 0.9088, 0.5024, 0.8891, 0.8472, 0.8336, 0.9359, 0.4896,\n",
       "       0.5681, 0.8453, 0.9517, 0.972 , 0.8914, 0.5275, 0.8317, 0.9872,\n",
       "       0.7911, 0.5532, 0.7925, 0.9481, 0.8042, 0.8472, 0.8238, 0.8099,\n",
       "       0.6646, 0.5526, 0.7092, 0.8743, 0.897 , 0.8233, 0.5433, 0.8164,\n",
       "       1.    , 0.9282, 0.8306, 0.601 , 0.6061, 0.9714, 0.8618, 0.4595,\n",
       "       0.8484, 0.9499, 0.9618, 0.8188, 0.5149, 0.7545, 0.965 , 0.8165,\n",
       "       0.9245, 0.9855, 0.5892, 0.9571, 0.518 , 0.5141, 0.9866, 0.3936,\n",
       "       0.6322, 0.9322, 0.9486, 0.5101, 0.5274, 0.9283, 0.9928, 0.8372,\n",
       "       0.4825, 0.8069, 0.8942, 0.4294, 0.8618, 0.9096, 0.8718, 0.861 ,\n",
       "       0.7309, 0.7883, 0.7567, 0.8729, 0.8551, 0.8512, 0.9568, 0.3754,\n",
       "       0.9562, 0.7886, 0.8835, 0.9555, 0.7949, 0.8194, 0.7077, 0.7346,\n",
       "       0.562 , 0.9642, 0.9736, 0.7705, 0.5671, 0.6538, 0.8566, 0.7172,\n",
       "       0.8079, 0.8123, 0.9227, 0.7832, 0.721 , 0.4854, 0.8187, 0.9381,\n",
       "       0.9439, 0.5646, 0.5245, 0.8731, 0.9236, 0.8446, 0.9516, 0.5808,\n",
       "       0.9794, 0.657 , 0.4682, 0.6697, 0.7581, 0.8563, 0.9307, 0.8194,\n",
       "       0.9036, 0.839 , 0.8253, 0.8255, 0.9265, 0.8284, 0.8389, 0.8984,\n",
       "       0.8752, 0.8197, 0.9651, 0.8223, 0.9321, 0.4978, 0.8268, 0.8114,\n",
       "       0.6851, 0.9835, 0.8445, 0.7807, 0.79  , 0.693 , 0.7513, 0.7694,\n",
       "       0.3975, 0.9864, 0.9544, 0.8108, 0.8894, 1.    , 0.9602, 0.4342,\n",
       "       0.81  , 0.8682, 0.921 , 0.8256, 0.8352, 0.9382, 0.9571, 0.4904,\n",
       "       0.9056, 0.8875, 0.8953, 0.8405, 0.8277, 0.9627, 0.6909, 0.9364,\n",
       "       0.923 , 0.9899, 0.5258, 0.9239, 0.9378, 0.5884, 0.7519, 0.9126,\n",
       "       0.7879, 0.8172, 0.8629, 0.7746, 0.5632, 0.9632, 0.7737, 0.7781,\n",
       "       0.7013, 0.9388, 0.7819, 0.8731, 0.4331, 0.4948, 0.4142, 0.8446,\n",
       "       0.98  , 0.7081, 0.8869, 0.8061, 0.9453, 0.8528, 0.8872, 0.7299,\n",
       "       0.8858, 0.9054, 0.4229, 0.3724, 0.8066, 0.5116, 0.598 , 0.793 ,\n",
       "       0.816 , 0.5583, 0.9582, 0.9756, 0.908 , 0.4757, 0.7647, 0.4421,\n",
       "       0.9129, 0.694 , 0.9084, 0.9481, 0.7319, 0.7938, 0.7653, 0.8994,\n",
       "       0.4537, 0.8361, 0.9713, 0.8423, 0.8923, 0.8752, 0.7658, 0.8542,\n",
       "       0.5839, 0.9454, 0.5835, 0.6785, 0.4188, 0.6557, 0.8805, 0.9674,\n",
       "       0.926 , 0.9639, 0.9537, 0.9736, 0.9476, 0.9626, 0.7292, 0.7622,\n",
       "       0.4743, 0.5856, 0.8332, 0.816 , 0.8572, 0.8329, 0.6011, 0.8687,\n",
       "       0.9156, 0.8611, 0.5454, 0.85  , 0.9416, 0.8971, 0.965 , 0.8486,\n",
       "       0.8326, 0.8714, 0.801 , 0.8504, 0.9786, 0.8835, 0.885 , 0.8774,\n",
       "       0.6337, 0.7346, 0.5496, 0.8417, 0.5541, 0.9637, 0.8557, 0.838 ,\n",
       "       0.753 , 0.7203, 0.9391, 0.8126, 0.8251, 0.9271, 0.9147, 0.8937,\n",
       "       0.9793, 0.8286, 0.949 , 0.5109, 0.8692, 0.4825, 0.9528, 0.9149,\n",
       "       0.7944, 0.815 , 0.4973, 0.8809, 0.8193, 0.9545, 0.9572, 0.9677])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ec0c755-9040-492a-b66b-2f780b8f4119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16fae0af-3587-48c8-bf2c-2cf921f60a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean? - Do a sanity check\n",
    "np.mean(np.round(all_valid_true_mean, 8) == np.round(np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3703d55f-ab32-4215-9b60-7c9261d7ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bts_mean = np.mean(all_valid_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ee27627-e1ea-4a2d-851e-593e48aadbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c95dc57e-0626-41a1-ab21-27abe7b5ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the bootstrap variance\n",
    "\n",
    "# let's use the same function as earlier - we cannot use this as is, as what we have now is a 2D array, and not a 3D one\n",
    "def equation_6_model_variance(all_preds):\n",
    "    all_vars = []\n",
    "    for i in range(all_preds.shape[1]):\n",
    "        var = (1/(all_preds.shape[0]  - 1))*np.sum(np.square(all_preds[:,i] - np.mean(all_preds[:,i])))\n",
    "        all_vars.append(var)\n",
    "\n",
    "    return np.array(all_vars, dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8ca45d4-17fb-4380-b04d-b3af9ccb3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bts_variance = equation_6_model_variance(all_train_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82d76c69-0e56-4893-b1c9-c2476fd290bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9eda79c8-9ecd-4731-8f40-fe336fe63eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0003b09-b3b5-4972-b094-9a5c18a40a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_train_bts_variance = np.var(all_train_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2567e931-3bc6-4f4a-9142-6aff6a0975d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_train_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1f42532-cd0f-4c2f-97f9-f6f17c781184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_train_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b822ceb3-9872-474f-9d1e-17b5eff2bbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.round(train_bts_variance, 6) == np.round(alt_train_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dd12901-5676-4646-b1a4-71054ceb1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variances on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3d45949-03ce-4dbe-8bef-3f86149bc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bts_variance = equation_6_model_variance(all_valid_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26e4c529-03e1-459c-95f9-1d04e6669f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e898c2c-a195-433c-a5c9-fe751566551b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111437, 0.00215758, 0.0016149 , 0.00110854, 0.00177893,\n",
       "       0.0010395 , 0.00045934, 0.00218281, 0.00072607, 0.00161189,\n",
       "       0.00129221, 0.00064204, 0.00067966, 0.00121964, 0.00054192,\n",
       "       0.00165475, 0.00982931, 0.00098922, 0.00127981, 0.00106769,\n",
       "       0.00047407, 0.00046066, 0.0012382 , 0.00408663, 0.004013  ,\n",
       "       0.00104843, 0.00157014, 0.00091257, 0.00140339, 0.00090362,\n",
       "       0.00096479, 0.00230562, 0.00060487, 0.0004463 , 0.00104181,\n",
       "       0.00054409, 0.00093233, 0.00048041, 0.0004866 , 0.00109376,\n",
       "       0.00160849, 0.00097167, 0.00090513, 0.00222176, 0.00079927,\n",
       "       0.00217688, 0.00070226, 0.0027635 , 0.00018478, 0.00072723,\n",
       "       0.00147327, 0.00253314, 0.00259756, 0.00222062, 0.00460852,\n",
       "       0.00138656, 0.00166871, 0.00168314, 0.00151581, 0.00082051,\n",
       "       0.00081417, 0.00152035, 0.00177849, 0.00065383, 0.00053532,\n",
       "       0.00061589, 0.00129038, 0.00253621, 0.0022012 , 0.00195072,\n",
       "       0.00133277, 0.00134491, 0.00129875, 0.00174996, 0.0011459 ,\n",
       "       0.0016283 , 0.00098838, 0.00261783, 0.00130107, 0.0007612 ,\n",
       "       0.00066536, 0.00308185, 0.0010137 , 0.00149181, 0.00068181,\n",
       "       0.001028  , 0.00209949, 0.0017831 , 0.00148183, 0.00159291,\n",
       "       0.00149922, 0.00026098, 0.00127723, 0.00040533, 0.00021145,\n",
       "       0.00054627, 0.00022489, 0.00101377, 0.0025006 , 0.00242447,\n",
       "       0.00056596, 0.00098948, 0.0004143 , 0.00014458, 0.00220332,\n",
       "       0.00259521, 0.00050684, 0.00129521, 0.00170243, 0.00187751,\n",
       "       0.00129532, 0.00105263, 0.00152395, 0.00137774, 0.00408934,\n",
       "       0.00160638, 0.00430861, 0.00097025, 0.00076396, 0.0006169 ,\n",
       "       0.00062589, 0.00089974, 0.00426047, 0.00072227, 0.00065956,\n",
       "       0.00127007, 0.00068025, 0.00164218, 0.00101867, 0.00138354,\n",
       "       0.00087166, 0.00106146, 0.00060696, 0.00114642, 0.00108146,\n",
       "       0.0023288 , 0.0006369 , 0.00110827, 0.0022268 , 0.01084748,\n",
       "       0.00207236, 0.00049356, 0.00092279, 0.00013559, 0.00316686,\n",
       "       0.00062504, 0.00087341, 0.00054916, 0.00118682, 0.00340552,\n",
       "       0.00485832, 0.00202214, 0.00092711, 0.00157767, 0.00171194,\n",
       "       0.00113766, 0.00145199, 0.0005724 , 0.00087721, 0.00181209,\n",
       "       0.00124976, 0.00465682, 0.00312604, 0.000772  , 0.00155951,\n",
       "       0.00045681, 0.0007997 , 0.00066409, 0.0012055 , 0.00082506,\n",
       "       0.00147647, 0.00064606, 0.00080337, 0.00082253, 0.00049905,\n",
       "       0.00010724, 0.00094698, 0.00051625, 0.00163088, 0.00047667,\n",
       "       0.00061257, 0.00085065, 0.00105394, 0.00115474, 0.00133143,\n",
       "       0.00188529, 0.00119353, 0.0006532 , 0.00139154, 0.00065593,\n",
       "       0.00051322, 0.00491763, 0.00057768, 0.00093863, 0.00084872,\n",
       "       0.0028553 , 0.00160565, 0.0007308 , 0.00075435, 0.00115496,\n",
       "       0.0050956 , 0.00139922, 0.00062893, 0.00159818, 0.00077946,\n",
       "       0.00062139, 0.00099101, 0.00031926, 0.00093404, 0.00094885,\n",
       "       0.00082847, 0.00099562, 0.00397389, 0.00112208, 0.00067516,\n",
       "       0.0017704 , 0.0006182 , 0.00278593, 0.00032323, 0.00029274,\n",
       "       0.00067467, 0.00026727, 0.00149579, 0.00545491, 0.00100247,\n",
       "       0.00075177, 0.00395448, 0.00033377, 0.00092599, 0.0010032 ,\n",
       "       0.00090203, 0.0004061 , 0.00035504, 0.00089337, 0.00038512,\n",
       "       0.00105838, 0.00231034, 0.00077672, 0.00140492, 0.00091546,\n",
       "       0.00093142, 0.0010422 , 0.00063638, 0.00160226, 0.00068731,\n",
       "       0.00027613, 0.00032815, 0.00031656, 0.00079909, 0.00079598,\n",
       "       0.00093946, 0.00077889, 0.00102815, 0.00158737, 0.00084231,\n",
       "       0.00114407, 0.00211012, 0.00123986, 0.00117477, 0.0002933 ,\n",
       "       0.00079184, 0.00055394, 0.00116673, 0.0002841 , 0.00291536,\n",
       "       0.00083093, 0.0012431 , 0.00038396, 0.00110359, 0.00160735,\n",
       "       0.00063898, 0.00086428, 0.00045736, 0.0005024 , 0.00066162,\n",
       "       0.00034281, 0.00061212, 0.00040914, 0.00093036, 0.00068864,\n",
       "       0.00301798, 0.00086001, 0.00068634, 0.00050899, 0.00204736,\n",
       "       0.00095523, 0.0013621 , 0.00125917, 0.00221715, 0.00182159,\n",
       "       0.00236309, 0.00113929, 0.00293465, 0.00047372, 0.00053546,\n",
       "       0.00061066, 0.00169449, 0.00055123, 0.00061831, 0.00141221,\n",
       "       0.00077642, 0.00046675, 0.00061578, 0.00180867, 0.00089534,\n",
       "       0.00097518, 0.00495033, 0.00068091, 0.00050159, 0.00021681,\n",
       "       0.00175101, 0.00083519, 0.00121055, 0.00264147, 0.00166136,\n",
       "       0.00377308, 0.00055078, 0.0011805 , 0.00044852, 0.00120195,\n",
       "       0.00335948, 0.00058967, 0.00098562, 0.00161253, 0.00541449,\n",
       "       0.00134901, 0.00238209, 0.00108658, 0.00099005, 0.0006062 ,\n",
       "       0.00095633, 0.00061301, 0.00193014, 0.00079055, 0.00058937,\n",
       "       0.00113174, 0.00048609, 0.00168928, 0.00072286, 0.00072432,\n",
       "       0.00116571, 0.00072778, 0.00029193, 0.00049241, 0.00198651,\n",
       "       0.00080815, 0.00052806, 0.0003045 , 0.00093751, 0.00123243,\n",
       "       0.00121947, 0.00084271, 0.00493923, 0.00067654, 0.00036083,\n",
       "       0.00610531, 0.00268537, 0.00051727, 0.00245968, 0.00352278,\n",
       "       0.00171874, 0.00095568, 0.00079208, 0.00147711, 0.00023246,\n",
       "       0.00042999, 0.00054933, 0.00375891, 0.00118486, 0.00069612,\n",
       "       0.00069247, 0.00054662, 0.00111342, 0.00119071, 0.00024484,\n",
       "       0.00273917, 0.00150194, 0.00111478, 0.00309851, 0.00066802,\n",
       "       0.0001998 , 0.00049799, 0.00210229, 0.00047212, 0.00120133,\n",
       "       0.00125867, 0.00048767, 0.00085409, 0.00088245, 0.00139486,\n",
       "       0.00228902, 0.00066739, 0.00119141, 0.00383528, 0.00056775,\n",
       "       0.0004135 , 0.00061151, 0.00102361, 0.00069337, 0.00278792,\n",
       "       0.00140879, 0.00097339, 0.00258038, 0.00057952, 0.00124561,\n",
       "       0.00277041, 0.00163902, 0.00564252, 0.0016113 , 0.00080569,\n",
       "       0.00111841, 0.00079754, 0.00281962, 0.00143285, 0.00104024,\n",
       "       0.0010277 , 0.00650669, 0.0010548 , 0.00301513, 0.00076471,\n",
       "       0.00082596, 0.00130718, 0.00119832, 0.00069612, 0.00158047,\n",
       "       0.00079008, 0.00077517, 0.00060112, 0.00295299, 0.00124195,\n",
       "       0.00124794, 0.00151106, 0.00051616, 0.00039265, 0.0008048 ,\n",
       "       0.0006836 , 0.00133884, 0.00146362, 0.0006472 , 0.00211735,\n",
       "       0.0020878 , 0.00071813, 0.00096882, 0.00130084, 0.00163396,\n",
       "       0.00100015, 0.00461533, 0.00176666, 0.00072248, 0.00091394,\n",
       "       0.00167622, 0.00142676, 0.00038982, 0.00202306, 0.00169813,\n",
       "       0.00114872, 0.00065482, 0.00052436, 0.00057793, 0.0003439 ,\n",
       "       0.00146245, 0.00320204, 0.00087077, 0.00091635, 0.00061048,\n",
       "       0.00070216, 0.00047625, 0.00037021, 0.00037495, 0.00100844,\n",
       "       0.00107673, 0.00163132, 0.00157283, 0.00070373, 0.00109324,\n",
       "       0.00054112, 0.00059863, 0.00100613, 0.00094687, 0.00082387,\n",
       "       0.00082754, 0.00042135, 0.00099309, 0.00125863, 0.00092761,\n",
       "       0.00096529, 0.00368414, 0.00050562, 0.00089967, 0.00112941,\n",
       "       0.0005801 , 0.00151167, 0.00470054, 0.00096426, 0.00119647,\n",
       "       0.0031124 , 0.00112936, 0.00147142, 0.0007101 , 0.00207768,\n",
       "       0.00122485, 0.00162991, 0.0005582 , 0.00182014, 0.00101851,\n",
       "       0.00058834, 0.00086792, 0.0004671 , 0.00111345, 0.00239792,\n",
       "       0.00271271, 0.00357127, 0.00170173, 0.00095539, 0.00105047,\n",
       "       0.00089738, 0.00144738, 0.00052082, 0.00080386, 0.00350234,\n",
       "       0.00134784, 0.00341248, 0.00063712, 0.00034559, 0.00117944,\n",
       "       0.00080277, 0.00123246, 0.00106737, 0.0003674 , 0.00069956,\n",
       "       0.0009247 , 0.00041208, 0.00041907, 0.00076658, 0.00248536,\n",
       "       0.00100878, 0.00064976, 0.00115932, 0.00186591, 0.00066364,\n",
       "       0.00061224, 0.00043174, 0.00221246, 0.00071504, 0.00150197,\n",
       "       0.0009884 , 0.00133055, 0.00136611, 0.00280815, 0.00045463,\n",
       "       0.00057309, 0.00054083, 0.00209187, 0.00131263, 0.00121416,\n",
       "       0.00382649, 0.00357206, 0.00100724, 0.0007925 , 0.00051446,\n",
       "       0.00097603, 0.00041763, 0.00186307, 0.00055646, 0.00041823,\n",
       "       0.00193285, 0.00070165, 0.00063762, 0.00052034, 0.00108322,\n",
       "       0.00042526, 0.00041957, 0.00037099, 0.00589372, 0.0009992 ,\n",
       "       0.00086158, 0.00261873, 0.00275991, 0.00166866, 0.00065667,\n",
       "       0.00404085, 0.00110494, 0.0009785 , 0.00106712, 0.00025789,\n",
       "       0.00077212, 0.00096516, 0.00102574, 0.00023336, 0.00044843,\n",
       "       0.00088378, 0.00051122, 0.00140039, 0.00156091, 0.00160515,\n",
       "       0.0023908 , 0.00141813, 0.0017309 , 0.00137478, 0.00384766,\n",
       "       0.00050437, 0.00025918, 0.00063944, 0.00300162, 0.00070797,\n",
       "       0.00052118, 0.00036356, 0.00059468, 0.00099111, 0.00014536,\n",
       "       0.0006835 , 0.00266032, 0.00446594, 0.00031148, 0.00089854,\n",
       "       0.00077864, 0.00084123, 0.00180083, 0.0006877 , 0.00424192,\n",
       "       0.00307816, 0.00167545, 0.00082432, 0.00077463, 0.00045917,\n",
       "       0.00065616, 0.00066917, 0.00084542, 0.00025436, 0.00208227,\n",
       "       0.00038752, 0.00273769, 0.00044491, 0.00134945, 0.0023394 ,\n",
       "       0.00269487, 0.00339019, 0.00101569, 0.00154722, 0.00057415,\n",
       "       0.00116672, 0.00095075, 0.00199425, 0.00241085, 0.0013695 ,\n",
       "       0.00106276, 0.00147437, 0.00148977, 0.00239441, 0.0006888 ,\n",
       "       0.00891248, 0.0003611 , 0.00068709, 0.00069611, 0.00036772,\n",
       "       0.00194125, 0.00164531, 0.00420409, 0.00063458, 0.00041462,\n",
       "       0.000689  , 0.00021805, 0.00244654, 0.00054487, 0.00111532,\n",
       "       0.00413392, 0.00063198, 0.00220092, 0.00142414, 0.00024459,\n",
       "       0.00117797, 0.00256422, 0.0012947 , 0.00065858, 0.00137657,\n",
       "       0.0005748 , 0.00056083, 0.00168503, 0.00120121, 0.00091593,\n",
       "       0.00082671, 0.00108469, 0.00085688, 0.0008909 , 0.00087093,\n",
       "       0.00032111, 0.00085796, 0.00021569, 0.00143944, 0.00027742,\n",
       "       0.0003089 , 0.00163305, 0.00122024, 0.00488824, 0.00042578,\n",
       "       0.00162441, 0.0005389 , 0.00028743, 0.00073288, 0.0005188 ,\n",
       "       0.00237475, 0.00076201, 0.00047954, 0.00577758, 0.00838328,\n",
       "       0.00100877, 0.00036935, 0.00126289, 0.001974  , 0.00079554,\n",
       "       0.00158147, 0.00118952, 0.0006143 , 0.00056484, 0.00029205,\n",
       "       0.00181143, 0.00030801, 0.0005496 , 0.00082446, 0.00079744,\n",
       "       0.00144612, 0.00024571, 0.00225912, 0.00081223, 0.0006999 ,\n",
       "       0.00110922, 0.00094909, 0.00115124, 0.00082941, 0.00087781,\n",
       "       0.0003585 , 0.00025385, 0.00074597, 0.00093697, 0.0014266 ,\n",
       "       0.00111156, 0.00078395, 0.00070163, 0.00092303, 0.00026096,\n",
       "       0.00387819, 0.00101602, 0.00080663, 0.00079298, 0.00112465,\n",
       "       0.00178329, 0.00083451, 0.00163911, 0.00081496, 0.00148372,\n",
       "       0.00098314, 0.0003159 , 0.00178096, 0.00208945, 0.00072003,\n",
       "       0.00138367, 0.00072286, 0.00058347, 0.00052239, 0.00151691,\n",
       "       0.00051487, 0.00105171, 0.0018359 , 0.00245905, 0.00252029,\n",
       "       0.00086322, 0.0011124 , 0.0005323 , 0.00224206, 0.00090803,\n",
       "       0.00119786, 0.00074357, 0.00094587, 0.00117573, 0.00067849,\n",
       "       0.00163543, 0.00120688, 0.00040188, 0.00066495, 0.00064465,\n",
       "       0.00061574, 0.00059363, 0.00114175, 0.00157684, 0.00316822,\n",
       "       0.00168412, 0.00132069, 0.0008337 , 0.00041386, 0.00171502,\n",
       "       0.00059639, 0.0011685 , 0.00160912, 0.00087413, 0.0005101 ,\n",
       "       0.0004163 , 0.00050066, 0.00061949, 0.00036394, 0.0012341 ,\n",
       "       0.00093126, 0.00070249, 0.00097955, 0.00058205, 0.00229092,\n",
       "       0.00046176, 0.0002963 , 0.00181373, 0.00177435, 0.00108046,\n",
       "       0.00059694, 0.00137939, 0.0003995 , 0.00044423, 0.00196723,\n",
       "       0.00084203, 0.00110342, 0.00092694, 0.00100881, 0.00033015,\n",
       "       0.00014838, 0.00197468, 0.00093645, 0.00158655, 0.00083611,\n",
       "       0.00078105, 0.00109045, 0.00083483, 0.00016769, 0.00079371,\n",
       "       0.00073259, 0.00115744, 0.00165336, 0.00101902, 0.00069598,\n",
       "       0.00156121, 0.00222795, 0.00274107, 0.00525939, 0.00109066,\n",
       "       0.00138599, 0.00101957, 0.0011632 , 0.0005676 , 0.00039047,\n",
       "       0.00079619, 0.00041009, 0.0030626 , 0.00049531, 0.00036155,\n",
       "       0.00115392, 0.0048724 , 0.00126186, 0.00286008, 0.00049507,\n",
       "       0.00081294, 0.00088043, 0.00067159, 0.00051938, 0.00041233,\n",
       "       0.0005212 , 0.00084813, 0.00088967, 0.00125715, 0.00024795,\n",
       "       0.00110514, 0.0002626 , 0.00023657, 0.00231094, 0.00133622,\n",
       "       0.00051289, 0.00053149, 0.00380331, 0.00194439, 0.00057296,\n",
       "       0.00016454, 0.001138  , 0.00032119, 0.00106684, 0.00094682,\n",
       "       0.00211666, 0.00046247, 0.00254521, 0.00300573, 0.00071205,\n",
       "       0.00269808, 0.00161356, 0.00147267, 0.00032867, 0.00090766,\n",
       "       0.00226665, 0.00102219, 0.0006289 , 0.00101173, 0.00226479,\n",
       "       0.00073744, 0.00058843, 0.00046138, 0.00053834, 0.00154637,\n",
       "       0.00096891, 0.00067495, 0.00167985, 0.0004735 , 0.00095513,\n",
       "       0.00090897, 0.00298344, 0.00094303, 0.00062568, 0.00056246,\n",
       "       0.00044453, 0.00025906, 0.00042629, 0.00053536, 0.00110848,\n",
       "       0.00035089, 0.00058977, 0.00041883, 0.00085621, 0.0007082 ,\n",
       "       0.00227974, 0.00252602, 0.00079055, 0.00094143, 0.00300012,\n",
       "       0.00069166, 0.00067549, 0.00204274, 0.00085753, 0.00126101,\n",
       "       0.00097221, 0.0006747 , 0.00039717, 0.00078344, 0.00086467,\n",
       "       0.00042727, 0.00079072, 0.00040505, 0.00110287, 0.00103668,\n",
       "       0.00186523, 0.00512457, 0.00094401, 0.00076019, 0.00225636,\n",
       "       0.00088667, 0.00081637, 0.00049976, 0.00072126, 0.00077627,\n",
       "       0.00111435, 0.00070633], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c3c6d6f-4c71-412a-8572-d1fda17f6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_valid_bts_variance = np.var(all_valid_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8710007-5528-4065-b868-777e4abebb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_valid_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2319348c-2bd4-44fa-97a9-e77a614a2d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111437, 0.00215758, 0.0016149 , 0.00110854, 0.00177893,\n",
       "       0.0010395 , 0.00045934, 0.00218281, 0.00072607, 0.00161189,\n",
       "       0.00129221, 0.00064204, 0.00067966, 0.00121964, 0.00054192,\n",
       "       0.00165475, 0.00982931, 0.00098922, 0.00127981, 0.00106769,\n",
       "       0.00047407, 0.00046066, 0.0012382 , 0.00408663, 0.004013  ,\n",
       "       0.00104843, 0.00157014, 0.00091257, 0.00140339, 0.00090362,\n",
       "       0.00096479, 0.00230562, 0.00060487, 0.0004463 , 0.00104181,\n",
       "       0.00054409, 0.00093233, 0.00048041, 0.0004866 , 0.00109376,\n",
       "       0.00160849, 0.00097167, 0.00090513, 0.00222176, 0.00079927,\n",
       "       0.00217688, 0.00070226, 0.0027635 , 0.00018478, 0.00072723,\n",
       "       0.00147327, 0.00253314, 0.00259756, 0.00222062, 0.00460852,\n",
       "       0.00138656, 0.00166871, 0.00168314, 0.00151581, 0.00082051,\n",
       "       0.00081417, 0.00152035, 0.00177849, 0.00065383, 0.00053532,\n",
       "       0.00061589, 0.00129038, 0.00253621, 0.0022012 , 0.00195072,\n",
       "       0.00133277, 0.00134491, 0.00129875, 0.00174996, 0.0011459 ,\n",
       "       0.0016283 , 0.00098838, 0.00261783, 0.00130107, 0.0007612 ,\n",
       "       0.00066536, 0.00308185, 0.0010137 , 0.00149181, 0.00068181,\n",
       "       0.001028  , 0.00209949, 0.0017831 , 0.00148183, 0.00159291,\n",
       "       0.00149922, 0.00026098, 0.00127723, 0.00040533, 0.00021145,\n",
       "       0.00054627, 0.00022489, 0.00101377, 0.0025006 , 0.00242447,\n",
       "       0.00056596, 0.00098948, 0.0004143 , 0.00014458, 0.00220332,\n",
       "       0.00259521, 0.00050684, 0.00129521, 0.00170243, 0.00187751,\n",
       "       0.00129532, 0.00105263, 0.00152395, 0.00137774, 0.00408934,\n",
       "       0.00160638, 0.00430861, 0.00097025, 0.00076396, 0.0006169 ,\n",
       "       0.00062589, 0.00089974, 0.00426047, 0.00072227, 0.00065956,\n",
       "       0.00127007, 0.00068025, 0.00164218, 0.00101867, 0.00138354,\n",
       "       0.00087166, 0.00106146, 0.00060696, 0.00114642, 0.00108146,\n",
       "       0.0023288 , 0.0006369 , 0.00110827, 0.0022268 , 0.01084747,\n",
       "       0.00207236, 0.00049356, 0.00092279, 0.00013559, 0.00316686,\n",
       "       0.00062504, 0.00087341, 0.00054916, 0.00118682, 0.00340552,\n",
       "       0.00485832, 0.00202214, 0.00092711, 0.00157767, 0.00171194,\n",
       "       0.00113766, 0.00145199, 0.0005724 , 0.00087721, 0.00181209,\n",
       "       0.00124976, 0.00465682, 0.00312604, 0.000772  , 0.00155951,\n",
       "       0.00045681, 0.0007997 , 0.00066409, 0.0012055 , 0.00082506,\n",
       "       0.00147647, 0.00064606, 0.00080337, 0.00082253, 0.00049905,\n",
       "       0.00010724, 0.00094698, 0.00051625, 0.00163088, 0.00047667,\n",
       "       0.00061257, 0.00085065, 0.00105394, 0.00115474, 0.00133143,\n",
       "       0.00188529, 0.00119353, 0.0006532 , 0.00139154, 0.00065593,\n",
       "       0.00051322, 0.00491763, 0.00057768, 0.00093863, 0.00084872,\n",
       "       0.0028553 , 0.00160565, 0.0007308 , 0.00075435, 0.00115496,\n",
       "       0.0050956 , 0.00139922, 0.00062893, 0.00159818, 0.00077946,\n",
       "       0.00062139, 0.00099101, 0.00031926, 0.00093404, 0.00094885,\n",
       "       0.00082847, 0.00099562, 0.00397389, 0.00112208, 0.00067516,\n",
       "       0.0017704 , 0.0006182 , 0.00278593, 0.00032323, 0.00029274,\n",
       "       0.00067467, 0.00026727, 0.00149579, 0.00545491, 0.00100247,\n",
       "       0.00075177, 0.00395448, 0.00033377, 0.00092599, 0.0010032 ,\n",
       "       0.00090203, 0.0004061 , 0.00035504, 0.00089337, 0.00038512,\n",
       "       0.00105838, 0.00231034, 0.00077672, 0.00140492, 0.00091546,\n",
       "       0.00093142, 0.0010422 , 0.00063638, 0.00160226, 0.00068731,\n",
       "       0.00027613, 0.00032815, 0.00031656, 0.00079909, 0.00079598,\n",
       "       0.00093946, 0.00077889, 0.00102815, 0.00158737, 0.00084231,\n",
       "       0.00114407, 0.00211012, 0.00123986, 0.00117477, 0.0002933 ,\n",
       "       0.00079184, 0.00055394, 0.00116673, 0.0002841 , 0.00291536,\n",
       "       0.00083093, 0.0012431 , 0.00038396, 0.00110359, 0.00160735,\n",
       "       0.00063898, 0.00086428, 0.00045736, 0.0005024 , 0.00066162,\n",
       "       0.00034281, 0.00061212, 0.00040914, 0.00093036, 0.00068864,\n",
       "       0.00301798, 0.00086001, 0.00068634, 0.00050899, 0.00204736,\n",
       "       0.00095523, 0.0013621 , 0.00125917, 0.00221715, 0.00182159,\n",
       "       0.00236309, 0.00113929, 0.00293465, 0.00047372, 0.00053546,\n",
       "       0.00061066, 0.00169449, 0.00055123, 0.00061831, 0.00141221,\n",
       "       0.00077642, 0.00046675, 0.00061578, 0.00180867, 0.00089534,\n",
       "       0.00097518, 0.00495033, 0.00068091, 0.00050159, 0.00021681,\n",
       "       0.00175101, 0.00083519, 0.00121055, 0.00264147, 0.00166136,\n",
       "       0.00377308, 0.00055078, 0.0011805 , 0.00044852, 0.00120195,\n",
       "       0.00335948, 0.00058967, 0.00098562, 0.00161253, 0.00541449,\n",
       "       0.00134901, 0.00238209, 0.00108658, 0.00099005, 0.0006062 ,\n",
       "       0.00095633, 0.00061301, 0.00193014, 0.00079055, 0.00058937,\n",
       "       0.00113174, 0.00048609, 0.00168928, 0.00072286, 0.00072432,\n",
       "       0.00116571, 0.00072778, 0.00029193, 0.00049241, 0.00198651,\n",
       "       0.00080815, 0.00052806, 0.0003045 , 0.00093751, 0.00123243,\n",
       "       0.00121947, 0.00084271, 0.00493923, 0.00067654, 0.00036083,\n",
       "       0.00610531, 0.00268537, 0.00051727, 0.00245968, 0.00352278,\n",
       "       0.00171874, 0.00095568, 0.00079208, 0.00147711, 0.00023246,\n",
       "       0.00042999, 0.00054933, 0.00375891, 0.00118486, 0.00069612,\n",
       "       0.00069247, 0.00054662, 0.00111342, 0.00119071, 0.00024484,\n",
       "       0.00273917, 0.00150194, 0.00111478, 0.00309851, 0.00066802,\n",
       "       0.0001998 , 0.00049799, 0.00210229, 0.00047212, 0.00120133,\n",
       "       0.00125867, 0.00048767, 0.00085409, 0.00088245, 0.00139486,\n",
       "       0.00228902, 0.00066739, 0.00119141, 0.00383528, 0.00056775,\n",
       "       0.0004135 , 0.00061151, 0.00102361, 0.00069337, 0.00278792,\n",
       "       0.00140879, 0.00097339, 0.00258038, 0.00057952, 0.00124561,\n",
       "       0.00277041, 0.00163902, 0.00564252, 0.0016113 , 0.00080569,\n",
       "       0.00111841, 0.00079754, 0.00281962, 0.00143285, 0.00104024,\n",
       "       0.0010277 , 0.00650669, 0.0010548 , 0.00301513, 0.00076471,\n",
       "       0.00082596, 0.00130718, 0.00119832, 0.00069612, 0.00158047,\n",
       "       0.00079008, 0.00077517, 0.00060112, 0.00295299, 0.00124195,\n",
       "       0.00124794, 0.00151106, 0.00051616, 0.00039265, 0.0008048 ,\n",
       "       0.0006836 , 0.00133884, 0.00146362, 0.0006472 , 0.00211735,\n",
       "       0.0020878 , 0.00071813, 0.00096882, 0.00130084, 0.00163396,\n",
       "       0.00100015, 0.00461533, 0.00176666, 0.00072248, 0.00091394,\n",
       "       0.00167622, 0.00142676, 0.00038982, 0.00202306, 0.00169813,\n",
       "       0.00114872, 0.00065482, 0.00052436, 0.00057793, 0.0003439 ,\n",
       "       0.00146245, 0.00320204, 0.00087077, 0.00091635, 0.00061048,\n",
       "       0.00070216, 0.00047625, 0.00037021, 0.00037495, 0.00100844,\n",
       "       0.00107673, 0.00163132, 0.00157283, 0.00070373, 0.00109324,\n",
       "       0.00054112, 0.00059863, 0.00100613, 0.00094687, 0.00082387,\n",
       "       0.00082754, 0.00042135, 0.00099309, 0.00125863, 0.00092761,\n",
       "       0.00096529, 0.00368414, 0.00050562, 0.00089967, 0.00112941,\n",
       "       0.0005801 , 0.00151167, 0.00470054, 0.00096426, 0.00119647,\n",
       "       0.0031124 , 0.00112936, 0.00147142, 0.0007101 , 0.00207768,\n",
       "       0.00122485, 0.00162991, 0.0005582 , 0.00182014, 0.00101851,\n",
       "       0.00058834, 0.00086792, 0.0004671 , 0.00111345, 0.00239792,\n",
       "       0.00271271, 0.00357127, 0.00170173, 0.00095539, 0.00105047,\n",
       "       0.00089738, 0.00144738, 0.00052082, 0.00080386, 0.00350234,\n",
       "       0.00134784, 0.00341248, 0.00063712, 0.00034559, 0.00117944,\n",
       "       0.00080277, 0.00123246, 0.00106737, 0.0003674 , 0.00069956,\n",
       "       0.0009247 , 0.00041208, 0.00041907, 0.00076658, 0.00248536,\n",
       "       0.00100878, 0.00064976, 0.00115932, 0.00186591, 0.00066364,\n",
       "       0.00061224, 0.00043174, 0.00221246, 0.00071504, 0.00150197,\n",
       "       0.0009884 , 0.00133055, 0.00136611, 0.00280815, 0.00045463,\n",
       "       0.00057309, 0.00054083, 0.00209187, 0.00131263, 0.00121416,\n",
       "       0.00382649, 0.00357207, 0.00100724, 0.0007925 , 0.00051446,\n",
       "       0.00097603, 0.00041763, 0.00186307, 0.00055646, 0.00041823,\n",
       "       0.00193285, 0.00070165, 0.00063762, 0.00052034, 0.00108322,\n",
       "       0.00042526, 0.00041957, 0.00037099, 0.00589372, 0.0009992 ,\n",
       "       0.00086158, 0.00261873, 0.00275991, 0.00166866, 0.00065667,\n",
       "       0.00404085, 0.00110494, 0.0009785 , 0.00106712, 0.00025789,\n",
       "       0.00077212, 0.00096516, 0.00102574, 0.00023336, 0.00044843,\n",
       "       0.00088378, 0.00051122, 0.00140039, 0.00156091, 0.00160515,\n",
       "       0.0023908 , 0.00141813, 0.0017309 , 0.00137478, 0.00384766,\n",
       "       0.00050437, 0.00025918, 0.00063944, 0.00300162, 0.00070797,\n",
       "       0.00052118, 0.00036356, 0.00059468, 0.00099111, 0.00014536,\n",
       "       0.0006835 , 0.00266032, 0.00446594, 0.00031148, 0.00089854,\n",
       "       0.00077864, 0.00084123, 0.00180083, 0.0006877 , 0.00424192,\n",
       "       0.00307816, 0.00167545, 0.00082432, 0.00077463, 0.00045917,\n",
       "       0.00065616, 0.00066917, 0.00084542, 0.00025436, 0.00208227,\n",
       "       0.00038752, 0.00273769, 0.00044491, 0.00134945, 0.0023394 ,\n",
       "       0.00269487, 0.00339019, 0.00101569, 0.00154722, 0.00057415,\n",
       "       0.00116672, 0.00095075, 0.00199425, 0.00241085, 0.0013695 ,\n",
       "       0.00106276, 0.00147437, 0.00148977, 0.00239441, 0.0006888 ,\n",
       "       0.00891248, 0.0003611 , 0.00068709, 0.00069611, 0.00036772,\n",
       "       0.00194125, 0.00164531, 0.00420409, 0.00063458, 0.00041462,\n",
       "       0.000689  , 0.00021805, 0.00244654, 0.00054487, 0.00111532,\n",
       "       0.00413392, 0.00063198, 0.00220092, 0.00142414, 0.00024459,\n",
       "       0.00117797, 0.00256422, 0.0012947 , 0.00065858, 0.00137657,\n",
       "       0.0005748 , 0.00056083, 0.00168503, 0.00120121, 0.00091593,\n",
       "       0.00082671, 0.00108469, 0.00085688, 0.0008909 , 0.00087093,\n",
       "       0.00032111, 0.00085796, 0.00021569, 0.00143944, 0.00027742,\n",
       "       0.0003089 , 0.00163305, 0.00122024, 0.00488824, 0.00042578,\n",
       "       0.00162441, 0.0005389 , 0.00028743, 0.00073288, 0.0005188 ,\n",
       "       0.00237475, 0.00076201, 0.00047954, 0.00577758, 0.00838328,\n",
       "       0.00100877, 0.00036935, 0.00126289, 0.001974  , 0.00079555,\n",
       "       0.00158147, 0.00118952, 0.0006143 , 0.00056484, 0.00029205,\n",
       "       0.00181143, 0.00030801, 0.0005496 , 0.00082446, 0.00079744,\n",
       "       0.00144612, 0.00024571, 0.00225912, 0.00081223, 0.0006999 ,\n",
       "       0.00110922, 0.00094909, 0.00115124, 0.00082941, 0.00087781,\n",
       "       0.0003585 , 0.00025385, 0.00074597, 0.00093697, 0.0014266 ,\n",
       "       0.00111156, 0.00078395, 0.00070163, 0.00092303, 0.00026096,\n",
       "       0.00387819, 0.00101602, 0.00080663, 0.00079298, 0.00112465,\n",
       "       0.00178329, 0.00083451, 0.00163911, 0.00081496, 0.00148372,\n",
       "       0.00098314, 0.0003159 , 0.00178096, 0.00208945, 0.00072003,\n",
       "       0.00138367, 0.00072286, 0.00058347, 0.00052239, 0.00151691,\n",
       "       0.00051487, 0.00105171, 0.0018359 , 0.00245905, 0.00252029,\n",
       "       0.00086322, 0.0011124 , 0.0005323 , 0.00224206, 0.00090803,\n",
       "       0.00119786, 0.00074357, 0.00094587, 0.00117573, 0.00067849,\n",
       "       0.00163543, 0.00120688, 0.00040188, 0.00066495, 0.00064465,\n",
       "       0.00061574, 0.00059363, 0.00114175, 0.00157684, 0.00316822,\n",
       "       0.00168412, 0.00132069, 0.0008337 , 0.00041386, 0.00171502,\n",
       "       0.00059639, 0.0011685 , 0.00160912, 0.00087413, 0.0005101 ,\n",
       "       0.0004163 , 0.00050066, 0.00061949, 0.00036394, 0.0012341 ,\n",
       "       0.00093126, 0.00070249, 0.00097955, 0.00058205, 0.00229092,\n",
       "       0.00046176, 0.0002963 , 0.00181373, 0.00177435, 0.00108046,\n",
       "       0.00059694, 0.00137939, 0.0003995 , 0.00044423, 0.00196723,\n",
       "       0.00084203, 0.00110342, 0.00092694, 0.00100881, 0.00033015,\n",
       "       0.00014838, 0.00197468, 0.00093645, 0.00158655, 0.00083611,\n",
       "       0.00078105, 0.00109045, 0.00083483, 0.00016769, 0.00079371,\n",
       "       0.00073259, 0.00115744, 0.00165336, 0.00101902, 0.00069598,\n",
       "       0.00156121, 0.00222795, 0.00274107, 0.00525939, 0.00109066,\n",
       "       0.00138599, 0.00101957, 0.0011632 , 0.0005676 , 0.00039047,\n",
       "       0.00079619, 0.00041009, 0.0030626 , 0.00049531, 0.00036155,\n",
       "       0.00115392, 0.0048724 , 0.00126186, 0.00286008, 0.00049507,\n",
       "       0.00081294, 0.00088043, 0.00067159, 0.00051938, 0.00041233,\n",
       "       0.0005212 , 0.00084813, 0.00088967, 0.00125715, 0.00024795,\n",
       "       0.00110514, 0.0002626 , 0.00023657, 0.00231094, 0.00133622,\n",
       "       0.00051289, 0.00053149, 0.00380331, 0.00194439, 0.00057296,\n",
       "       0.00016454, 0.001138  , 0.00032119, 0.00106684, 0.00094682,\n",
       "       0.00211666, 0.00046247, 0.00254521, 0.00300573, 0.00071205,\n",
       "       0.00269808, 0.00161356, 0.00147267, 0.00032867, 0.00090766,\n",
       "       0.00226665, 0.00102219, 0.0006289 , 0.00101173, 0.00226479,\n",
       "       0.00073744, 0.00058843, 0.00046138, 0.00053834, 0.00154637,\n",
       "       0.00096891, 0.00067495, 0.00167985, 0.0004735 , 0.00095513,\n",
       "       0.00090897, 0.00298344, 0.00094303, 0.00062568, 0.00056246,\n",
       "       0.00044453, 0.00025906, 0.00042629, 0.00053536, 0.00110848,\n",
       "       0.00035089, 0.00058977, 0.00041883, 0.00085621, 0.0007082 ,\n",
       "       0.00227974, 0.00252602, 0.00079055, 0.00094143, 0.00300012,\n",
       "       0.00069166, 0.00067549, 0.00204274, 0.00085753, 0.00126101,\n",
       "       0.00097221, 0.0006747 , 0.00039717, 0.00078344, 0.00086467,\n",
       "       0.00042727, 0.00079072, 0.00040505, 0.00110287, 0.00103668,\n",
       "       0.00186523, 0.00512457, 0.00094401, 0.00076019, 0.00225636,\n",
       "       0.00088667, 0.00081637, 0.00049976, 0.00072126, 0.00077627,\n",
       "       0.00111435, 0.00070633], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_valid_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a1a62ea-147b-436c-9c51-0c86748d9fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check ot see if the variances are inded correct\n",
    "np.mean(np.round(valid_bts_variance, 6) == np.round(alt_valid_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "715d2502-9ec6-4dd3-ab2b-c25b480e2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the means\n",
    "catch_train_mean = []\n",
    "for i in range(all_train_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_train_preds_array[:,i])\n",
    "    catch_train_mean.append(computed_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5f41e04-4957-4feb-9684-4faac1d51ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_train_means = np.array(catch_train_mean)\n",
    "np.mean(np.round(train_bts_mean,2) == np.round(sanity_check_train_means, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75d80d65-0758-4c7a-9dc7-8fb5a19a622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the validation means\n",
    "catch_valid_mean = []\n",
    "for i in range(all_valid_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_valid_preds_array[:,i])\n",
    "    catch_valid_mean.append(computed_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7440535e-e19c-4715-86a9-64f153a03af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_valid_means = np.array(catch_valid_mean)\n",
    "np.mean(np.round(valid_bts_mean, 3) == np.round(sanity_check_valid_means, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68a0176a-3f4e-472b-a873-fa630aa36ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we have the variances and the means, now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e74b99c0-480f-4b13-9628-6b1c217be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute the rsquare value mentioned in equation 10 of the paper -  we will do this for both train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd6d722f-e67f-4355-a08b-639c9e1180d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e5a7c82-aed4-40d2-a3e4-6bf06cdf9e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f2d9822-cd2d-417c-a632-3c2821fc3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71373927-1f61-420f-a5ac-0ee57bddb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2_true_train = compute_r_squared(train_keep[\"AUC\"].values, train_bts_mean, train_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e013f824-2646-40f3-aef3-7545f965337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6eded242-0eb1-48ef-a533-3aaa48f484e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00820237, 0.00362897, 0.00079942, ..., 0.07589278, 0.00014988,\n",
       "       0.01846294])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2347b2cc-a62d-47c6-acf1-51d1811e3a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there any zeros?\n",
    "np.min(r_2_true_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fcdc25da-e0c0-4401-97cd-900243997bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros\n",
    "num_zeros_train = np.count_nonzero(r_2_true_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56d2e010-bdb2-47d5-86ed-9069ee16d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3852"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0d67be6-7109-485b-b88d-a94dc83c6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is a lot of zeros, but let's role with this number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e1e404a-d97f-4b94-a4d8-e590116378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about teh validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74ef3464-6cc5-4aa1-a3fd-cac346538a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2_true_valid = compute_r_squared(valid_keep[\"AUC\"].values, valid_bts_mean, valid_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5af42c92-f404-4b49-90ad-92fecec23fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39c97501-26a8-4b93-a3e5-a342a52d3f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there any zeros?\n",
    "np.min(r_2_true_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3200b39e-1421-42ae-8b4e-569a988b0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros\n",
    "num_zeros_valid = np.count_nonzero(r_2_true_valid == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "466c9f2c-3cc2-4f7f-bb68-355522d91513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60e66b-57b8-4642-ac15-09ecb7c57cce",
   "metadata": {},
   "source": [
    "#### We will define the custom loss function in equation 12 and also define a very basic model for the NNe -  Okay, this isn't as straight forward as how we would have liked, so we need to revisit this, and decide what the model structure we need for the NNe. Continue the work on this tomorrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "026e8c3e-23b3-4cbc-9a39-99946e53f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm thinking, maybe for now, we should not worry too much about the custom loss function, and proceed with a regular loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83cb6543-c30f-4de2-b830-4bbfac3695e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function as described in equation 12\n",
    "def correct_custom_loss(r_true, r_pred):\n",
    "    # first term in equation 12\n",
    "    term_1 = tf.math.log(r_pred + 1)\n",
    "    # define the second term\n",
    "    term_2 = r_true/r_pred\n",
    "    # cost function\n",
    "    cost = 0.5 * tf.reduce_mean(term_1 + term_2)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f20de1eb-e5e0-4d93-8ab9-1cb87b74cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Define the noise variance estimation network (Phase II)\n",
    "# def create_noise_variance_nn(input_shape):\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(1, activation=tf.keras.activations.exponential)  # Output layer for variance prediction\n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce9eb6fc-4429-453d-b0a6-4febbb2766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use the model above as we should be able to match all our features comin from the different omic types. Therefore we will use the same model that we used for training for training the NNe as well. The only difference will be, now instead of the original response y (AUC) we use the computed r_square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa7a6b61-c6d7-4068-b147-3b249f193e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's define the model as follows - with just one adjustment, make sure the last dense layer has an exponential activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba3187c5-b70b-4b5c-9e1d-cd327e3d1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "dropout1 = 0.10\n",
    "dropout2 = 0.20\n",
    "## get the model architecture\n",
    "def deepcdrgcn_NNe(dict_features, dict_adj_mat, samp_drug, samp_ach, cancer_dna_methy_model, cancer_gen_expr_model, cancer_gen_mut_model, training = training, dropout1 = dropout1, dropout2 = dropout2):\n",
    "    \n",
    "    input_gcn_features = tf.keras.layers.Input(shape = (dict_features[samp_drug].shape[0], 75))\n",
    "    input_norm_adj_mat = tf.keras.layers.Input(shape = (dict_adj_mat[samp_drug].shape[0], dict_adj_mat[samp_drug].shape[0]))\n",
    "    mult_1 = tf.keras.layers.Dot(1)([input_norm_adj_mat, input_gcn_features])\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(256, activation = \"relu\")\n",
    "    dense_out = dense_layer_gcn(mult_1)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "    mult_2 = tf.keras.layers.Dot(1)([input_norm_adj_mat, dense_out])\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(256, activation = \"relu\")\n",
    "    dense_out = dense_layer_gcn(mult_2)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    mult_3 = tf.keras.layers.Dot(1)([input_norm_adj_mat, dense_out])\n",
    "    dense_out = dense_layer_gcn(mult_3)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "\n",
    "    dense_out = tf.keras.layers.GlobalAvgPool1D()(dense_out)\n",
    "    # All above code is for GCN for drugs\n",
    "\n",
    "    # methylation data\n",
    "    input_gen_methy1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_methy = cancer_dna_methy_model(input_gen_methy1)\n",
    "    input_gen_methy.trainable = False\n",
    "    gen_methy_layer = tf.keras.layers.Dense(256, activation = \"tanh\")\n",
    "    \n",
    "    gen_methy_emb = gen_methy_layer(input_gen_methy)\n",
    "    gen_methy_emb = tf.keras.layers.BatchNormalization()(gen_methy_emb)\n",
    "    gen_methy_emb = tf.keras.layers.Dropout(dropout1)(gen_methy_emb, training = training)\n",
    "    gen_methy_layer = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    gen_methy_emb = gen_methy_layer(gen_methy_emb)\n",
    "\n",
    "    # gene expression data\n",
    "    input_gen_expr1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_expr = cancer_gen_expr_model(input_gen_expr1)\n",
    "    input_gen_expr.trainable = False\n",
    "    gen_expr_layer = tf.keras.layers.Dense(256, activation = \"tanh\")\n",
    "    \n",
    "    gen_expr_emb = gen_expr_layer(input_gen_expr)\n",
    "    gen_expr_emb = tf.keras.layers.BatchNormalization()(gen_expr_emb)\n",
    "    gen_expr_emb = tf.keras.layers.Dropout(dropout1)(gen_expr_emb, training = training)\n",
    "    gen_expr_layer = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    gen_expr_emb = gen_expr_layer(gen_expr_emb)\n",
    "    \n",
    "    \n",
    "    input_gen_mut1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_mut = cancer_gen_mut_model(input_gen_mut1)\n",
    "    input_gen_mut.trainable = False\n",
    "    \n",
    "    reshape_gen_mut = tf.keras.layers.Reshape((1, cancer_gen_mut_model(samp_ach).numpy().shape[0], 1))\n",
    "    reshape_gen_mut = reshape_gen_mut(input_gen_mut)\n",
    "    gen_mut_layer = tf.keras.layers.Conv2D(50, (1, 700), strides=5, activation = \"tanh\")\n",
    "    gen_mut_emb = gen_mut_layer(reshape_gen_mut)\n",
    "    pool_layer = tf.keras.layers.MaxPooling2D((1,5))\n",
    "    pool_out = pool_layer(gen_mut_emb)\n",
    "    gen_mut_layer = tf.keras.layers.Conv2D(30, (1, 5), strides=2, activation = \"relu\")\n",
    "    gen_mut_emb = gen_mut_layer(pool_out)\n",
    "    pool_layer = tf.keras.layers.MaxPooling2D((1,10))\n",
    "    pool_out = pool_layer(gen_mut_emb)\n",
    "    flatten_layer = tf.keras.layers.Flatten()\n",
    "    flatten_out = flatten_layer(pool_out)\n",
    "    x_mut = tf.keras.layers.Dense(100,activation = 'relu')(flatten_out)\n",
    "    x_mut = tf.keras.layers.Dropout(dropout1)(x_mut)\n",
    "    \n",
    "    all_omics = tf.keras.layers.Concatenate()([dense_out, gen_methy_emb, gen_expr_emb, x_mut])\n",
    "    x = tf.keras.layers.Dense(300,activation = 'tanh')(all_omics)\n",
    "    x = tf.keras.layers.Dropout(dropout1)(x, training = training)\n",
    "    x = tf.keras.layers.Lambda(lambda x: K.expand_dims(x,axis=-1))(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: K.expand_dims(x,axis=1))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=30, kernel_size=(1,150),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=10, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=5, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout1)(x, training = training)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout2)(x, training = training)\n",
    "    final_out_layer = tf.keras.layers.Dense(1, activation = tf.keras.activations.exponential)\n",
    "    final_out = final_out_layer(x)\n",
    "    simplecdr = tf.keras.models.Model([input_gcn_features, input_norm_adj_mat, input_gen_expr1,\n",
    "                                   input_gen_methy1, input_gen_mut1], final_out)\n",
    "    \n",
    "    return simplecdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "565c52d0-0100-445f-a072-3326c36be985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train function\n",
    "# def train_model_nne(X_train, y_train, bootstrap_predictions, model_variance, model):\n",
    "#     # Define custom loss inside model.compile using lambda\n",
    "    \n",
    "#     model.compile(optimizer='adam', \n",
    "#                   loss=lambda y_true, y_pred: correct_custom_loss(\n",
    "#                       y_true, y_pred))  # y_true corresponds to r_true, and y_pred corresponds to r_pred\n",
    "    \n",
    "#     bootstrap_mean_predictions = np.squeeze(np.mean(bootstrap_predictions, axis = 0))\n",
    "#     # The true residuals (r_true) can be computed outside and passed during training\n",
    "#     r_true = compute_r_squared(y_train, bootstrap_mean_predictions, model_variance)\n",
    "    \n",
    "#     # Fit the model with the true residuals\n",
    "#     model.fit(X_train, r_true, epochs=50, batch_size=32)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "72e1202d-3356-4728-9cc2-784a981332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's try and train this model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4f38f83b-907d-4570-bedd-6b036b67c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "209ac81b-8740-4f20-860d-e06bf7790ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "dropout1 = 0.10\n",
    "dropout2 = 0.20\n",
    "NNE_model = deepcdrgcn_NNe(dict_features, dict_adj_mat, samp_drug, samp_ach, cancer_dna_methy_model, cancer_gen_expr_model, cancer_gen_mut_model,  training = training, dropout1 = dropout1, dropout2 = dropout2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f5e56159-f03e-4df8-9979-33fd564c8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 223, 223)]   0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 223, 75)]    0           []                               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 223, 75)      0           ['input_2[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 223, 256)     19456       ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 223, 256)    1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 223, 256)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 223, 256)     0           ['input_2[0][0]',                \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 18739)        19226214    ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 223, 256)     65792       ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 18739, 1)  0           ['model_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 223, 256)    1024        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1, 3608, 50)  35050       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 223, 256)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 721, 50)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 223, 256)     0           ['input_2[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " model_2 (Functional)           (None, 19606)        16194556    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 30805)        31082245    ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 359, 30)   7530        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 223, 100)     25700       ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          5019392     ['model_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          7886336     ['model[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 35, 30)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 223, 100)    400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1050)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 223, 100)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 100)          105100      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 100)         0           ['dropout_2[0][0]']              \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          25700       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 100)          25700       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 100)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 400)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'dense_4[0][0]',               \n",
      "                                                                  'dense_6[0][0]',                \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 300)          120300      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 300)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 300, 1)       0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1, 300, 1)    0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 1, 151, 30)   4530        ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 75, 30)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 1, 71, 10)    1510        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 23, 10)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 1, 19, 5)     255         ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 6, 5)     0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 1, 6, 5)      0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 30)           0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 30)           0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            31          ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 79,849,893\n",
      "Trainable params: 13,344,630\n",
      "Non-trainable params: 66,505,263\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNE_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66309370-a831-4b3e-a3e4-14500a95fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the train and validation data generators, now with our output for the NNe and not the AUC value we have had earleir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56913f44-e1cc-4d53-a9a0-03f3ead5853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_NNe = DataGenerator(train_gcn_feats, train_adj_list, train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), r_2_true_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ec27361-7c82-4cdd-aa85-2e06abe0a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen_NNe = DataGenerator(valid_gcn_feats, valid_adj_list, valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), r_2_true_valid, batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b570723-3abf-434f-8fe2-05fc8b79861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "lr = 0.001\n",
    "NNE_model.compile(loss = tf.keras.losses.MeanSquaredError(), \n",
    "                      # optimizer = tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False), \n",
    "                    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0aeb4b11-56dc-4cfd-bed3-5b12d72f266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model   \n",
    "batch_size = 32\n",
    "generator_batch_size = 32\n",
    "epoch_num = 150\n",
    "patience_val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "49cefa42-d77e-4e20-b2f7-c0c321ad85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNE_model.fit(train_gen_NNe, validation_data = val_gen_NNe, epochs = epoch_num, \n",
    "#          callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_val, restore_best_weights=True, \n",
    "#                                                      mode = \"min\") ,validation_batch_size = generator_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "78779e36-58d2-4bf2-9e3b-4496831de1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might need to copy and paste all these in a python script as we run into a graph execution error - Also we are currently rolling with the MSE loss -  we will adjust the loss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49089018-0494-4086-9ea1-eb83ce4ed977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f845d-2168-46a4-92de-0ee5bab22f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf0a708-1052-4434-8938-4ee933ce09c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcdr_improve_env)",
   "language": "python",
   "name": "deepcdr_improve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
