{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba46773-9b8f-48d8-9df7-eafaaf079b91",
   "metadata": {},
   "source": [
    "Here we will construct the CIs from the multiple trained bootstrap models. We have trained 10 bootstraps for now, we will use these to get the required CIs as we have done earlier for a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99614bf9-c29d-40a3-957a-61b092aae0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:15:21.604717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-21 21:15:22.692891: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# These are the ones that is already on the train script\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import Dict, Union\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "# generator related imports\n",
    "from New_data_generator_with_tf import DataGenerator, BootstrapGenerator, batch_predict\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# [Req] IMPROVE imports\n",
    "# notice that the improvelibs are in the folder that is a level above, but in the same parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'IMPROVE')))\n",
    "from improvelib.applications.drug_response_prediction.config import DRPTrainConfig\n",
    "from improvelib.utils import str2bool\n",
    "import improvelib.utils as frm\n",
    "from improvelib.metrics import compute_metrics\n",
    "\n",
    "# Model-specific imports\n",
    "from model_params_def import train_params # [Req]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf40477-e5ef-44a2-97c9-b374728ca106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we compute the model misspecification variance (estimate) using the already trained models using the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae03f33-2295-458d-be89-b22bcb4498c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computing the PIs we also need to estimate the variance of the errors. Once we compute the above, we can follow the steps in the paper \"Constructing Optimal Predictio Intervals by Using Neural Networks and Bootstrap Method\" (we follow their section II and not their proposed method as it seems a little complex to implement)by Khosravi et al. in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b887ce-7070-4841-adaf-e49b017663ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with the first part stated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b91d7a0-6277-4f35-9a48-ba36dd3247f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the predictions first for the train data - let's get those and later think about what we need next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f5afd2-81d7-4ad3-8255-9f80c7f53d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to arrange our train and validation data for this exercise too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0031c18-1f32-4120-9ac8-6d40bbf9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory where preprocessed data is stored\n",
    "data_dir = 'exp_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a20cf23-1619-4061-94be-06b72083a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"drug_features.pickle\"),\"rb\") as f:\n",
    "        dict_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd690c54-37ed-4eba-a7d4-5762a5299406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"norm_adj_mat.pickle\"),\"rb\") as f:\n",
    "        dict_adj_mat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f6413c-e84b-4fc4-aed1-857120e50ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep = pd.read_csv(os.path.join(data_dir, \"train_y_data.csv\"))\n",
    "valid_keep = pd.read_csv(os.path.join(data_dir, \"val_y_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73786ad7-6296-4c1b-9ae3-aa3a84c0c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7616, 3), (952, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep.shape, valid_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c18113a-3b52-4560-ba3c-52d482d42c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]\n",
    "valid_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd45df5-654a-4a3d-ac69-f186445cde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_drug = valid_keep[\"Drug_ID\"].unique()[-1]\n",
    "samp_ach = np.array(valid_keep[\"Cell_Line\"].unique()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76b23d6a-091a-4195-87e7-7b0618e381ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug_1326\n",
      "ACH-000828\n"
     ]
    }
   ],
   "source": [
    "print(samp_drug)\n",
    "print(samp_ach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b24b4c61-cef0-40f5-b5c0-43d8a268a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gcn_feats = []\n",
    "train_adj_list = []\n",
    "for drug_id in train_keep[\"Drug_ID\"].values:\n",
    "    train_gcn_feats.append(dict_features[drug_id])\n",
    "    train_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e3f2fa-93b7-44fd-9f65-b123efdf12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616, 7616)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_gcn_feats), len(train_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4de18330-5614-41f8-8508-7b621b7a9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gcn_feats = []\n",
    "valid_adj_list = []\n",
    "for drug_id in valid_keep[\"Drug_ID\"].values:\n",
    "    valid_gcn_feats.append(dict_features[drug_id])\n",
    "    valid_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26c5b59-fc90-4c16-b368-212608e8572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952, 952)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_gcn_feats), len(valid_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bba16bb-9ef7-4857-b7bf-c5c39e9fad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 509 ms, sys: 779 ms, total: 1.29 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce the values to float16\n",
    "train_gcn_feats = np.array(train_gcn_feats).astype(\"float32\")\n",
    "valid_gcn_feats = np.array(valid_gcn_feats).astype(\"float32\")\n",
    "\n",
    "train_adj_list = np.array(train_adj_list).astype(\"float32\")\n",
    "valid_adj_list = np.array(valid_adj_list).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81909e7e-1c8c-4d43-8c3e-8fd17ee81709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators for both train and validation datasets. Let's use the train one now, and later think how we can use the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3208b84-9691-4a94-8d68-5b3cf7ebf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(train_gcn_feats, train_adj_list, train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fcce2a4-30e1-4845-b70c-91c38d3480e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = DataGenerator(valid_gcn_feats, valid_adj_list, valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372954e9-acde-4ffd-bf97-cf3412198d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, I think now once the model is loaded, we can just get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72759aaf-027e-4f46-80ba-90796d997714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:15:42.361558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-21 21:15:44.561828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30960 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2025-04-21 21:15:49.430102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "CPU times: user 2min 40s, sys: 13 s, total: 2min 53s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I don't think we need a function for this, we should be able to just get it done in a for loop\n",
    "\n",
    "# location of the models\n",
    "folder_path = 'bootstrap_results_all'\n",
    "\n",
    "# name of the trained model\n",
    "model_nm = 'DeepCDR_model'\n",
    "\n",
    "all_train_predictions = []\n",
    "train_true = []\n",
    "# number of boostraps\n",
    "B = 10\n",
    "\n",
    "# start the for loop\n",
    "for i in range(1, B + 1):\n",
    "    # create the folder\n",
    "    folder_nm = 'bootstrap_' + str(i)\n",
    "    # joined path\n",
    "    folder_loc = os.path.join(folder_path, folder_nm, model_nm)\n",
    "    # load the model?\n",
    "    model = tf.keras.models.load_model(folder_loc)\n",
    "    # get the predictions on the train data\n",
    "    y_train_preds, y_train_true = batch_predict(model, train_data_gen)\n",
    "    all_train_predictions.append(y_train_preds)\n",
    "    train_true.append(y_train_true)\n",
    "\n",
    "# Notice that we can add some lists to capture the predictions on the validation data as well, eventhough we have these stored, as this will save time - and I think we also need these on the test data inorder the compute the evaluation metrics, like the coverages and the widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6170062-26b8-4df6-86db-1b9c6a0fb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_array = np.array(all_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f23f8569-461c-42cb-8609-c3c5cb09d655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91058ecd-ae01-4ee3-8e11-1cfed56b2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trues = np.array(train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "935216ba-d717-42a3-9fac-2b905ec4f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a771191a-77f1-4993-8922-08fccdb11ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_mean = np.mean(all_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1535eb63-36a4-41a6-af0f-d8cc2836b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "691104d7-d389-4149-93cc-f70e6c3f6f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00b58b1c-1f72-47e6-bb7c-95a5a6b3e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db441cb5-6915-45db-b585-f8b35b4b7f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f38fbc9e-7c63-4b8b-b80a-bc7307807625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean?\n",
    "np.mean(np.round(all_true_mean, 8) == np.round(np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a938d2e-bcff-4a9b-a376-5456945798a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed the y true values match - sanity check complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cace187f-d4e5-49a1-ae2b-4f9f77c8942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool, so what next?\n",
    "\n",
    "# I think computing the bootstrap quantities, the means and the variance of the predictions. Then we can work on training the NNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f437200-e310-4936-b432-d086cbf8e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first compute the bootstrap means\n",
    "\n",
    "train_bts_mean = np.mean(all_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e7868e3-dfcb-4fe1-bcd6-913d4d2c8b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c95dc57e-0626-41a1-ab21-27abe7b5ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the bootstrap variance\n",
    "\n",
    "# let's use the same function as earlier - we cannot use this as is, as what we have now is a 2D array, and not a 3D one\n",
    "def equation_6_model_variance(all_preds):\n",
    "    all_vars = []\n",
    "    for i in range(all_preds.shape[1]):\n",
    "        var = (1/(all_preds.shape[0]  - 1))*np.sum(np.square(all_preds[:,i] - np.mean(all_preds[:,i])))\n",
    "        all_vars.append(var)\n",
    "\n",
    "    return np.array(all_vars, dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8ca45d4-17fb-4380-b04d-b3af9ccb3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bts_variance = equation_6_model_variance(all_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82d76c69-0e56-4893-b1c9-c2476fd290bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eda79c8-9ecd-4731-8f40-fe336fe63eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0003b09-b3b5-4972-b094-9a5c18a40a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_bts_variance = np.var(all_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2567e931-3bc6-4f4a-9142-6aff6a0975d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1f42532-cd0f-4c2f-97f9-f6f17c781184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9a45760-47c2-4cc2-9400-84c075157094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alt_bts_variance), type(train_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b822ceb3-9872-474f-9d1e-17b5eff2bbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.round(train_bts_variance, 6) == np.round(alt_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "715d2502-9ec6-4dd3-ab2b-c25b480e2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the means\n",
    "catch_mean = []\n",
    "for i in range(all_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_preds_array[:,i])\n",
    "    catch_mean.append(computed_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7440535e-e19c-4715-86a9-64f153a03af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_means = np.array(catch_mean)\n",
    "np.mean(np.round(train_bts_mean,2) == np.round(sanity_check_means, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68a0176a-3f4e-472b-a873-fa630aa36ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we have the variances and the means, now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e74b99c0-480f-4b13-9628-6b1c217be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute the rsquare value mentioned in equation 10 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd6d722f-e67f-4355-a08b-639c9e1180d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f2d9822-cd2d-417c-a632-3c2821fc3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71373927-1f61-420f-a5ac-0ee57bddb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2_true = compute_r_squared(train_keep[\"AUC\"].values, train_bts_mean, train_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e013f824-2646-40f3-aef3-7545f965337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6eded242-0eb1-48ef-a533-3aaa48f484e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00820237, 0.00362897, 0.00079942, ..., 0.07589278, 0.00014988,\n",
       "       0.01846294])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2347b2cc-a62d-47c6-acf1-51d1811e3a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there any zeros?\n",
    "np.min(r_2_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcdc25da-e0c0-4401-97cd-900243997bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros\n",
    "num_zeros = np.count_nonzero(r_2_true == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56d2e010-bdb2-47d5-86ed-9069ee16d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3852"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0d67be6-7109-485b-b88d-a94dc83c6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is a lot of zeros, but let's role with this number?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60e66b-57b8-4642-ac15-09ecb7c57cce",
   "metadata": {},
   "source": [
    "#### We will define the custom loss function in equation 12 and also define a very basic model for the NNe -  Okay, this isn't as straight forward as how we would have liked, so we need to revisit this, and decide what the model structure we need for the NNe. Continue the work on this tomorrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83cb6543-c30f-4de2-b830-4bbfac3695e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function as described in equation 12\n",
    "def correct_custom_loss(r_true, r_pred):\n",
    "    # first term in equation 12\n",
    "    term_1 = tf.math.log(r_pred + 1)\n",
    "    # define the second term\n",
    "    term_2 = r_true/r_pred\n",
    "    # cost function\n",
    "    cost = 0.5 * tf.reduce_mean(term_1 + term_2)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f20de1eb-e5e0-4d93-8ab9-1cb87b74cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the noise variance estimation network (Phase II)\n",
    "def create_noise_variance_nn(input_shape):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation=tf.keras.activations.exponential)  # Output layer for variance prediction\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "565c52d0-0100-445f-a072-3326c36be985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train_model_nne(X_train, y_train, bootstrap_predictions, model_variance, model):\n",
    "    # Define custom loss inside model.compile using lambda\n",
    "    \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=lambda y_true, y_pred: correct_custom_loss(\n",
    "                      y_true, y_pred))  # y_true corresponds to r_true, and y_pred corresponds to r_pred\n",
    "    \n",
    "    bootstrap_mean_predictions = np.squeeze(np.mean(bootstrap_predictions, axis = 0))\n",
    "    # The true residuals (r_true) can be computed outside and passed during training\n",
    "    r_true = compute_r_squared(y_train, bootstrap_mean_predictions, model_variance)\n",
    "    \n",
    "    # Fit the model with the true residuals\n",
    "    model.fit(X_train, r_true, epochs=50, batch_size=32)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcdr_improve_env)",
   "language": "python",
   "name": "deepcdr_improve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
