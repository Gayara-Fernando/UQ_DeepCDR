{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba46773-9b8f-48d8-9df7-eafaaf079b91",
   "metadata": {},
   "source": [
    "Here we will construct the CIs from the multiple trained bootstrap models. We have trained 10 bootstraps for now, we will use these to get the required CIs as we have done earlier for a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99614bf9-c29d-40a3-957a-61b092aae0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 10:22:00.265337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-05 10:22:01.089385: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# These are the ones that is already on the train script\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import Dict, Union\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "# generator related imports\n",
    "from New_data_generator_with_tf import DataGenerator, batch_predict\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# [Req] IMPROVE imports\n",
    "# notice that the improvelibs are in the folder that is a level above, but in the same parent directory\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'IMPROVE')))\n",
    "from improvelib.applications.drug_response_prediction.config import DRPTrainConfig\n",
    "from improvelib.utils import str2bool\n",
    "import improvelib.utils as frm\n",
    "from improvelib.metrics import compute_metrics\n",
    "\n",
    "# Model-specific imports\n",
    "from model_params_def import train_params # [Req]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf40477-e5ef-44a2-97c9-b374728ca106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we compute the model misspecification variance (estimate) using the already trained models using the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae03f33-2295-458d-be89-b22bcb4498c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computing the PIs we also need to estimate the variance of the errors. Once we compute the above, we can follow the steps in the paper \"Constructing Optimal Predictio Intervals by Using Neural Networks and Bootstrap Method\" (we follow their section II and not their proposed method as it seems a little complex to implement)by Khosravi et al. in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b887ce-7070-4841-adaf-e49b017663ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get started with the first part stated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b91d7a0-6277-4f35-9a48-ba36dd3247f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the predictions first for the train data - let's get those and later think about what we need next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f5afd2-81d7-4ad3-8255-9f80c7f53d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to arrange our train and validation data for this exercise too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0031c18-1f32-4120-9ac8-6d40bbf9f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory where preprocessed data is stored\n",
    "data_dir = 'exp_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8fcd5a-4b83-4457-9744-68a8bf7cc6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 10:22:17.182090: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-05 10:22:19.165387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30960 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-05-05 10:22:20,124:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-05-05 10:22:20,386:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING tensorflow 2025-05-05 10:22:20,602:\tNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "cancer_gen_expr_model = tf.keras.models.load_model(os.path.join(data_dir,\"cancer_gen_expr_model\"))\n",
    "cancer_gen_mut_model = tf.keras.models.load_model(os.path.join(data_dir, \"cancer_gen_mut_model\"))\n",
    "cancer_dna_methy_model = tf.keras.models.load_model(os.path.join(data_dir, \"cancer_dna_methy_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52fd524-cab9-44b2-8cbf-ad8f32d9af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_gen_expr_model.trainable = False\n",
    "cancer_gen_mut_model.trainable = False\n",
    "cancer_dna_methy_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a20cf23-1619-4061-94be-06b72083a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"drug_features.pickle\"),\"rb\") as f:\n",
    "        dict_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd690c54-37ed-4eba-a7d4-5762a5299406",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"norm_adj_mat.pickle\"),\"rb\") as f:\n",
    "        dict_adj_mat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f6413c-e84b-4fc4-aed1-857120e50ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep = pd.read_csv(os.path.join(data_dir, \"train_y_data.csv\"))\n",
    "valid_keep = pd.read_csv(os.path.join(data_dir, \"val_y_data.csv\"))\n",
    "test_keep = pd.read_csv(os.path.join(data_dir, \"test_y_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73786ad7-6296-4c1b-9ae3-aa3a84c0c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7616, 3), (952, 3), (951, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep.shape, valid_keep.shape, test_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c18113a-3b52-4560-ba3c-52d482d42c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]\n",
    "valid_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]\n",
    "test_keep.columns = [\"Cell_Line\", \"Drug_ID\", \"AUC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fd45df5-654a-4a3d-ac69-f186445cde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_drug = valid_keep[\"Drug_ID\"].unique()[-1]\n",
    "samp_ach = np.array(valid_keep[\"Cell_Line\"].unique()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76b23d6a-091a-4195-87e7-7b0618e381ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug_1326\n",
      "ACH-000828\n"
     ]
    }
   ],
   "source": [
    "print(samp_drug)\n",
    "print(samp_ach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b24b4c61-cef0-40f5-b5c0-43d8a268a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gcn_feats = []\n",
    "train_adj_list = []\n",
    "for drug_id in train_keep[\"Drug_ID\"].values:\n",
    "    train_gcn_feats.append(dict_features[drug_id])\n",
    "    train_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e3f2fa-93b7-44fd-9f65-b123efdf12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616, 7616)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_gcn_feats), len(train_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de18330-5614-41f8-8508-7b621b7a9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gcn_feats = []\n",
    "valid_adj_list = []\n",
    "for drug_id in valid_keep[\"Drug_ID\"].values:\n",
    "    valid_gcn_feats.append(dict_features[drug_id])\n",
    "    valid_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a26c5b59-fc90-4c16-b368-212608e8572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952, 952)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_gcn_feats), len(valid_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48a87b59-2c34-497b-a238-1b5c8c1ae202",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gcn_feats = []\n",
    "test_adj_list = []\n",
    "for drug_id in test_keep[\"Drug_ID\"].values:\n",
    "    test_gcn_feats.append(dict_features[drug_id])\n",
    "    test_adj_list.append(dict_adj_mat[drug_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a396f26-e006-4f43-8fe0-4675dac026e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951, 951)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_gcn_feats), len(test_adj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bba16bb-9ef7-4857-b7bf-c5c39e9fad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 573 ms, sys: 903 ms, total: 1.48 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reduce the values to float16\n",
    "train_gcn_feats = np.array(train_gcn_feats).astype(\"float32\")\n",
    "valid_gcn_feats = np.array(valid_gcn_feats).astype(\"float32\")\n",
    "test_gcn_feats = np.array(test_gcn_feats).astype(\"float32\")\n",
    "\n",
    "train_adj_list = np.array(train_adj_list).astype(\"float32\")\n",
    "valid_adj_list = np.array(valid_adj_list).astype(\"float32\")\n",
    "test_adj_list = np.array(test_adj_list).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81909e7e-1c8c-4d43-8c3e-8fd17ee81709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data generators for both train and validation datasets. Let's use the train one now, and later think how we can use the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3208b84-9691-4a94-8d68-5b3cf7ebf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = DataGenerator(train_gcn_feats, train_adj_list, train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fcce2a4-30e1-4845-b70c-91c38d3480e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_gen = DataGenerator(valid_gcn_feats, valid_adj_list, valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3247669-398c-4b0e-a8f2-75f7b459c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = DataGenerator(test_gcn_feats, test_adj_list, test_keep[\"Cell_Line\"].values.reshape(-1,1), test_keep[\"Cell_Line\"].values.reshape(-1,1), test_keep[\"Cell_Line\"].values.reshape(-1,1), test_keep[\"AUC\"].values.reshape(-1,1), batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "372954e9-acde-4ffd-bf97-cf3412198d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, I think now once the model is loaded, we can just get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72759aaf-027e-4f46-80ba-90796d997714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 10:25:30.746146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "Predictions: 7616\n",
      "True: 7616\n",
      "Predictions: 952\n",
      "True: 952\n",
      "Predictions: 951\n",
      "True: 951\n",
      "CPU times: user 3min 18s, sys: 13.9 s, total: 3min 32s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I don't think we need a function for this, we should be able to just get it done in a for loop - we will also capture the predeictions for the validation data.\n",
    "\n",
    "# location of the models\n",
    "folder_path = 'bootstrap_results_all'\n",
    "\n",
    "# name of the trained model\n",
    "model_nm = 'DeepCDR_model'\n",
    "\n",
    "all_train_predictions = []\n",
    "all_valid_predictions = []\n",
    "all_test_predictions = []\n",
    "train_true = []\n",
    "valid_true = []\n",
    "test_true = []\n",
    "# number of boostraps\n",
    "B = 10\n",
    "\n",
    "# start the for loop\n",
    "for i in range(1, B + 1):\n",
    "    # create the folder\n",
    "    folder_nm = 'bootstrap_' + str(i)\n",
    "    # joined path\n",
    "    folder_loc = os.path.join(folder_path, folder_nm, model_nm)\n",
    "    # load the model?\n",
    "    model = tf.keras.models.load_model(folder_loc)\n",
    "    # get the predictions on the train data\n",
    "    y_train_preds, y_train_true = batch_predict(model, train_data_gen)\n",
    "    y_val_preds, y_val_true = batch_predict(model, val_data_gen)\n",
    "    y_test_preds, y_test_true = batch_predict(model, test_data_gen)\n",
    "    all_train_predictions.append(y_train_preds)\n",
    "    all_valid_predictions.append(y_val_preds)\n",
    "    all_test_predictions.append(y_test_preds)\n",
    "    train_true.append(y_train_true)\n",
    "    valid_true.append(y_val_true)\n",
    "    test_true.append(y_test_true)\n",
    "\n",
    "# Notice that we can add some lists to capture the predictions on the validation data as well, eventhough we have these stored, as this will save time - and I think we also need these on the test data inorder the compute the evaluation metrics, like the coverages and the widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6170062-26b8-4df6-86db-1b9c6a0fb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_preds_array = np.array(all_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f23f8569-461c-42cb-8609-c3c5cb09d655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91058ecd-ae01-4ee3-8e11-1cfed56b2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_trues = np.array(train_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "935216ba-d717-42a3-9fac-2b905ec4f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7616)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a771191a-77f1-4993-8922-08fccdb11ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_true_mean = np.mean(all_train_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1535eb63-36a4-41a6-af0f-d8cc2836b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "691104d7-d389-4149-93cc-f70e6c3f6f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00b58b1c-1f72-47e6-bb7c-95a5a6b3e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7153, 0.9579, 0.413 , ..., 0.522 , 0.9436, 0.9835])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db441cb5-6915-45db-b585-f8b35b4b7f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f38fbc9e-7c63-4b8b-b80a-bc7307807625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean?\n",
    "np.mean(np.round(all_train_true_mean, 8) == np.round(np.squeeze(train_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a938d2e-bcff-4a9b-a376-5456945798a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed the y true values match - sanity check complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cace187f-d4e5-49a1-ae2b-4f9f77c8942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool, so what next?\n",
    "\n",
    "# I think computing the bootstrap quantities, the means and the variance of the predictions. Then we can work on training the NNe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f437200-e310-4936-b432-d086cbf8e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first compute the bootstrap means\n",
    "\n",
    "train_bts_mean = np.mean(all_train_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e7868e3-dfcb-4fe1-bcd6-913d4d2c8b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e385854-18bf-40ad-b5c6-009b6bbb4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same measure above for the validation data, as they are required for the validation data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c72fba0-e079-4658-b242-4684859700a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_preds_array = np.array(all_valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1042566c-ddec-4554-9321-2090f82aa957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 952)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40ecd957-39a2-4c7c-b7a8-7c17938ff5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_trues = np.array(valid_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd2c535a-9229-4fb4-9bf6-b0058d094d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 952)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b446d5f-6a5a-4100-8285-db2771b6dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_valid_true_mean = np.mean(all_valid_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "229b2049-a1aa-4d1a-8e42-8bbe37258682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e03ec80-b55f-42e0-addc-d5638a13b556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5071, 0.6525, 0.8944, 0.8691, 0.8527, 0.7833, 0.977 , 0.3598,\n",
       "       0.8045, 0.6042, 0.4953, 0.7821, 0.7977, 0.7289, 0.6565, 0.9151,\n",
       "       0.8609, 0.8073, 0.4476, 0.8136, 0.8707, 0.9311, 0.8245, 0.8403,\n",
       "       0.913 , 0.6812, 0.869 , 0.6503, 0.6457, 0.5061, 0.4772, 0.5123,\n",
       "       0.913 , 0.8945, 0.8858, 0.8849, 0.8343, 0.8553, 0.8076, 0.8347,\n",
       "       0.5389, 0.6081, 0.9519, 0.467 , 0.7841, 0.8478, 0.8789, 0.4217,\n",
       "       0.8834, 0.9832, 0.7933, 0.3334, 0.6786, 0.8028, 0.8078, 0.6597,\n",
       "       0.8299, 0.7734, 0.9431, 0.815 , 0.9919, 0.7359, 0.8435, 0.8259,\n",
       "       0.9377, 0.8228, 0.8317, 0.789 , 0.4609, 0.8475, 0.8412, 0.8198,\n",
       "       0.9699, 1.    , 0.6977, 0.7845, 0.8114, 0.3618, 0.7679, 0.9478,\n",
       "       0.8955, 0.7926, 0.8577, 0.7597, 0.6948, 0.7047, 0.7249, 0.3023,\n",
       "       0.8096, 0.5337, 0.558 , 0.9474, 0.7133, 0.9376, 0.9154, 0.9816,\n",
       "       0.931 , 0.6853, 0.639 , 0.8035, 0.9536, 0.9689, 0.7831, 0.4605,\n",
       "       0.7896, 0.836 , 0.9121, 0.8031, 0.8777, 0.8264, 0.8804, 0.8472,\n",
       "       0.7336, 0.8284, 0.4648, 0.81  , 0.6727, 0.6154, 0.7974, 0.8679,\n",
       "       0.9734, 0.8365, 0.6812, 0.8129, 0.9482, 0.5973, 0.9694, 0.6143,\n",
       "       0.8502, 0.9736, 0.862 , 1.    , 0.8367, 0.9431, 0.7394, 0.696 ,\n",
       "       0.7255, 0.9754, 0.7716, 0.652 , 0.5451, 0.9738, 0.7971, 0.7876,\n",
       "       0.8961, 0.8053, 0.677 , 0.8305, 0.8847, 0.1263, 0.6077, 0.9573,\n",
       "       0.898 , 0.9271, 0.8659, 0.98  , 0.8223, 0.8491, 0.8435, 0.8253,\n",
       "       0.6143, 0.7065, 0.9511, 0.8032, 0.6085, 0.9534, 0.7511, 0.7292,\n",
       "       0.9451, 0.9363, 0.6127, 0.8282, 0.8049, 0.8274, 0.5607, 0.9629,\n",
       "       0.8301, 0.4704, 0.826 , 0.7712, 0.9028, 0.951 , 0.9494, 0.9772,\n",
       "       0.8543, 0.5883, 0.9137, 0.8504, 0.8922, 0.9129, 0.9026, 0.8254,\n",
       "       0.9791, 0.9558, 0.6034, 0.6608, 0.7294, 0.7174, 0.7406, 0.8564,\n",
       "       0.3789, 0.4984, 0.8793, 0.7341, 0.8612, 0.9187, 0.7486, 0.8466,\n",
       "       0.445 , 0.7642, 0.9457, 0.8261, 0.2664, 0.8379, 0.8173, 0.7759,\n",
       "       0.8476, 0.6629, 0.9609, 0.9342, 0.9096, 0.8361, 0.8238, 0.4311,\n",
       "       0.9631, 0.7825, 0.5996, 0.7967, 0.8053, 0.9782, 0.8283, 0.9431,\n",
       "       0.543 , 0.691 , 0.818 , 0.7001, 0.603 , 0.9004, 0.9724, 0.9638,\n",
       "       0.9426, 0.7796, 0.7331, 0.7472, 0.8053, 0.8342, 0.8336, 0.971 ,\n",
       "       0.7693, 0.8345, 0.858 , 0.8998, 0.8945, 0.8285, 0.8099, 0.942 ,\n",
       "       0.5069, 0.7785, 0.7994, 0.826 , 0.4835, 0.8551, 0.5133, 0.9084,\n",
       "       0.6393, 0.9711, 0.8192, 0.8816, 0.8602, 0.9363, 0.7663, 0.9136,\n",
       "       0.8235, 0.8317, 0.8121, 0.8292, 0.8114, 0.8336, 0.9321, 0.8626,\n",
       "       0.8814, 0.8084, 0.9729, 0.947 , 0.2994, 0.5939, 0.9596, 0.8277,\n",
       "       0.3621, 0.9144, 0.7672, 0.9608, 0.6717, 0.8532, 0.8116, 0.7804,\n",
       "       0.8368, 0.9423, 0.969 , 0.8382, 0.8939, 0.862 , 0.8536, 0.8487,\n",
       "       0.7735, 0.8336, 0.4848, 0.8928, 0.9309, 0.8875, 0.9437, 0.8439,\n",
       "       0.9762, 0.8216, 0.9425, 0.8224, 0.4698, 0.7181, 0.8172, 0.9322,\n",
       "       0.702 , 0.9186, 0.8635, 0.9907, 0.8744, 0.8223, 0.6693, 0.6518,\n",
       "       0.6051, 0.8535, 0.8426, 0.8533, 0.7237, 0.8897, 0.8609, 0.8458,\n",
       "       0.9431, 0.4931, 0.8317, 0.7612, 0.8379, 0.8073, 0.9605, 0.8575,\n",
       "       0.7611, 0.7406, 0.9842, 0.8364, 0.914 , 0.7308, 0.5361, 0.8392,\n",
       "       0.7604, 0.7898, 0.7981, 0.2219, 0.9413, 0.8099, 0.9137, 0.1322,\n",
       "       0.6843, 0.5959, 0.6493, 0.7427, 0.7805, 0.9052, 0.85  , 0.6295,\n",
       "       0.8889, 0.8814, 0.8022, 0.8036, 0.9377, 0.8477, 0.801 , 0.6065,\n",
       "       0.6864, 0.4006, 0.4165, 0.8371, 0.9647, 0.732 , 0.7758, 0.9616,\n",
       "       0.7649, 0.7367, 0.9032, 0.7187, 0.8929, 1.    , 0.766 , 0.8005,\n",
       "       0.8179, 0.2762, 0.8371, 0.83  , 0.8616, 0.7949, 0.7685, 0.4635,\n",
       "       0.7063, 0.869 , 0.7935, 0.8493, 0.7754, 0.9604, 0.8295, 0.5925,\n",
       "       1.    , 0.7141, 0.7391, 0.7991, 0.6758, 0.6975, 0.9634, 0.8716,\n",
       "       0.8824, 0.7855, 0.9864, 0.8788, 0.8265, 0.8505, 0.849 , 0.7488,\n",
       "       0.9076, 0.8561, 0.7953, 0.8   , 0.6973, 0.8532, 0.839 , 0.8693,\n",
       "       0.9264, 0.8455, 0.8228, 0.991 , 0.6212, 0.9687, 0.9146, 0.8371,\n",
       "       0.7502, 0.4586, 0.9654, 0.8849, 0.7539, 0.9303, 0.5145, 0.8553,\n",
       "       0.9485, 0.857 , 0.6734, 0.7888, 0.7647, 0.6057, 0.6144, 0.8715,\n",
       "       0.9593, 0.8745, 0.9825, 0.8067, 0.7488, 0.6194, 0.9839, 0.9278,\n",
       "       0.8567, 0.935 , 0.8573, 0.9211, 0.8474, 0.9213, 0.8707, 0.8781,\n",
       "       0.524 , 0.926 , 0.9326, 0.8653, 0.7844, 0.7885, 0.6834, 0.7537,\n",
       "       0.7547, 0.8201, 0.8751, 0.9631, 0.7276, 0.8172, 0.7611, 0.8995,\n",
       "       0.8162, 1.    , 0.8779, 0.6979, 0.6332, 0.7855, 0.9685, 0.4766,\n",
       "       0.8886, 0.8789, 0.7884, 0.7703, 0.9645, 0.7517, 0.8849, 0.6858,\n",
       "       0.97  , 0.9254, 0.9639, 0.6205, 0.8122, 0.3281, 0.6907, 0.752 ,\n",
       "       0.7572, 0.4227, 0.9276, 0.9312, 0.8219, 0.8607, 0.9169, 0.5576,\n",
       "       0.9906, 0.7074, 0.96  , 0.8598, 0.4622, 0.947 , 0.8947, 0.82  ,\n",
       "       0.9578, 0.8584, 0.9049, 0.928 , 0.8673, 0.9576, 0.5601, 0.8944,\n",
       "       0.8826, 0.8598, 0.7672, 0.9905, 0.8276, 0.5206, 0.6252, 0.9187,\n",
       "       0.9553, 0.8979, 0.8508, 0.8814, 0.7053, 0.9728, 0.986 , 0.8224,\n",
       "       0.4454, 0.8862, 0.4787, 0.6587, 0.3936, 0.8558, 0.971 , 0.8129,\n",
       "       0.492 , 0.9836, 0.5267, 0.7794, 0.8998, 0.6339, 0.8905, 0.9781,\n",
       "       0.8868, 0.839 , 0.752 , 0.8201, 0.8317, 0.9146, 0.85  , 0.7615,\n",
       "       0.8349, 0.6176, 0.8116, 0.4638, 0.6725, 0.7923, 0.8103, 0.7979,\n",
       "       0.8434, 0.8129, 0.5165, 0.9519, 0.8004, 0.5484, 0.5575, 0.8856,\n",
       "       0.495 , 0.9784, 0.8425, 0.7909, 0.8914, 0.9162, 0.7185, 0.3606,\n",
       "       0.8459, 0.8478, 0.838 , 0.7518, 0.7946, 0.8314, 0.794 , 0.9343,\n",
       "       0.6345, 0.9714, 0.9776, 0.8055, 0.8468, 0.839 , 0.8957, 0.9123,\n",
       "       0.8267, 0.8172, 0.9586, 0.8301, 0.9403, 0.6539, 0.8381, 0.876 ,\n",
       "       0.9865, 0.8284, 0.8691, 0.8118, 0.9663, 0.8569, 0.837 , 0.8241,\n",
       "       0.9702, 0.9687, 0.7049, 0.5329, 0.771 , 0.7072, 0.7441, 0.8497,\n",
       "       0.9142, 0.8135, 0.7129, 0.8343, 0.493 , 0.4611, 0.9686, 0.931 ,\n",
       "       0.6926, 0.9088, 0.5024, 0.8891, 0.8472, 0.8336, 0.9359, 0.4896,\n",
       "       0.5681, 0.8453, 0.9517, 0.972 , 0.8914, 0.5275, 0.8317, 0.9872,\n",
       "       0.7911, 0.5532, 0.7925, 0.9481, 0.8042, 0.8472, 0.8238, 0.8099,\n",
       "       0.6646, 0.5526, 0.7092, 0.8743, 0.897 , 0.8233, 0.5433, 0.8164,\n",
       "       1.    , 0.9282, 0.8306, 0.601 , 0.6061, 0.9714, 0.8618, 0.4595,\n",
       "       0.8484, 0.9499, 0.9618, 0.8188, 0.5149, 0.7545, 0.965 , 0.8165,\n",
       "       0.9245, 0.9855, 0.5892, 0.9571, 0.518 , 0.5141, 0.9866, 0.3936,\n",
       "       0.6322, 0.9322, 0.9486, 0.5101, 0.5274, 0.9283, 0.9928, 0.8372,\n",
       "       0.4825, 0.8069, 0.8942, 0.4294, 0.8618, 0.9096, 0.8718, 0.861 ,\n",
       "       0.7309, 0.7883, 0.7567, 0.8729, 0.8551, 0.8512, 0.9568, 0.3754,\n",
       "       0.9562, 0.7886, 0.8835, 0.9555, 0.7949, 0.8194, 0.7077, 0.7346,\n",
       "       0.562 , 0.9642, 0.9736, 0.7705, 0.5671, 0.6538, 0.8566, 0.7172,\n",
       "       0.8079, 0.8123, 0.9227, 0.7832, 0.721 , 0.4854, 0.8187, 0.9381,\n",
       "       0.9439, 0.5646, 0.5245, 0.8731, 0.9236, 0.8446, 0.9516, 0.5808,\n",
       "       0.9794, 0.657 , 0.4682, 0.6697, 0.7581, 0.8563, 0.9307, 0.8194,\n",
       "       0.9036, 0.839 , 0.8253, 0.8255, 0.9265, 0.8284, 0.8389, 0.8984,\n",
       "       0.8752, 0.8197, 0.9651, 0.8223, 0.9321, 0.4978, 0.8268, 0.8114,\n",
       "       0.6851, 0.9835, 0.8445, 0.7807, 0.79  , 0.693 , 0.7513, 0.7694,\n",
       "       0.3975, 0.9864, 0.9544, 0.8108, 0.8894, 1.    , 0.9602, 0.4342,\n",
       "       0.81  , 0.8682, 0.921 , 0.8256, 0.8352, 0.9382, 0.9571, 0.4904,\n",
       "       0.9056, 0.8875, 0.8953, 0.8405, 0.8277, 0.9627, 0.6909, 0.9364,\n",
       "       0.923 , 0.9899, 0.5258, 0.9239, 0.9378, 0.5884, 0.7519, 0.9126,\n",
       "       0.7879, 0.8172, 0.8629, 0.7746, 0.5632, 0.9632, 0.7737, 0.7781,\n",
       "       0.7013, 0.9388, 0.7819, 0.8731, 0.4331, 0.4948, 0.4142, 0.8446,\n",
       "       0.98  , 0.7081, 0.8869, 0.8061, 0.9453, 0.8528, 0.8872, 0.7299,\n",
       "       0.8858, 0.9054, 0.4229, 0.3724, 0.8066, 0.5116, 0.598 , 0.793 ,\n",
       "       0.816 , 0.5583, 0.9582, 0.9756, 0.908 , 0.4757, 0.7647, 0.4421,\n",
       "       0.9129, 0.694 , 0.9084, 0.9481, 0.7319, 0.7938, 0.7653, 0.8994,\n",
       "       0.4537, 0.8361, 0.9713, 0.8423, 0.8923, 0.8752, 0.7658, 0.8542,\n",
       "       0.5839, 0.9454, 0.5835, 0.6785, 0.4188, 0.6557, 0.8805, 0.9674,\n",
       "       0.926 , 0.9639, 0.9537, 0.9736, 0.9476, 0.9626, 0.7292, 0.7622,\n",
       "       0.4743, 0.5856, 0.8332, 0.816 , 0.8572, 0.8329, 0.6011, 0.8687,\n",
       "       0.9156, 0.8611, 0.5454, 0.85  , 0.9416, 0.8971, 0.965 , 0.8486,\n",
       "       0.8326, 0.8714, 0.801 , 0.8504, 0.9786, 0.8835, 0.885 , 0.8774,\n",
       "       0.6337, 0.7346, 0.5496, 0.8417, 0.5541, 0.9637, 0.8557, 0.838 ,\n",
       "       0.753 , 0.7203, 0.9391, 0.8126, 0.8251, 0.9271, 0.9147, 0.8937,\n",
       "       0.9793, 0.8286, 0.949 , 0.5109, 0.8692, 0.4825, 0.9528, 0.9149,\n",
       "       0.7944, 0.815 , 0.4973, 0.8809, 0.8193, 0.9545, 0.9572, 0.9677])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_valid_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee8069d3-21eb-406f-86e8-ef9193a33b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5071, 0.6525, 0.8944, 0.8691, 0.8527, 0.7833, 0.977 , 0.3598,\n",
       "       0.8045, 0.6042, 0.4953, 0.7821, 0.7977, 0.7289, 0.6565, 0.9151,\n",
       "       0.8609, 0.8073, 0.4476, 0.8136, 0.8707, 0.9311, 0.8245, 0.8403,\n",
       "       0.913 , 0.6812, 0.869 , 0.6503, 0.6457, 0.5061, 0.4772, 0.5123,\n",
       "       0.913 , 0.8945, 0.8858, 0.8849, 0.8343, 0.8553, 0.8076, 0.8347,\n",
       "       0.5389, 0.6081, 0.9519, 0.467 , 0.7841, 0.8478, 0.8789, 0.4217,\n",
       "       0.8834, 0.9832, 0.7933, 0.3334, 0.6786, 0.8028, 0.8078, 0.6597,\n",
       "       0.8299, 0.7734, 0.9431, 0.815 , 0.9919, 0.7359, 0.8435, 0.8259,\n",
       "       0.9377, 0.8228, 0.8317, 0.789 , 0.4609, 0.8475, 0.8412, 0.8198,\n",
       "       0.9699, 1.    , 0.6977, 0.7845, 0.8114, 0.3618, 0.7679, 0.9478,\n",
       "       0.8955, 0.7926, 0.8577, 0.7597, 0.6948, 0.7047, 0.7249, 0.3023,\n",
       "       0.8096, 0.5337, 0.558 , 0.9474, 0.7133, 0.9376, 0.9154, 0.9816,\n",
       "       0.931 , 0.6853, 0.639 , 0.8035, 0.9536, 0.9689, 0.7831, 0.4605,\n",
       "       0.7896, 0.836 , 0.9121, 0.8031, 0.8777, 0.8264, 0.8804, 0.8472,\n",
       "       0.7336, 0.8284, 0.4648, 0.81  , 0.6727, 0.6154, 0.7974, 0.8679,\n",
       "       0.9734, 0.8365, 0.6812, 0.8129, 0.9482, 0.5973, 0.9694, 0.6143,\n",
       "       0.8502, 0.9736, 0.862 , 1.    , 0.8367, 0.9431, 0.7394, 0.696 ,\n",
       "       0.7255, 0.9754, 0.7716, 0.652 , 0.5451, 0.9738, 0.7971, 0.7876,\n",
       "       0.8961, 0.8053, 0.677 , 0.8305, 0.8847, 0.1263, 0.6077, 0.9573,\n",
       "       0.898 , 0.9271, 0.8659, 0.98  , 0.8223, 0.8491, 0.8435, 0.8253,\n",
       "       0.6143, 0.7065, 0.9511, 0.8032, 0.6085, 0.9534, 0.7511, 0.7292,\n",
       "       0.9451, 0.9363, 0.6127, 0.8282, 0.8049, 0.8274, 0.5607, 0.9629,\n",
       "       0.8301, 0.4704, 0.826 , 0.7712, 0.9028, 0.951 , 0.9494, 0.9772,\n",
       "       0.8543, 0.5883, 0.9137, 0.8504, 0.8922, 0.9129, 0.9026, 0.8254,\n",
       "       0.9791, 0.9558, 0.6034, 0.6608, 0.7294, 0.7174, 0.7406, 0.8564,\n",
       "       0.3789, 0.4984, 0.8793, 0.7341, 0.8612, 0.9187, 0.7486, 0.8466,\n",
       "       0.445 , 0.7642, 0.9457, 0.8261, 0.2664, 0.8379, 0.8173, 0.7759,\n",
       "       0.8476, 0.6629, 0.9609, 0.9342, 0.9096, 0.8361, 0.8238, 0.4311,\n",
       "       0.9631, 0.7825, 0.5996, 0.7967, 0.8053, 0.9782, 0.8283, 0.9431,\n",
       "       0.543 , 0.691 , 0.818 , 0.7001, 0.603 , 0.9004, 0.9724, 0.9638,\n",
       "       0.9426, 0.7796, 0.7331, 0.7472, 0.8053, 0.8342, 0.8336, 0.971 ,\n",
       "       0.7693, 0.8345, 0.858 , 0.8998, 0.8945, 0.8285, 0.8099, 0.942 ,\n",
       "       0.5069, 0.7785, 0.7994, 0.826 , 0.4835, 0.8551, 0.5133, 0.9084,\n",
       "       0.6393, 0.9711, 0.8192, 0.8816, 0.8602, 0.9363, 0.7663, 0.9136,\n",
       "       0.8235, 0.8317, 0.8121, 0.8292, 0.8114, 0.8336, 0.9321, 0.8626,\n",
       "       0.8814, 0.8084, 0.9729, 0.947 , 0.2994, 0.5939, 0.9596, 0.8277,\n",
       "       0.3621, 0.9144, 0.7672, 0.9608, 0.6717, 0.8532, 0.8116, 0.7804,\n",
       "       0.8368, 0.9423, 0.969 , 0.8382, 0.8939, 0.862 , 0.8536, 0.8487,\n",
       "       0.7735, 0.8336, 0.4848, 0.8928, 0.9309, 0.8875, 0.9437, 0.8439,\n",
       "       0.9762, 0.8216, 0.9425, 0.8224, 0.4698, 0.7181, 0.8172, 0.9322,\n",
       "       0.702 , 0.9186, 0.8635, 0.9907, 0.8744, 0.8223, 0.6693, 0.6518,\n",
       "       0.6051, 0.8535, 0.8426, 0.8533, 0.7237, 0.8897, 0.8609, 0.8458,\n",
       "       0.9431, 0.4931, 0.8317, 0.7612, 0.8379, 0.8073, 0.9605, 0.8575,\n",
       "       0.7611, 0.7406, 0.9842, 0.8364, 0.914 , 0.7308, 0.5361, 0.8392,\n",
       "       0.7604, 0.7898, 0.7981, 0.2219, 0.9413, 0.8099, 0.9137, 0.1322,\n",
       "       0.6843, 0.5959, 0.6493, 0.7427, 0.7805, 0.9052, 0.85  , 0.6295,\n",
       "       0.8889, 0.8814, 0.8022, 0.8036, 0.9377, 0.8477, 0.801 , 0.6065,\n",
       "       0.6864, 0.4006, 0.4165, 0.8371, 0.9647, 0.732 , 0.7758, 0.9616,\n",
       "       0.7649, 0.7367, 0.9032, 0.7187, 0.8929, 1.    , 0.766 , 0.8005,\n",
       "       0.8179, 0.2762, 0.8371, 0.83  , 0.8616, 0.7949, 0.7685, 0.4635,\n",
       "       0.7063, 0.869 , 0.7935, 0.8493, 0.7754, 0.9604, 0.8295, 0.5925,\n",
       "       1.    , 0.7141, 0.7391, 0.7991, 0.6758, 0.6975, 0.9634, 0.8716,\n",
       "       0.8824, 0.7855, 0.9864, 0.8788, 0.8265, 0.8505, 0.849 , 0.7488,\n",
       "       0.9076, 0.8561, 0.7953, 0.8   , 0.6973, 0.8532, 0.839 , 0.8693,\n",
       "       0.9264, 0.8455, 0.8228, 0.991 , 0.6212, 0.9687, 0.9146, 0.8371,\n",
       "       0.7502, 0.4586, 0.9654, 0.8849, 0.7539, 0.9303, 0.5145, 0.8553,\n",
       "       0.9485, 0.857 , 0.6734, 0.7888, 0.7647, 0.6057, 0.6144, 0.8715,\n",
       "       0.9593, 0.8745, 0.9825, 0.8067, 0.7488, 0.6194, 0.9839, 0.9278,\n",
       "       0.8567, 0.935 , 0.8573, 0.9211, 0.8474, 0.9213, 0.8707, 0.8781,\n",
       "       0.524 , 0.926 , 0.9326, 0.8653, 0.7844, 0.7885, 0.6834, 0.7537,\n",
       "       0.7547, 0.8201, 0.8751, 0.9631, 0.7276, 0.8172, 0.7611, 0.8995,\n",
       "       0.8162, 1.    , 0.8779, 0.6979, 0.6332, 0.7855, 0.9685, 0.4766,\n",
       "       0.8886, 0.8789, 0.7884, 0.7703, 0.9645, 0.7517, 0.8849, 0.6858,\n",
       "       0.97  , 0.9254, 0.9639, 0.6205, 0.8122, 0.3281, 0.6907, 0.752 ,\n",
       "       0.7572, 0.4227, 0.9276, 0.9312, 0.8219, 0.8607, 0.9169, 0.5576,\n",
       "       0.9906, 0.7074, 0.96  , 0.8598, 0.4622, 0.947 , 0.8947, 0.82  ,\n",
       "       0.9578, 0.8584, 0.9049, 0.928 , 0.8673, 0.9576, 0.5601, 0.8944,\n",
       "       0.8826, 0.8598, 0.7672, 0.9905, 0.8276, 0.5206, 0.6252, 0.9187,\n",
       "       0.9553, 0.8979, 0.8508, 0.8814, 0.7053, 0.9728, 0.986 , 0.8224,\n",
       "       0.4454, 0.8862, 0.4787, 0.6587, 0.3936, 0.8558, 0.971 , 0.8129,\n",
       "       0.492 , 0.9836, 0.5267, 0.7794, 0.8998, 0.6339, 0.8905, 0.9781,\n",
       "       0.8868, 0.839 , 0.752 , 0.8201, 0.8317, 0.9146, 0.85  , 0.7615,\n",
       "       0.8349, 0.6176, 0.8116, 0.4638, 0.6725, 0.7923, 0.8103, 0.7979,\n",
       "       0.8434, 0.8129, 0.5165, 0.9519, 0.8004, 0.5484, 0.5575, 0.8856,\n",
       "       0.495 , 0.9784, 0.8425, 0.7909, 0.8914, 0.9162, 0.7185, 0.3606,\n",
       "       0.8459, 0.8478, 0.838 , 0.7518, 0.7946, 0.8314, 0.794 , 0.9343,\n",
       "       0.6345, 0.9714, 0.9776, 0.8055, 0.8468, 0.839 , 0.8957, 0.9123,\n",
       "       0.8267, 0.8172, 0.9586, 0.8301, 0.9403, 0.6539, 0.8381, 0.876 ,\n",
       "       0.9865, 0.8284, 0.8691, 0.8118, 0.9663, 0.8569, 0.837 , 0.8241,\n",
       "       0.9702, 0.9687, 0.7049, 0.5329, 0.771 , 0.7072, 0.7441, 0.8497,\n",
       "       0.9142, 0.8135, 0.7129, 0.8343, 0.493 , 0.4611, 0.9686, 0.931 ,\n",
       "       0.6926, 0.9088, 0.5024, 0.8891, 0.8472, 0.8336, 0.9359, 0.4896,\n",
       "       0.5681, 0.8453, 0.9517, 0.972 , 0.8914, 0.5275, 0.8317, 0.9872,\n",
       "       0.7911, 0.5532, 0.7925, 0.9481, 0.8042, 0.8472, 0.8238, 0.8099,\n",
       "       0.6646, 0.5526, 0.7092, 0.8743, 0.897 , 0.8233, 0.5433, 0.8164,\n",
       "       1.    , 0.9282, 0.8306, 0.601 , 0.6061, 0.9714, 0.8618, 0.4595,\n",
       "       0.8484, 0.9499, 0.9618, 0.8188, 0.5149, 0.7545, 0.965 , 0.8165,\n",
       "       0.9245, 0.9855, 0.5892, 0.9571, 0.518 , 0.5141, 0.9866, 0.3936,\n",
       "       0.6322, 0.9322, 0.9486, 0.5101, 0.5274, 0.9283, 0.9928, 0.8372,\n",
       "       0.4825, 0.8069, 0.8942, 0.4294, 0.8618, 0.9096, 0.8718, 0.861 ,\n",
       "       0.7309, 0.7883, 0.7567, 0.8729, 0.8551, 0.8512, 0.9568, 0.3754,\n",
       "       0.9562, 0.7886, 0.8835, 0.9555, 0.7949, 0.8194, 0.7077, 0.7346,\n",
       "       0.562 , 0.9642, 0.9736, 0.7705, 0.5671, 0.6538, 0.8566, 0.7172,\n",
       "       0.8079, 0.8123, 0.9227, 0.7832, 0.721 , 0.4854, 0.8187, 0.9381,\n",
       "       0.9439, 0.5646, 0.5245, 0.8731, 0.9236, 0.8446, 0.9516, 0.5808,\n",
       "       0.9794, 0.657 , 0.4682, 0.6697, 0.7581, 0.8563, 0.9307, 0.8194,\n",
       "       0.9036, 0.839 , 0.8253, 0.8255, 0.9265, 0.8284, 0.8389, 0.8984,\n",
       "       0.8752, 0.8197, 0.9651, 0.8223, 0.9321, 0.4978, 0.8268, 0.8114,\n",
       "       0.6851, 0.9835, 0.8445, 0.7807, 0.79  , 0.693 , 0.7513, 0.7694,\n",
       "       0.3975, 0.9864, 0.9544, 0.8108, 0.8894, 1.    , 0.9602, 0.4342,\n",
       "       0.81  , 0.8682, 0.921 , 0.8256, 0.8352, 0.9382, 0.9571, 0.4904,\n",
       "       0.9056, 0.8875, 0.8953, 0.8405, 0.8277, 0.9627, 0.6909, 0.9364,\n",
       "       0.923 , 0.9899, 0.5258, 0.9239, 0.9378, 0.5884, 0.7519, 0.9126,\n",
       "       0.7879, 0.8172, 0.8629, 0.7746, 0.5632, 0.9632, 0.7737, 0.7781,\n",
       "       0.7013, 0.9388, 0.7819, 0.8731, 0.4331, 0.4948, 0.4142, 0.8446,\n",
       "       0.98  , 0.7081, 0.8869, 0.8061, 0.9453, 0.8528, 0.8872, 0.7299,\n",
       "       0.8858, 0.9054, 0.4229, 0.3724, 0.8066, 0.5116, 0.598 , 0.793 ,\n",
       "       0.816 , 0.5583, 0.9582, 0.9756, 0.908 , 0.4757, 0.7647, 0.4421,\n",
       "       0.9129, 0.694 , 0.9084, 0.9481, 0.7319, 0.7938, 0.7653, 0.8994,\n",
       "       0.4537, 0.8361, 0.9713, 0.8423, 0.8923, 0.8752, 0.7658, 0.8542,\n",
       "       0.5839, 0.9454, 0.5835, 0.6785, 0.4188, 0.6557, 0.8805, 0.9674,\n",
       "       0.926 , 0.9639, 0.9537, 0.9736, 0.9476, 0.9626, 0.7292, 0.7622,\n",
       "       0.4743, 0.5856, 0.8332, 0.816 , 0.8572, 0.8329, 0.6011, 0.8687,\n",
       "       0.9156, 0.8611, 0.5454, 0.85  , 0.9416, 0.8971, 0.965 , 0.8486,\n",
       "       0.8326, 0.8714, 0.801 , 0.8504, 0.9786, 0.8835, 0.885 , 0.8774,\n",
       "       0.6337, 0.7346, 0.5496, 0.8417, 0.5541, 0.9637, 0.8557, 0.838 ,\n",
       "       0.753 , 0.7203, 0.9391, 0.8126, 0.8251, 0.9271, 0.9147, 0.8937,\n",
       "       0.9793, 0.8286, 0.949 , 0.5109, 0.8692, 0.4825, 0.9528, 0.9149,\n",
       "       0.7944, 0.815 , 0.4973, 0.8809, 0.8193, 0.9545, 0.9572, 0.9677])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ec0c755-9040-492a-b66b-2f780b8f4119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16fae0af-3587-48c8-bf2c-2cf921f60a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean? - Do a sanity check\n",
    "np.mean(np.round(all_valid_true_mean, 8) == np.round(np.squeeze(valid_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3703d55f-ab32-4215-9b60-7c9261d7ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bts_mean = np.mean(all_valid_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ee27627-e1ea-4a2d-851e-593e48aadbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08a8356a-0907-45bc-ae41-8c6cdb11bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test data, get the bootstrap means and the variances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb65eb5e-2f20-48bb-82ff-d59a7e6673d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds_array = np.array(all_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b33983d-123b-4670-a308-9c81146da1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 951)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84219d32-7fa7-466d-97a6-6980d243216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_trues = np.array(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc73b13d-8449-4bd7-ac38-564e8e93ccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 951)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3090f78a-92ac-44b9-97af-30bff178dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_true_mean = np.mean(all_test_trues, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af45dfe4-f647-4c61-ab12-0c2b1fa6b241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_true_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c5515f0-d5b9-42fb-84d3-3a81a10f1cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8148, 0.8126, 0.4573, 0.4997, 0.7675, 0.4433, 0.8721, 0.7955,\n",
       "       0.7903, 0.8935, 0.7613, 0.7959, 0.826 , 0.9244, 0.473 , 0.9191,\n",
       "       0.8211, 0.9359, 0.8892, 0.8452, 0.6969, 0.8997, 0.5141, 0.9252,\n",
       "       0.8562, 0.8477, 1.    , 0.4335, 0.837 , 0.9478, 0.9332, 0.9241,\n",
       "       0.794 , 0.6029, 0.7139, 0.5935, 0.7085, 0.966 , 0.9012, 0.2722,\n",
       "       0.9527, 0.4557, 0.825 , 0.895 , 0.4876, 0.7888, 0.7641, 0.6534,\n",
       "       0.9904, 0.9333, 0.7537, 0.9372, 0.9884, 0.9022, 0.891 , 0.2615,\n",
       "       0.3785, 0.7665, 0.5943, 0.9283, 0.8238, 0.5041, 0.6022, 0.7699,\n",
       "       0.7899, 0.6179, 0.9872, 0.9453, 0.8636, 0.9912, 0.7533, 0.7133,\n",
       "       0.6643, 0.4047, 0.6888, 0.7082, 0.7348, 0.666 , 0.8022, 0.9126,\n",
       "       0.9426, 0.4981, 0.8327, 0.975 , 0.7054, 0.5137, 0.5671, 0.9201,\n",
       "       0.8093, 0.8352, 0.7234, 0.9159, 0.9925, 0.7912, 0.9701, 0.3488,\n",
       "       0.8331, 0.965 , 0.6526, 0.8677, 0.6111, 0.9516, 0.7942, 0.8191,\n",
       "       0.8036, 0.6477, 0.7882, 0.8717, 0.9719, 0.8172, 0.87  , 0.8223,\n",
       "       0.8245, 0.9379, 0.7402, 0.43  , 0.7072, 0.6113, 0.7899, 0.8034,\n",
       "       0.9317, 0.9204, 0.9408, 0.9396, 0.922 , 0.8454, 1.    , 0.9255,\n",
       "       0.8123, 0.4218, 0.9216, 0.8405, 0.9305, 0.7632, 0.6173, 0.859 ,\n",
       "       0.8543, 0.5712, 0.4713, 0.691 , 0.8325, 0.406 , 0.3624, 0.8793,\n",
       "       0.6205, 1.    , 0.8205, 0.7814, 0.4623, 0.6556, 0.9809, 0.9452,\n",
       "       0.5062, 0.7415, 0.8776, 0.5927, 0.9405, 0.7986, 0.538 , 0.6692,\n",
       "       0.8253, 0.5152, 1.    , 0.7293, 0.7153, 0.7353, 0.395 , 0.9055,\n",
       "       0.8634, 0.938 , 0.9086, 0.793 , 0.7308, 0.9456, 0.8765, 0.7956,\n",
       "       0.9233, 0.7863, 0.8011, 0.8172, 0.7001, 0.4185, 0.9125, 0.657 ,\n",
       "       0.7844, 0.7858, 0.9843, 0.8435, 0.9186, 0.8668, 0.8239, 0.9545,\n",
       "       0.9137, 0.8673, 0.3455, 0.9118, 0.9671, 1.    , 0.8423, 0.9559,\n",
       "       0.8292, 0.7504, 0.8152, 0.8198, 0.8615, 0.9186, 0.5795, 0.7996,\n",
       "       0.9344, 0.9243, 0.9003, 0.8143, 0.8735, 0.8516, 0.6079, 0.5412,\n",
       "       0.791 , 0.8518, 0.7682, 0.9133, 0.9532, 0.8165, 0.81  , 0.7876,\n",
       "       0.8594, 0.7218, 0.8076, 0.6264, 0.5856, 0.7098, 0.4804, 0.7762,\n",
       "       0.4751, 0.9672, 0.8186, 0.8975, 0.8276, 0.826 , 0.5218, 0.5575,\n",
       "       0.8201, 0.478 , 0.8878, 0.8197, 0.6192, 0.9465, 0.4269, 0.9221,\n",
       "       0.8902, 0.8322, 0.7326, 0.954 , 0.8783, 0.725 , 0.5515, 0.6415,\n",
       "       0.4396, 0.7871, 0.8348, 0.7845, 0.7992, 0.7936, 0.954 , 0.8905,\n",
       "       0.7058, 0.9736, 0.8609, 0.8761, 0.8997, 0.7274, 0.9644, 0.9152,\n",
       "       0.7017, 0.9395, 0.5845, 0.455 , 0.4118, 0.7396, 0.8449, 0.7417,\n",
       "       0.7829, 0.7851, 0.6798, 0.8225, 0.8251, 0.8033, 0.9366, 0.9792,\n",
       "       0.7241, 0.9332, 0.8361, 0.7992, 0.8951, 0.9346, 0.8332, 0.841 ,\n",
       "       0.8525, 0.8178, 0.963 , 0.8423, 0.7717, 0.5567, 0.9319, 0.6527,\n",
       "       0.8712, 0.7556, 0.8579, 0.6322, 0.8229, 0.4641, 0.9711, 0.9284,\n",
       "       0.8087, 0.9339, 0.7637, 0.8964, 0.912 , 0.6088, 0.8417, 0.472 ,\n",
       "       0.8412, 0.7995, 0.7991, 0.7565, 0.7682, 0.9072, 0.9882, 0.8102,\n",
       "       0.9708, 0.836 , 0.79  , 0.6967, 0.9255, 0.8131, 0.7416, 0.8053,\n",
       "       0.9394, 0.9778, 0.8771, 0.8325, 0.8626, 0.7989, 0.8331, 0.9661,\n",
       "       0.0822, 0.7328, 0.9743, 0.9461, 0.4552, 0.9209, 0.8158, 0.9117,\n",
       "       0.6215, 0.574 , 0.9368, 0.8618, 0.7727, 0.8247, 0.9352, 0.8224,\n",
       "       0.5286, 0.824 , 0.816 , 0.4362, 0.5432, 0.9334, 0.5728, 0.6731,\n",
       "       0.8843, 0.842 , 0.3896, 0.7487, 0.8106, 0.6714, 0.8028, 0.9021,\n",
       "       0.8751, 0.5655, 0.8528, 0.8481, 0.9665, 0.5628, 0.5189, 0.7062,\n",
       "       0.9924, 0.7797, 0.6875, 0.4012, 0.8092, 0.9706, 0.8786, 0.4795,\n",
       "       0.5763, 0.7142, 0.9253, 0.7052, 0.9609, 0.6207, 0.8622, 0.7936,\n",
       "       0.8843, 0.5741, 0.9403, 0.8343, 0.4801, 0.9682, 0.8685, 0.8219,\n",
       "       0.5802, 0.744 , 0.6782, 0.9063, 0.9449, 0.5639, 0.8801, 0.9357,\n",
       "       0.9436, 0.8358, 0.869 , 0.933 , 0.9019, 0.7928, 0.6943, 0.9537,\n",
       "       0.5251, 0.9   , 0.9719, 0.8564, 0.6176, 0.7719, 0.961 , 0.7835,\n",
       "       0.8194, 0.758 , 0.8361, 0.6321, 0.9523, 0.8869, 0.4643, 0.2995,\n",
       "       0.9207, 0.9684, 0.8081, 0.913 , 0.8308, 0.8292, 0.4156, 0.8412,\n",
       "       0.8214, 0.8308, 0.8359, 0.842 , 0.7377, 0.8592, 0.9344, 0.9134,\n",
       "       0.9904, 0.8325, 0.8115, 0.6716, 0.8259, 0.7722, 0.7834, 0.8004,\n",
       "       0.4771, 0.6645, 0.7769, 0.4619, 0.7208, 0.8772, 0.878 , 0.8875,\n",
       "       0.9264, 0.815 , 0.5858, 0.8255, 0.8673, 0.5824, 0.7655, 0.9906,\n",
       "       0.8699, 0.6147, 0.6176, 0.9409, 0.9543, 0.807 , 0.3464, 0.6713,\n",
       "       0.8609, 0.8583, 0.8174, 0.8575, 0.8074, 0.3189, 0.9399, 0.9886,\n",
       "       0.6855, 0.3968, 0.9772, 0.823 , 0.608 , 0.8718, 0.8317, 0.9545,\n",
       "       0.9581, 0.9141, 0.9018, 0.7923, 0.7841, 0.8411, 0.4299, 0.3869,\n",
       "       0.7301, 0.8737, 0.902 , 0.869 , 0.7921, 0.7968, 0.8687, 0.7927,\n",
       "       0.9907, 0.7543, 0.9588, 0.8292, 0.9729, 1.    , 0.9537, 0.9842,\n",
       "       0.7381, 0.8791, 0.7557, 0.8447, 0.8534, 0.8492, 0.6851, 0.8525,\n",
       "       0.773 , 0.9897, 0.9179, 0.576 , 0.3474, 0.8548, 0.9237, 0.8501,\n",
       "       0.3295, 0.7208, 0.7332, 0.9969, 0.8925, 0.8438, 0.8486, 0.843 ,\n",
       "       0.8319, 0.6188, 0.9519, 0.7927, 0.8261, 0.7426, 0.3844, 0.8308,\n",
       "       0.5546, 0.9798, 0.8961, 0.7072, 0.5149, 0.6097, 0.2418, 0.5251,\n",
       "       0.8446, 0.866 , 0.8425, 0.447 , 0.4644, 0.7995, 0.7903, 0.9681,\n",
       "       0.9596, 0.6329, 0.4361, 0.9781, 0.4707, 0.9944, 0.8332, 0.8673,\n",
       "       0.6773, 0.9276, 0.761 , 0.8568, 0.6868, 0.8935, 0.8412, 0.972 ,\n",
       "       0.959 , 0.798 , 0.9854, 0.8333, 0.9426, 0.6093, 0.9673, 0.9554,\n",
       "       0.7861, 0.946 , 0.9714, 0.8177, 0.8656, 0.9717, 0.7929, 0.7851,\n",
       "       0.8714, 0.9574, 0.8749, 0.7226, 0.8101, 0.8484, 0.5085, 0.3265,\n",
       "       0.5043, 0.8758, 0.3711, 0.2753, 0.6054, 0.484 , 0.2959, 0.9074,\n",
       "       0.7218, 0.9563, 0.7803, 0.9798, 0.5945, 0.7783, 0.5266, 0.8446,\n",
       "       0.5826, 0.7528, 0.9105, 0.8305, 0.899 , 0.6063, 0.838 , 0.8915,\n",
       "       0.8752, 0.508 , 0.8635, 0.7727, 0.8569, 0.3732, 0.4218, 0.7111,\n",
       "       0.7307, 0.6552, 0.9218, 0.7804, 0.7459, 0.3688, 0.4951, 0.9515,\n",
       "       0.9787, 0.9555, 0.8761, 0.8334, 0.7833, 0.8068, 0.6106, 0.7274,\n",
       "       0.8011, 0.8401, 0.6335, 0.9475, 0.7874, 0.8239, 0.9809, 0.2354,\n",
       "       0.817 , 0.7658, 0.8073, 0.8343, 1.    , 0.823 , 0.9794, 0.8067,\n",
       "       0.4131, 0.5266, 0.8428, 0.9677, 0.9472, 0.8702, 1.    , 0.5807,\n",
       "       0.79  , 0.9741, 0.8143, 0.7805, 0.9767, 0.761 , 0.9545, 0.8002,\n",
       "       0.7801, 0.5475, 0.8245, 0.9022, 0.5541, 0.9888, 0.8146, 0.8795,\n",
       "       0.8944, 0.8593, 0.5549, 0.7404, 0.6298, 0.7837, 0.8612, 0.558 ,\n",
       "       0.855 , 0.85  , 0.9091, 0.5004, 0.8012, 0.3992, 0.849 , 0.9787,\n",
       "       0.8499, 0.9444, 0.7334, 0.8253, 0.5071, 0.7172, 0.7818, 0.4029,\n",
       "       0.6764, 0.8671, 0.948 , 0.7075, 0.7421, 0.8268, 0.8513, 0.9629,\n",
       "       0.9704, 0.8366, 0.8785, 0.6028, 0.8422, 0.9699, 0.777 , 0.9144,\n",
       "       0.9536, 0.8705, 0.8136, 0.4747, 0.8553, 0.7788, 0.6258, 0.7957,\n",
       "       0.8114, 0.8463, 0.983 , 0.954 , 0.7611, 0.4208, 0.8332, 0.9479,\n",
       "       0.7873, 0.6444, 0.7532, 0.8223, 0.8975, 0.7996, 0.9587, 0.8384,\n",
       "       0.9285, 0.3788, 0.5926, 0.9171, 0.8806, 0.6913, 0.8042, 0.7352,\n",
       "       0.7691, 0.8637, 0.6337, 0.8118, 0.8073, 0.3835, 0.8689, 0.9014,\n",
       "       0.9257, 0.8434, 0.9489, 0.8137, 0.8314, 0.8612, 0.9578, 0.9233,\n",
       "       0.8675, 0.5389, 0.7268, 0.8073, 0.8352, 0.5259, 0.5041, 0.4667,\n",
       "       0.8794, 0.8343, 0.931 , 0.9407, 0.6616, 0.9323, 0.5715, 0.7952,\n",
       "       0.5574, 0.7445, 0.7044, 0.9425, 0.8864, 0.5775, 0.9057, 0.9922,\n",
       "       0.8072, 0.8315, 0.7843, 0.9876, 0.8631, 0.5926, 0.7452, 0.9394,\n",
       "       0.7913, 0.959 , 0.8004, 0.9228, 0.7581, 0.8625, 0.9182, 0.7613,\n",
       "       0.7749, 0.8116, 0.5452, 0.5928, 0.8386, 0.7518, 0.8238, 0.9747,\n",
       "       0.9519, 0.8326, 0.7912, 0.7665, 0.8208, 0.6282, 0.8372, 0.7561,\n",
       "       0.8655, 0.8203, 0.9098, 0.6533, 0.8518, 0.4984, 0.753 , 0.5066,\n",
       "       0.909 , 0.568 , 0.3162, 0.9843, 0.465 , 0.5995, 0.9033, 0.5881,\n",
       "       0.8073, 0.8422, 0.8053, 0.8991, 0.9491, 0.9857, 0.3735, 0.9314,\n",
       "       0.9458, 0.8212, 0.9929, 0.8317, 0.9301, 0.8223, 0.8245, 0.9286,\n",
       "       0.8368, 0.8099, 0.8532, 0.8472, 0.9305, 0.9399, 0.8057, 0.9406,\n",
       "       0.9675, 0.6636, 0.7091, 0.6619, 0.9661, 0.8909, 0.8986, 0.8727,\n",
       "       0.9562, 0.9721, 0.8343, 0.7693, 0.8625, 0.8198, 0.9529, 0.7498,\n",
       "       0.7155, 0.9423, 0.7888, 0.8053, 0.8194, 0.4455, 0.7934, 0.9561,\n",
       "       0.7579, 0.8486, 0.7809, 0.7741, 0.8416, 0.9806, 0.7456, 0.7692,\n",
       "       0.6647, 0.8083, 0.9798, 0.7817, 0.9971, 0.9756, 0.1548, 0.8402,\n",
       "       0.4392, 0.8605, 0.5227, 0.995 , 0.8516, 0.5638, 0.8762, 0.8045,\n",
       "       0.7707, 1.    , 0.7114, 0.9216, 0.8483, 0.9028, 0.6643, 0.9519,\n",
       "       0.6967, 0.9659, 0.3855, 0.6438, 0.9371, 0.9529, 0.8819])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72e42fb7-1990-4593-a532-b49f8f55a455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8148, 0.8126, 0.4573, 0.4997, 0.7675, 0.4433, 0.8721, 0.7955,\n",
       "       0.7903, 0.8935, 0.7613, 0.7959, 0.826 , 0.9244, 0.473 , 0.9191,\n",
       "       0.8211, 0.9359, 0.8892, 0.8452, 0.6969, 0.8997, 0.5141, 0.9252,\n",
       "       0.8562, 0.8477, 1.    , 0.4335, 0.837 , 0.9478, 0.9332, 0.9241,\n",
       "       0.794 , 0.6029, 0.7139, 0.5935, 0.7085, 0.966 , 0.9012, 0.2722,\n",
       "       0.9527, 0.4557, 0.825 , 0.895 , 0.4876, 0.7888, 0.7641, 0.6534,\n",
       "       0.9904, 0.9333, 0.7537, 0.9372, 0.9884, 0.9022, 0.891 , 0.2615,\n",
       "       0.3785, 0.7665, 0.5943, 0.9283, 0.8238, 0.5041, 0.6022, 0.7699,\n",
       "       0.7899, 0.6179, 0.9872, 0.9453, 0.8636, 0.9912, 0.7533, 0.7133,\n",
       "       0.6643, 0.4047, 0.6888, 0.7082, 0.7348, 0.666 , 0.8022, 0.9126,\n",
       "       0.9426, 0.4981, 0.8327, 0.975 , 0.7054, 0.5137, 0.5671, 0.9201,\n",
       "       0.8093, 0.8352, 0.7234, 0.9159, 0.9925, 0.7912, 0.9701, 0.3488,\n",
       "       0.8331, 0.965 , 0.6526, 0.8677, 0.6111, 0.9516, 0.7942, 0.8191,\n",
       "       0.8036, 0.6477, 0.7882, 0.8717, 0.9719, 0.8172, 0.87  , 0.8223,\n",
       "       0.8245, 0.9379, 0.7402, 0.43  , 0.7072, 0.6113, 0.7899, 0.8034,\n",
       "       0.9317, 0.9204, 0.9408, 0.9396, 0.922 , 0.8454, 1.    , 0.9255,\n",
       "       0.8123, 0.4218, 0.9216, 0.8405, 0.9305, 0.7632, 0.6173, 0.859 ,\n",
       "       0.8543, 0.5712, 0.4713, 0.691 , 0.8325, 0.406 , 0.3624, 0.8793,\n",
       "       0.6205, 1.    , 0.8205, 0.7814, 0.4623, 0.6556, 0.9809, 0.9452,\n",
       "       0.5062, 0.7415, 0.8776, 0.5927, 0.9405, 0.7986, 0.538 , 0.6692,\n",
       "       0.8253, 0.5152, 1.    , 0.7293, 0.7153, 0.7353, 0.395 , 0.9055,\n",
       "       0.8634, 0.938 , 0.9086, 0.793 , 0.7308, 0.9456, 0.8765, 0.7956,\n",
       "       0.9233, 0.7863, 0.8011, 0.8172, 0.7001, 0.4185, 0.9125, 0.657 ,\n",
       "       0.7844, 0.7858, 0.9843, 0.8435, 0.9186, 0.8668, 0.8239, 0.9545,\n",
       "       0.9137, 0.8673, 0.3455, 0.9118, 0.9671, 1.    , 0.8423, 0.9559,\n",
       "       0.8292, 0.7504, 0.8152, 0.8198, 0.8615, 0.9186, 0.5795, 0.7996,\n",
       "       0.9344, 0.9243, 0.9003, 0.8143, 0.8735, 0.8516, 0.6079, 0.5412,\n",
       "       0.791 , 0.8518, 0.7682, 0.9133, 0.9532, 0.8165, 0.81  , 0.7876,\n",
       "       0.8594, 0.7218, 0.8076, 0.6264, 0.5856, 0.7098, 0.4804, 0.7762,\n",
       "       0.4751, 0.9672, 0.8186, 0.8975, 0.8276, 0.826 , 0.5218, 0.5575,\n",
       "       0.8201, 0.478 , 0.8878, 0.8197, 0.6192, 0.9465, 0.4269, 0.9221,\n",
       "       0.8902, 0.8322, 0.7326, 0.954 , 0.8783, 0.725 , 0.5515, 0.6415,\n",
       "       0.4396, 0.7871, 0.8348, 0.7845, 0.7992, 0.7936, 0.954 , 0.8905,\n",
       "       0.7058, 0.9736, 0.8609, 0.8761, 0.8997, 0.7274, 0.9644, 0.9152,\n",
       "       0.7017, 0.9395, 0.5845, 0.455 , 0.4118, 0.7396, 0.8449, 0.7417,\n",
       "       0.7829, 0.7851, 0.6798, 0.8225, 0.8251, 0.8033, 0.9366, 0.9792,\n",
       "       0.7241, 0.9332, 0.8361, 0.7992, 0.8951, 0.9346, 0.8332, 0.841 ,\n",
       "       0.8525, 0.8178, 0.963 , 0.8423, 0.7717, 0.5567, 0.9319, 0.6527,\n",
       "       0.8712, 0.7556, 0.8579, 0.6322, 0.8229, 0.4641, 0.9711, 0.9284,\n",
       "       0.8087, 0.9339, 0.7637, 0.8964, 0.912 , 0.6088, 0.8417, 0.472 ,\n",
       "       0.8412, 0.7995, 0.7991, 0.7565, 0.7682, 0.9072, 0.9882, 0.8102,\n",
       "       0.9708, 0.836 , 0.79  , 0.6967, 0.9255, 0.8131, 0.7416, 0.8053,\n",
       "       0.9394, 0.9778, 0.8771, 0.8325, 0.8626, 0.7989, 0.8331, 0.9661,\n",
       "       0.0822, 0.7328, 0.9743, 0.9461, 0.4552, 0.9209, 0.8158, 0.9117,\n",
       "       0.6215, 0.574 , 0.9368, 0.8618, 0.7727, 0.8247, 0.9352, 0.8224,\n",
       "       0.5286, 0.824 , 0.816 , 0.4362, 0.5432, 0.9334, 0.5728, 0.6731,\n",
       "       0.8843, 0.842 , 0.3896, 0.7487, 0.8106, 0.6714, 0.8028, 0.9021,\n",
       "       0.8751, 0.5655, 0.8528, 0.8481, 0.9665, 0.5628, 0.5189, 0.7062,\n",
       "       0.9924, 0.7797, 0.6875, 0.4012, 0.8092, 0.9706, 0.8786, 0.4795,\n",
       "       0.5763, 0.7142, 0.9253, 0.7052, 0.9609, 0.6207, 0.8622, 0.7936,\n",
       "       0.8843, 0.5741, 0.9403, 0.8343, 0.4801, 0.9682, 0.8685, 0.8219,\n",
       "       0.5802, 0.744 , 0.6782, 0.9063, 0.9449, 0.5639, 0.8801, 0.9357,\n",
       "       0.9436, 0.8358, 0.869 , 0.933 , 0.9019, 0.7928, 0.6943, 0.9537,\n",
       "       0.5251, 0.9   , 0.9719, 0.8564, 0.6176, 0.7719, 0.961 , 0.7835,\n",
       "       0.8194, 0.758 , 0.8361, 0.6321, 0.9523, 0.8869, 0.4643, 0.2995,\n",
       "       0.9207, 0.9684, 0.8081, 0.913 , 0.8308, 0.8292, 0.4156, 0.8412,\n",
       "       0.8214, 0.8308, 0.8359, 0.842 , 0.7377, 0.8592, 0.9344, 0.9134,\n",
       "       0.9904, 0.8325, 0.8115, 0.6716, 0.8259, 0.7722, 0.7834, 0.8004,\n",
       "       0.4771, 0.6645, 0.7769, 0.4619, 0.7208, 0.8772, 0.878 , 0.8875,\n",
       "       0.9264, 0.815 , 0.5858, 0.8255, 0.8673, 0.5824, 0.7655, 0.9906,\n",
       "       0.8699, 0.6147, 0.6176, 0.9409, 0.9543, 0.807 , 0.3464, 0.6713,\n",
       "       0.8609, 0.8583, 0.8174, 0.8575, 0.8074, 0.3189, 0.9399, 0.9886,\n",
       "       0.6855, 0.3968, 0.9772, 0.823 , 0.608 , 0.8718, 0.8317, 0.9545,\n",
       "       0.9581, 0.9141, 0.9018, 0.7923, 0.7841, 0.8411, 0.4299, 0.3869,\n",
       "       0.7301, 0.8737, 0.902 , 0.869 , 0.7921, 0.7968, 0.8687, 0.7927,\n",
       "       0.9907, 0.7543, 0.9588, 0.8292, 0.9729, 1.    , 0.9537, 0.9842,\n",
       "       0.7381, 0.8791, 0.7557, 0.8447, 0.8534, 0.8492, 0.6851, 0.8525,\n",
       "       0.773 , 0.9897, 0.9179, 0.576 , 0.3474, 0.8548, 0.9237, 0.8501,\n",
       "       0.3295, 0.7208, 0.7332, 0.9969, 0.8925, 0.8438, 0.8486, 0.843 ,\n",
       "       0.8319, 0.6188, 0.9519, 0.7927, 0.8261, 0.7426, 0.3844, 0.8308,\n",
       "       0.5546, 0.9798, 0.8961, 0.7072, 0.5149, 0.6097, 0.2418, 0.5251,\n",
       "       0.8446, 0.866 , 0.8425, 0.447 , 0.4644, 0.7995, 0.7903, 0.9681,\n",
       "       0.9596, 0.6329, 0.4361, 0.9781, 0.4707, 0.9944, 0.8332, 0.8673,\n",
       "       0.6773, 0.9276, 0.761 , 0.8568, 0.6868, 0.8935, 0.8412, 0.972 ,\n",
       "       0.959 , 0.798 , 0.9854, 0.8333, 0.9426, 0.6093, 0.9673, 0.9554,\n",
       "       0.7861, 0.946 , 0.9714, 0.8177, 0.8656, 0.9717, 0.7929, 0.7851,\n",
       "       0.8714, 0.9574, 0.8749, 0.7226, 0.8101, 0.8484, 0.5085, 0.3265,\n",
       "       0.5043, 0.8758, 0.3711, 0.2753, 0.6054, 0.484 , 0.2959, 0.9074,\n",
       "       0.7218, 0.9563, 0.7803, 0.9798, 0.5945, 0.7783, 0.5266, 0.8446,\n",
       "       0.5826, 0.7528, 0.9105, 0.8305, 0.899 , 0.6063, 0.838 , 0.8915,\n",
       "       0.8752, 0.508 , 0.8635, 0.7727, 0.8569, 0.3732, 0.4218, 0.7111,\n",
       "       0.7307, 0.6552, 0.9218, 0.7804, 0.7459, 0.3688, 0.4951, 0.9515,\n",
       "       0.9787, 0.9555, 0.8761, 0.8334, 0.7833, 0.8068, 0.6106, 0.7274,\n",
       "       0.8011, 0.8401, 0.6335, 0.9475, 0.7874, 0.8239, 0.9809, 0.2354,\n",
       "       0.817 , 0.7658, 0.8073, 0.8343, 1.    , 0.823 , 0.9794, 0.8067,\n",
       "       0.4131, 0.5266, 0.8428, 0.9677, 0.9472, 0.8702, 1.    , 0.5807,\n",
       "       0.79  , 0.9741, 0.8143, 0.7805, 0.9767, 0.761 , 0.9545, 0.8002,\n",
       "       0.7801, 0.5475, 0.8245, 0.9022, 0.5541, 0.9888, 0.8146, 0.8795,\n",
       "       0.8944, 0.8593, 0.5549, 0.7404, 0.6298, 0.7837, 0.8612, 0.558 ,\n",
       "       0.855 , 0.85  , 0.9091, 0.5004, 0.8012, 0.3992, 0.849 , 0.9787,\n",
       "       0.8499, 0.9444, 0.7334, 0.8253, 0.5071, 0.7172, 0.7818, 0.4029,\n",
       "       0.6764, 0.8671, 0.948 , 0.7075, 0.7421, 0.8268, 0.8513, 0.9629,\n",
       "       0.9704, 0.8366, 0.8785, 0.6028, 0.8422, 0.9699, 0.777 , 0.9144,\n",
       "       0.9536, 0.8705, 0.8136, 0.4747, 0.8553, 0.7788, 0.6258, 0.7957,\n",
       "       0.8114, 0.8463, 0.983 , 0.954 , 0.7611, 0.4208, 0.8332, 0.9479,\n",
       "       0.7873, 0.6444, 0.7532, 0.8223, 0.8975, 0.7996, 0.9587, 0.8384,\n",
       "       0.9285, 0.3788, 0.5926, 0.9171, 0.8806, 0.6913, 0.8042, 0.7352,\n",
       "       0.7691, 0.8637, 0.6337, 0.8118, 0.8073, 0.3835, 0.8689, 0.9014,\n",
       "       0.9257, 0.8434, 0.9489, 0.8137, 0.8314, 0.8612, 0.9578, 0.9233,\n",
       "       0.8675, 0.5389, 0.7268, 0.8073, 0.8352, 0.5259, 0.5041, 0.4667,\n",
       "       0.8794, 0.8343, 0.931 , 0.9407, 0.6616, 0.9323, 0.5715, 0.7952,\n",
       "       0.5574, 0.7445, 0.7044, 0.9425, 0.8864, 0.5775, 0.9057, 0.9922,\n",
       "       0.8072, 0.8315, 0.7843, 0.9876, 0.8631, 0.5926, 0.7452, 0.9394,\n",
       "       0.7913, 0.959 , 0.8004, 0.9228, 0.7581, 0.8625, 0.9182, 0.7613,\n",
       "       0.7749, 0.8116, 0.5452, 0.5928, 0.8386, 0.7518, 0.8238, 0.9747,\n",
       "       0.9519, 0.8326, 0.7912, 0.7665, 0.8208, 0.6282, 0.8372, 0.7561,\n",
       "       0.8655, 0.8203, 0.9098, 0.6533, 0.8518, 0.4984, 0.753 , 0.5066,\n",
       "       0.909 , 0.568 , 0.3162, 0.9843, 0.465 , 0.5995, 0.9033, 0.5881,\n",
       "       0.8073, 0.8422, 0.8053, 0.8991, 0.9491, 0.9857, 0.3735, 0.9314,\n",
       "       0.9458, 0.8212, 0.9929, 0.8317, 0.9301, 0.8223, 0.8245, 0.9286,\n",
       "       0.8368, 0.8099, 0.8532, 0.8472, 0.9305, 0.9399, 0.8057, 0.9406,\n",
       "       0.9675, 0.6636, 0.7091, 0.6619, 0.9661, 0.8909, 0.8986, 0.8727,\n",
       "       0.9562, 0.9721, 0.8343, 0.7693, 0.8625, 0.8198, 0.9529, 0.7498,\n",
       "       0.7155, 0.9423, 0.7888, 0.8053, 0.8194, 0.4455, 0.7934, 0.9561,\n",
       "       0.7579, 0.8486, 0.7809, 0.7741, 0.8416, 0.9806, 0.7456, 0.7692,\n",
       "       0.6647, 0.8083, 0.9798, 0.7817, 0.9971, 0.9756, 0.1548, 0.8402,\n",
       "       0.4392, 0.8605, 0.5227, 0.995 , 0.8516, 0.5638, 0.8762, 0.8045,\n",
       "       0.7707, 1.    , 0.7114, 0.9216, 0.8483, 0.9028, 0.6643, 0.9519,\n",
       "       0.6967, 0.9659, 0.3855, 0.6438, 0.9371, 0.9529, 0.8819])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(test_keep[\"AUC\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "203a1df4-23e4-4095-beb4-44ca32d10ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(test_keep[\"AUC\"].values.reshape(-1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fffb86c5-c265-4a6b-99cd-9b9296b8a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these do look the same, should we do an np.mean?\n",
    "np.mean(np.round(all_test_true_mean, 8) == np.round(np.squeeze(test_keep[\"AUC\"].values.reshape(-1,1)), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af17b0cd-9285-497b-a64b-4e77157558d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first compute the bootstrap means\n",
    "\n",
    "test_bts_mean = np.mean(all_test_preds_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d045377-576c-4486-b580-195f39e316e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c95dc57e-0626-41a1-ab21-27abe7b5ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the bootstrap variance\n",
    "\n",
    "# let's use the same function as earlier - we cannot use this as is, as what we have now is a 2D array, and not a 3D one\n",
    "def equation_6_model_variance(all_preds):\n",
    "    all_vars = []\n",
    "    for i in range(all_preds.shape[1]):\n",
    "        var = (1/(all_preds.shape[0]  - 1))*np.sum(np.square(all_preds[:,i] - np.mean(all_preds[:,i])))\n",
    "        all_vars.append(var)\n",
    "\n",
    "    return np.array(all_vars, dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8ca45d4-17fb-4380-b04d-b3af9ccb3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bts_variance = equation_6_model_variance(all_train_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82d76c69-0e56-4893-b1c9-c2476fd290bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9eda79c8-9ecd-4731-8f40-fe336fe63eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0003b09-b3b5-4972-b094-9a5c18a40a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_train_bts_variance = np.var(all_train_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2567e931-3bc6-4f4a-9142-6aff6a0975d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_train_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1f42532-cd0f-4c2f-97f9-f6f17c781184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00183184, 0.00109681, 0.00047159, ..., 0.00504019, 0.00060388,\n",
       "       0.0024684 ], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_train_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b822ceb3-9872-474f-9d1e-17b5eff2bbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.round(train_bts_variance, 6) == np.round(alt_train_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5dd12901-5676-4646-b1a4-71054ceb1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variances on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3d45949-03ce-4dbe-8bef-3f86149bc6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bts_variance = equation_6_model_variance(all_valid_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26e4c529-03e1-459c-95f9-1d04e6669f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e898c2c-a195-433c-a5c9-fe751566551b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111437, 0.00215758, 0.0016149 , 0.00110854, 0.00177893,\n",
       "       0.0010395 , 0.00045934, 0.00218281, 0.00072607, 0.00161189,\n",
       "       0.00129221, 0.00064204, 0.00067966, 0.00121964, 0.00054192,\n",
       "       0.00165475, 0.00982931, 0.00098922, 0.00127981, 0.00106769,\n",
       "       0.00047407, 0.00046066, 0.0012382 , 0.00408663, 0.004013  ,\n",
       "       0.00104843, 0.00157014, 0.00091257, 0.00140339, 0.00090362,\n",
       "       0.00096479, 0.00230562, 0.00060487, 0.0004463 , 0.00104181,\n",
       "       0.00054409, 0.00093233, 0.00048041, 0.0004866 , 0.00109376,\n",
       "       0.00160849, 0.00097167, 0.00090513, 0.00222176, 0.00079927,\n",
       "       0.00217688, 0.00070226, 0.0027635 , 0.00018478, 0.00072723,\n",
       "       0.00147327, 0.00253314, 0.00259756, 0.00222062, 0.00460852,\n",
       "       0.00138656, 0.00166871, 0.00168314, 0.00151581, 0.00082051,\n",
       "       0.00081417, 0.00152035, 0.00177849, 0.00065383, 0.00053532,\n",
       "       0.00061589, 0.00129038, 0.00253621, 0.0022012 , 0.00195072,\n",
       "       0.00133277, 0.00134491, 0.00129875, 0.00174996, 0.0011459 ,\n",
       "       0.0016283 , 0.00098838, 0.00261783, 0.00130107, 0.0007612 ,\n",
       "       0.00066536, 0.00308185, 0.0010137 , 0.00149181, 0.00068181,\n",
       "       0.001028  , 0.00209949, 0.0017831 , 0.00148183, 0.00159291,\n",
       "       0.00149922, 0.00026098, 0.00127723, 0.00040533, 0.00021145,\n",
       "       0.00054627, 0.00022489, 0.00101377, 0.0025006 , 0.00242447,\n",
       "       0.00056596, 0.00098948, 0.0004143 , 0.00014458, 0.00220332,\n",
       "       0.00259521, 0.00050684, 0.00129521, 0.00170243, 0.00187751,\n",
       "       0.00129532, 0.00105263, 0.00152395, 0.00137774, 0.00408934,\n",
       "       0.00160638, 0.00430861, 0.00097025, 0.00076396, 0.0006169 ,\n",
       "       0.00062589, 0.00089974, 0.00426047, 0.00072227, 0.00065956,\n",
       "       0.00127007, 0.00068025, 0.00164218, 0.00101867, 0.00138354,\n",
       "       0.00087166, 0.00106146, 0.00060696, 0.00114642, 0.00108146,\n",
       "       0.0023288 , 0.0006369 , 0.00110827, 0.0022268 , 0.01084748,\n",
       "       0.00207236, 0.00049356, 0.00092279, 0.00013559, 0.00316686,\n",
       "       0.00062504, 0.00087341, 0.00054916, 0.00118682, 0.00340552,\n",
       "       0.00485832, 0.00202214, 0.00092711, 0.00157767, 0.00171194,\n",
       "       0.00113766, 0.00145199, 0.0005724 , 0.00087721, 0.00181209,\n",
       "       0.00124976, 0.00465682, 0.00312604, 0.000772  , 0.00155951,\n",
       "       0.00045681, 0.0007997 , 0.00066409, 0.0012055 , 0.00082506,\n",
       "       0.00147647, 0.00064606, 0.00080337, 0.00082253, 0.00049905,\n",
       "       0.00010724, 0.00094698, 0.00051625, 0.00163088, 0.00047667,\n",
       "       0.00061257, 0.00085065, 0.00105394, 0.00115474, 0.00133143,\n",
       "       0.00188529, 0.00119353, 0.0006532 , 0.00139154, 0.00065593,\n",
       "       0.00051322, 0.00491763, 0.00057768, 0.00093863, 0.00084872,\n",
       "       0.0028553 , 0.00160565, 0.0007308 , 0.00075435, 0.00115496,\n",
       "       0.0050956 , 0.00139922, 0.00062893, 0.00159818, 0.00077946,\n",
       "       0.00062139, 0.00099101, 0.00031926, 0.00093404, 0.00094885,\n",
       "       0.00082847, 0.00099562, 0.00397389, 0.00112208, 0.00067516,\n",
       "       0.0017704 , 0.0006182 , 0.00278593, 0.00032323, 0.00029274,\n",
       "       0.00067467, 0.00026727, 0.00149579, 0.00545491, 0.00100247,\n",
       "       0.00075177, 0.00395448, 0.00033377, 0.00092599, 0.0010032 ,\n",
       "       0.00090203, 0.0004061 , 0.00035504, 0.00089337, 0.00038512,\n",
       "       0.00105838, 0.00231034, 0.00077672, 0.00140492, 0.00091546,\n",
       "       0.00093142, 0.0010422 , 0.00063638, 0.00160226, 0.00068731,\n",
       "       0.00027613, 0.00032815, 0.00031656, 0.00079909, 0.00079598,\n",
       "       0.00093946, 0.00077889, 0.00102815, 0.00158737, 0.00084231,\n",
       "       0.00114407, 0.00211012, 0.00123986, 0.00117477, 0.0002933 ,\n",
       "       0.00079184, 0.00055394, 0.00116673, 0.0002841 , 0.00291536,\n",
       "       0.00083093, 0.0012431 , 0.00038396, 0.00110359, 0.00160735,\n",
       "       0.00063898, 0.00086428, 0.00045736, 0.0005024 , 0.00066162,\n",
       "       0.00034281, 0.00061212, 0.00040914, 0.00093036, 0.00068864,\n",
       "       0.00301798, 0.00086001, 0.00068634, 0.00050899, 0.00204736,\n",
       "       0.00095523, 0.0013621 , 0.00125917, 0.00221715, 0.00182159,\n",
       "       0.00236309, 0.00113929, 0.00293465, 0.00047372, 0.00053546,\n",
       "       0.00061066, 0.00169449, 0.00055123, 0.00061831, 0.00141221,\n",
       "       0.00077642, 0.00046675, 0.00061578, 0.00180867, 0.00089534,\n",
       "       0.00097518, 0.00495033, 0.00068091, 0.00050159, 0.00021681,\n",
       "       0.00175101, 0.00083519, 0.00121055, 0.00264147, 0.00166136,\n",
       "       0.00377308, 0.00055078, 0.0011805 , 0.00044852, 0.00120195,\n",
       "       0.00335948, 0.00058967, 0.00098562, 0.00161253, 0.00541449,\n",
       "       0.00134901, 0.00238209, 0.00108658, 0.00099005, 0.0006062 ,\n",
       "       0.00095633, 0.00061301, 0.00193014, 0.00079055, 0.00058937,\n",
       "       0.00113174, 0.00048609, 0.00168928, 0.00072286, 0.00072432,\n",
       "       0.00116571, 0.00072778, 0.00029193, 0.00049241, 0.00198651,\n",
       "       0.00080815, 0.00052806, 0.0003045 , 0.00093751, 0.00123243,\n",
       "       0.00121947, 0.00084271, 0.00493923, 0.00067654, 0.00036083,\n",
       "       0.00610531, 0.00268537, 0.00051727, 0.00245968, 0.00352278,\n",
       "       0.00171874, 0.00095568, 0.00079208, 0.00147711, 0.00023246,\n",
       "       0.00042999, 0.00054933, 0.00375891, 0.00118486, 0.00069612,\n",
       "       0.00069247, 0.00054662, 0.00111342, 0.00119071, 0.00024484,\n",
       "       0.00273917, 0.00150194, 0.00111478, 0.00309851, 0.00066802,\n",
       "       0.0001998 , 0.00049799, 0.00210229, 0.00047212, 0.00120133,\n",
       "       0.00125867, 0.00048767, 0.00085409, 0.00088245, 0.00139486,\n",
       "       0.00228902, 0.00066739, 0.00119141, 0.00383528, 0.00056775,\n",
       "       0.0004135 , 0.00061151, 0.00102361, 0.00069337, 0.00278792,\n",
       "       0.00140879, 0.00097339, 0.00258038, 0.00057952, 0.00124561,\n",
       "       0.00277041, 0.00163902, 0.00564252, 0.0016113 , 0.00080569,\n",
       "       0.00111841, 0.00079754, 0.00281962, 0.00143285, 0.00104024,\n",
       "       0.0010277 , 0.00650669, 0.0010548 , 0.00301513, 0.00076471,\n",
       "       0.00082596, 0.00130718, 0.00119832, 0.00069612, 0.00158047,\n",
       "       0.00079008, 0.00077517, 0.00060112, 0.00295299, 0.00124195,\n",
       "       0.00124794, 0.00151106, 0.00051616, 0.00039265, 0.0008048 ,\n",
       "       0.0006836 , 0.00133884, 0.00146362, 0.0006472 , 0.00211735,\n",
       "       0.0020878 , 0.00071813, 0.00096882, 0.00130084, 0.00163396,\n",
       "       0.00100015, 0.00461533, 0.00176666, 0.00072248, 0.00091394,\n",
       "       0.00167622, 0.00142676, 0.00038982, 0.00202306, 0.00169813,\n",
       "       0.00114872, 0.00065482, 0.00052436, 0.00057793, 0.0003439 ,\n",
       "       0.00146245, 0.00320204, 0.00087077, 0.00091635, 0.00061048,\n",
       "       0.00070216, 0.00047625, 0.00037021, 0.00037495, 0.00100844,\n",
       "       0.00107673, 0.00163132, 0.00157283, 0.00070373, 0.00109324,\n",
       "       0.00054112, 0.00059863, 0.00100613, 0.00094687, 0.00082387,\n",
       "       0.00082754, 0.00042135, 0.00099309, 0.00125863, 0.00092761,\n",
       "       0.00096529, 0.00368414, 0.00050562, 0.00089967, 0.00112941,\n",
       "       0.0005801 , 0.00151167, 0.00470054, 0.00096426, 0.00119647,\n",
       "       0.0031124 , 0.00112936, 0.00147142, 0.0007101 , 0.00207768,\n",
       "       0.00122485, 0.00162991, 0.0005582 , 0.00182014, 0.00101851,\n",
       "       0.00058834, 0.00086792, 0.0004671 , 0.00111345, 0.00239792,\n",
       "       0.00271271, 0.00357127, 0.00170173, 0.00095539, 0.00105047,\n",
       "       0.00089738, 0.00144738, 0.00052082, 0.00080386, 0.00350234,\n",
       "       0.00134784, 0.00341248, 0.00063712, 0.00034559, 0.00117944,\n",
       "       0.00080277, 0.00123246, 0.00106737, 0.0003674 , 0.00069956,\n",
       "       0.0009247 , 0.00041208, 0.00041907, 0.00076658, 0.00248536,\n",
       "       0.00100878, 0.00064976, 0.00115932, 0.00186591, 0.00066364,\n",
       "       0.00061224, 0.00043174, 0.00221246, 0.00071504, 0.00150197,\n",
       "       0.0009884 , 0.00133055, 0.00136611, 0.00280815, 0.00045463,\n",
       "       0.00057309, 0.00054083, 0.00209187, 0.00131263, 0.00121416,\n",
       "       0.00382649, 0.00357206, 0.00100724, 0.0007925 , 0.00051446,\n",
       "       0.00097603, 0.00041763, 0.00186307, 0.00055646, 0.00041823,\n",
       "       0.00193285, 0.00070165, 0.00063762, 0.00052034, 0.00108322,\n",
       "       0.00042526, 0.00041957, 0.00037099, 0.00589372, 0.0009992 ,\n",
       "       0.00086158, 0.00261873, 0.00275991, 0.00166866, 0.00065667,\n",
       "       0.00404085, 0.00110494, 0.0009785 , 0.00106712, 0.00025789,\n",
       "       0.00077212, 0.00096516, 0.00102574, 0.00023336, 0.00044843,\n",
       "       0.00088378, 0.00051122, 0.00140039, 0.00156091, 0.00160515,\n",
       "       0.0023908 , 0.00141813, 0.0017309 , 0.00137478, 0.00384766,\n",
       "       0.00050437, 0.00025918, 0.00063944, 0.00300162, 0.00070797,\n",
       "       0.00052118, 0.00036356, 0.00059468, 0.00099111, 0.00014536,\n",
       "       0.0006835 , 0.00266032, 0.00446594, 0.00031148, 0.00089854,\n",
       "       0.00077864, 0.00084123, 0.00180083, 0.0006877 , 0.00424192,\n",
       "       0.00307816, 0.00167545, 0.00082432, 0.00077463, 0.00045917,\n",
       "       0.00065616, 0.00066917, 0.00084542, 0.00025436, 0.00208227,\n",
       "       0.00038752, 0.00273769, 0.00044491, 0.00134945, 0.0023394 ,\n",
       "       0.00269487, 0.00339019, 0.00101569, 0.00154722, 0.00057415,\n",
       "       0.00116672, 0.00095075, 0.00199425, 0.00241085, 0.0013695 ,\n",
       "       0.00106276, 0.00147437, 0.00148977, 0.00239441, 0.0006888 ,\n",
       "       0.00891248, 0.0003611 , 0.00068709, 0.00069611, 0.00036772,\n",
       "       0.00194125, 0.00164531, 0.00420409, 0.00063458, 0.00041462,\n",
       "       0.000689  , 0.00021805, 0.00244654, 0.00054487, 0.00111532,\n",
       "       0.00413392, 0.00063198, 0.00220092, 0.00142414, 0.00024459,\n",
       "       0.00117797, 0.00256422, 0.0012947 , 0.00065858, 0.00137657,\n",
       "       0.0005748 , 0.00056083, 0.00168503, 0.00120121, 0.00091593,\n",
       "       0.00082671, 0.00108469, 0.00085688, 0.0008909 , 0.00087093,\n",
       "       0.00032111, 0.00085796, 0.00021569, 0.00143944, 0.00027742,\n",
       "       0.0003089 , 0.00163305, 0.00122024, 0.00488824, 0.00042578,\n",
       "       0.00162441, 0.0005389 , 0.00028743, 0.00073288, 0.0005188 ,\n",
       "       0.00237475, 0.00076201, 0.00047954, 0.00577758, 0.00838328,\n",
       "       0.00100877, 0.00036935, 0.00126289, 0.001974  , 0.00079554,\n",
       "       0.00158147, 0.00118952, 0.0006143 , 0.00056484, 0.00029205,\n",
       "       0.00181143, 0.00030801, 0.0005496 , 0.00082446, 0.00079744,\n",
       "       0.00144612, 0.00024571, 0.00225912, 0.00081223, 0.0006999 ,\n",
       "       0.00110922, 0.00094909, 0.00115124, 0.00082941, 0.00087781,\n",
       "       0.0003585 , 0.00025385, 0.00074597, 0.00093697, 0.0014266 ,\n",
       "       0.00111156, 0.00078395, 0.00070163, 0.00092303, 0.00026096,\n",
       "       0.00387819, 0.00101602, 0.00080663, 0.00079298, 0.00112465,\n",
       "       0.00178329, 0.00083451, 0.00163911, 0.00081496, 0.00148372,\n",
       "       0.00098314, 0.0003159 , 0.00178096, 0.00208945, 0.00072003,\n",
       "       0.00138367, 0.00072286, 0.00058347, 0.00052239, 0.00151691,\n",
       "       0.00051487, 0.00105171, 0.0018359 , 0.00245905, 0.00252029,\n",
       "       0.00086322, 0.0011124 , 0.0005323 , 0.00224206, 0.00090803,\n",
       "       0.00119786, 0.00074357, 0.00094587, 0.00117573, 0.00067849,\n",
       "       0.00163543, 0.00120688, 0.00040188, 0.00066495, 0.00064465,\n",
       "       0.00061574, 0.00059363, 0.00114175, 0.00157684, 0.00316822,\n",
       "       0.00168412, 0.00132069, 0.0008337 , 0.00041386, 0.00171502,\n",
       "       0.00059639, 0.0011685 , 0.00160912, 0.00087413, 0.0005101 ,\n",
       "       0.0004163 , 0.00050066, 0.00061949, 0.00036394, 0.0012341 ,\n",
       "       0.00093126, 0.00070249, 0.00097955, 0.00058205, 0.00229092,\n",
       "       0.00046176, 0.0002963 , 0.00181373, 0.00177435, 0.00108046,\n",
       "       0.00059694, 0.00137939, 0.0003995 , 0.00044423, 0.00196723,\n",
       "       0.00084203, 0.00110342, 0.00092694, 0.00100881, 0.00033015,\n",
       "       0.00014838, 0.00197468, 0.00093645, 0.00158655, 0.00083611,\n",
       "       0.00078105, 0.00109045, 0.00083483, 0.00016769, 0.00079371,\n",
       "       0.00073259, 0.00115744, 0.00165336, 0.00101902, 0.00069598,\n",
       "       0.00156121, 0.00222795, 0.00274107, 0.00525939, 0.00109066,\n",
       "       0.00138599, 0.00101957, 0.0011632 , 0.0005676 , 0.00039047,\n",
       "       0.00079619, 0.00041009, 0.0030626 , 0.00049531, 0.00036155,\n",
       "       0.00115392, 0.0048724 , 0.00126186, 0.00286008, 0.00049507,\n",
       "       0.00081294, 0.00088043, 0.00067159, 0.00051938, 0.00041233,\n",
       "       0.0005212 , 0.00084813, 0.00088967, 0.00125715, 0.00024795,\n",
       "       0.00110514, 0.0002626 , 0.00023657, 0.00231094, 0.00133622,\n",
       "       0.00051289, 0.00053149, 0.00380331, 0.00194439, 0.00057296,\n",
       "       0.00016454, 0.001138  , 0.00032119, 0.00106684, 0.00094682,\n",
       "       0.00211666, 0.00046247, 0.00254521, 0.00300573, 0.00071205,\n",
       "       0.00269808, 0.00161356, 0.00147267, 0.00032867, 0.00090766,\n",
       "       0.00226665, 0.00102219, 0.0006289 , 0.00101173, 0.00226479,\n",
       "       0.00073744, 0.00058843, 0.00046138, 0.00053834, 0.00154637,\n",
       "       0.00096891, 0.00067495, 0.00167985, 0.0004735 , 0.00095513,\n",
       "       0.00090897, 0.00298344, 0.00094303, 0.00062568, 0.00056246,\n",
       "       0.00044453, 0.00025906, 0.00042629, 0.00053536, 0.00110848,\n",
       "       0.00035089, 0.00058977, 0.00041883, 0.00085621, 0.0007082 ,\n",
       "       0.00227974, 0.00252602, 0.00079055, 0.00094143, 0.00300012,\n",
       "       0.00069166, 0.00067549, 0.00204274, 0.00085753, 0.00126101,\n",
       "       0.00097221, 0.0006747 , 0.00039717, 0.00078344, 0.00086467,\n",
       "       0.00042727, 0.00079072, 0.00040505, 0.00110287, 0.00103668,\n",
       "       0.00186523, 0.00512457, 0.00094401, 0.00076019, 0.00225636,\n",
       "       0.00088667, 0.00081637, 0.00049976, 0.00072126, 0.00077627,\n",
       "       0.00111435, 0.00070633], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5c3c6d6f-4c71-412a-8572-d1fda17f6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_valid_bts_variance = np.var(all_valid_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8710007-5528-4065-b868-777e4abebb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_valid_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2319348c-2bd4-44fa-97a9-e77a614a2d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111437, 0.00215758, 0.0016149 , 0.00110854, 0.00177893,\n",
       "       0.0010395 , 0.00045934, 0.00218281, 0.00072607, 0.00161189,\n",
       "       0.00129221, 0.00064204, 0.00067966, 0.00121964, 0.00054192,\n",
       "       0.00165475, 0.00982931, 0.00098922, 0.00127981, 0.00106769,\n",
       "       0.00047407, 0.00046066, 0.0012382 , 0.00408663, 0.004013  ,\n",
       "       0.00104843, 0.00157014, 0.00091257, 0.00140339, 0.00090362,\n",
       "       0.00096479, 0.00230562, 0.00060487, 0.0004463 , 0.00104181,\n",
       "       0.00054409, 0.00093233, 0.00048041, 0.0004866 , 0.00109376,\n",
       "       0.00160849, 0.00097167, 0.00090513, 0.00222176, 0.00079927,\n",
       "       0.00217688, 0.00070226, 0.0027635 , 0.00018478, 0.00072723,\n",
       "       0.00147327, 0.00253314, 0.00259756, 0.00222062, 0.00460852,\n",
       "       0.00138656, 0.00166871, 0.00168314, 0.00151581, 0.00082051,\n",
       "       0.00081417, 0.00152035, 0.00177849, 0.00065383, 0.00053532,\n",
       "       0.00061589, 0.00129038, 0.00253621, 0.0022012 , 0.00195072,\n",
       "       0.00133277, 0.00134491, 0.00129875, 0.00174996, 0.0011459 ,\n",
       "       0.0016283 , 0.00098838, 0.00261783, 0.00130107, 0.0007612 ,\n",
       "       0.00066536, 0.00308185, 0.0010137 , 0.00149181, 0.00068181,\n",
       "       0.001028  , 0.00209949, 0.0017831 , 0.00148183, 0.00159291,\n",
       "       0.00149922, 0.00026098, 0.00127723, 0.00040533, 0.00021145,\n",
       "       0.00054627, 0.00022489, 0.00101377, 0.0025006 , 0.00242447,\n",
       "       0.00056596, 0.00098948, 0.0004143 , 0.00014458, 0.00220332,\n",
       "       0.00259521, 0.00050684, 0.00129521, 0.00170243, 0.00187751,\n",
       "       0.00129532, 0.00105263, 0.00152395, 0.00137774, 0.00408934,\n",
       "       0.00160638, 0.00430861, 0.00097025, 0.00076396, 0.0006169 ,\n",
       "       0.00062589, 0.00089974, 0.00426047, 0.00072227, 0.00065956,\n",
       "       0.00127007, 0.00068025, 0.00164218, 0.00101867, 0.00138354,\n",
       "       0.00087166, 0.00106146, 0.00060696, 0.00114642, 0.00108146,\n",
       "       0.0023288 , 0.0006369 , 0.00110827, 0.0022268 , 0.01084747,\n",
       "       0.00207236, 0.00049356, 0.00092279, 0.00013559, 0.00316686,\n",
       "       0.00062504, 0.00087341, 0.00054916, 0.00118682, 0.00340552,\n",
       "       0.00485832, 0.00202214, 0.00092711, 0.00157767, 0.00171194,\n",
       "       0.00113766, 0.00145199, 0.0005724 , 0.00087721, 0.00181209,\n",
       "       0.00124976, 0.00465682, 0.00312604, 0.000772  , 0.00155951,\n",
       "       0.00045681, 0.0007997 , 0.00066409, 0.0012055 , 0.00082506,\n",
       "       0.00147647, 0.00064606, 0.00080337, 0.00082253, 0.00049905,\n",
       "       0.00010724, 0.00094698, 0.00051625, 0.00163088, 0.00047667,\n",
       "       0.00061257, 0.00085065, 0.00105394, 0.00115474, 0.00133143,\n",
       "       0.00188529, 0.00119353, 0.0006532 , 0.00139154, 0.00065593,\n",
       "       0.00051322, 0.00491763, 0.00057768, 0.00093863, 0.00084872,\n",
       "       0.0028553 , 0.00160565, 0.0007308 , 0.00075435, 0.00115496,\n",
       "       0.0050956 , 0.00139922, 0.00062893, 0.00159818, 0.00077946,\n",
       "       0.00062139, 0.00099101, 0.00031926, 0.00093404, 0.00094885,\n",
       "       0.00082847, 0.00099562, 0.00397389, 0.00112208, 0.00067516,\n",
       "       0.0017704 , 0.0006182 , 0.00278593, 0.00032323, 0.00029274,\n",
       "       0.00067467, 0.00026727, 0.00149579, 0.00545491, 0.00100247,\n",
       "       0.00075177, 0.00395448, 0.00033377, 0.00092599, 0.0010032 ,\n",
       "       0.00090203, 0.0004061 , 0.00035504, 0.00089337, 0.00038512,\n",
       "       0.00105838, 0.00231034, 0.00077672, 0.00140492, 0.00091546,\n",
       "       0.00093142, 0.0010422 , 0.00063638, 0.00160226, 0.00068731,\n",
       "       0.00027613, 0.00032815, 0.00031656, 0.00079909, 0.00079598,\n",
       "       0.00093946, 0.00077889, 0.00102815, 0.00158737, 0.00084231,\n",
       "       0.00114407, 0.00211012, 0.00123986, 0.00117477, 0.0002933 ,\n",
       "       0.00079184, 0.00055394, 0.00116673, 0.0002841 , 0.00291536,\n",
       "       0.00083093, 0.0012431 , 0.00038396, 0.00110359, 0.00160735,\n",
       "       0.00063898, 0.00086428, 0.00045736, 0.0005024 , 0.00066162,\n",
       "       0.00034281, 0.00061212, 0.00040914, 0.00093036, 0.00068864,\n",
       "       0.00301798, 0.00086001, 0.00068634, 0.00050899, 0.00204736,\n",
       "       0.00095523, 0.0013621 , 0.00125917, 0.00221715, 0.00182159,\n",
       "       0.00236309, 0.00113929, 0.00293465, 0.00047372, 0.00053546,\n",
       "       0.00061066, 0.00169449, 0.00055123, 0.00061831, 0.00141221,\n",
       "       0.00077642, 0.00046675, 0.00061578, 0.00180867, 0.00089534,\n",
       "       0.00097518, 0.00495033, 0.00068091, 0.00050159, 0.00021681,\n",
       "       0.00175101, 0.00083519, 0.00121055, 0.00264147, 0.00166136,\n",
       "       0.00377308, 0.00055078, 0.0011805 , 0.00044852, 0.00120195,\n",
       "       0.00335948, 0.00058967, 0.00098562, 0.00161253, 0.00541449,\n",
       "       0.00134901, 0.00238209, 0.00108658, 0.00099005, 0.0006062 ,\n",
       "       0.00095633, 0.00061301, 0.00193014, 0.00079055, 0.00058937,\n",
       "       0.00113174, 0.00048609, 0.00168928, 0.00072286, 0.00072432,\n",
       "       0.00116571, 0.00072778, 0.00029193, 0.00049241, 0.00198651,\n",
       "       0.00080815, 0.00052806, 0.0003045 , 0.00093751, 0.00123243,\n",
       "       0.00121947, 0.00084271, 0.00493923, 0.00067654, 0.00036083,\n",
       "       0.00610531, 0.00268537, 0.00051727, 0.00245968, 0.00352278,\n",
       "       0.00171874, 0.00095568, 0.00079208, 0.00147711, 0.00023246,\n",
       "       0.00042999, 0.00054933, 0.00375891, 0.00118486, 0.00069612,\n",
       "       0.00069247, 0.00054662, 0.00111342, 0.00119071, 0.00024484,\n",
       "       0.00273917, 0.00150194, 0.00111478, 0.00309851, 0.00066802,\n",
       "       0.0001998 , 0.00049799, 0.00210229, 0.00047212, 0.00120133,\n",
       "       0.00125867, 0.00048767, 0.00085409, 0.00088245, 0.00139486,\n",
       "       0.00228902, 0.00066739, 0.00119141, 0.00383528, 0.00056775,\n",
       "       0.0004135 , 0.00061151, 0.00102361, 0.00069337, 0.00278792,\n",
       "       0.00140879, 0.00097339, 0.00258038, 0.00057952, 0.00124561,\n",
       "       0.00277041, 0.00163902, 0.00564252, 0.0016113 , 0.00080569,\n",
       "       0.00111841, 0.00079754, 0.00281962, 0.00143285, 0.00104024,\n",
       "       0.0010277 , 0.00650669, 0.0010548 , 0.00301513, 0.00076471,\n",
       "       0.00082596, 0.00130718, 0.00119832, 0.00069612, 0.00158047,\n",
       "       0.00079008, 0.00077517, 0.00060112, 0.00295299, 0.00124195,\n",
       "       0.00124794, 0.00151106, 0.00051616, 0.00039265, 0.0008048 ,\n",
       "       0.0006836 , 0.00133884, 0.00146362, 0.0006472 , 0.00211735,\n",
       "       0.0020878 , 0.00071813, 0.00096882, 0.00130084, 0.00163396,\n",
       "       0.00100015, 0.00461533, 0.00176666, 0.00072248, 0.00091394,\n",
       "       0.00167622, 0.00142676, 0.00038982, 0.00202306, 0.00169813,\n",
       "       0.00114872, 0.00065482, 0.00052436, 0.00057793, 0.0003439 ,\n",
       "       0.00146245, 0.00320204, 0.00087077, 0.00091635, 0.00061048,\n",
       "       0.00070216, 0.00047625, 0.00037021, 0.00037495, 0.00100844,\n",
       "       0.00107673, 0.00163132, 0.00157283, 0.00070373, 0.00109324,\n",
       "       0.00054112, 0.00059863, 0.00100613, 0.00094687, 0.00082387,\n",
       "       0.00082754, 0.00042135, 0.00099309, 0.00125863, 0.00092761,\n",
       "       0.00096529, 0.00368414, 0.00050562, 0.00089967, 0.00112941,\n",
       "       0.0005801 , 0.00151167, 0.00470054, 0.00096426, 0.00119647,\n",
       "       0.0031124 , 0.00112936, 0.00147142, 0.0007101 , 0.00207768,\n",
       "       0.00122485, 0.00162991, 0.0005582 , 0.00182014, 0.00101851,\n",
       "       0.00058834, 0.00086792, 0.0004671 , 0.00111345, 0.00239792,\n",
       "       0.00271271, 0.00357127, 0.00170173, 0.00095539, 0.00105047,\n",
       "       0.00089738, 0.00144738, 0.00052082, 0.00080386, 0.00350234,\n",
       "       0.00134784, 0.00341248, 0.00063712, 0.00034559, 0.00117944,\n",
       "       0.00080277, 0.00123246, 0.00106737, 0.0003674 , 0.00069956,\n",
       "       0.0009247 , 0.00041208, 0.00041907, 0.00076658, 0.00248536,\n",
       "       0.00100878, 0.00064976, 0.00115932, 0.00186591, 0.00066364,\n",
       "       0.00061224, 0.00043174, 0.00221246, 0.00071504, 0.00150197,\n",
       "       0.0009884 , 0.00133055, 0.00136611, 0.00280815, 0.00045463,\n",
       "       0.00057309, 0.00054083, 0.00209187, 0.00131263, 0.00121416,\n",
       "       0.00382649, 0.00357207, 0.00100724, 0.0007925 , 0.00051446,\n",
       "       0.00097603, 0.00041763, 0.00186307, 0.00055646, 0.00041823,\n",
       "       0.00193285, 0.00070165, 0.00063762, 0.00052034, 0.00108322,\n",
       "       0.00042526, 0.00041957, 0.00037099, 0.00589372, 0.0009992 ,\n",
       "       0.00086158, 0.00261873, 0.00275991, 0.00166866, 0.00065667,\n",
       "       0.00404085, 0.00110494, 0.0009785 , 0.00106712, 0.00025789,\n",
       "       0.00077212, 0.00096516, 0.00102574, 0.00023336, 0.00044843,\n",
       "       0.00088378, 0.00051122, 0.00140039, 0.00156091, 0.00160515,\n",
       "       0.0023908 , 0.00141813, 0.0017309 , 0.00137478, 0.00384766,\n",
       "       0.00050437, 0.00025918, 0.00063944, 0.00300162, 0.00070797,\n",
       "       0.00052118, 0.00036356, 0.00059468, 0.00099111, 0.00014536,\n",
       "       0.0006835 , 0.00266032, 0.00446594, 0.00031148, 0.00089854,\n",
       "       0.00077864, 0.00084123, 0.00180083, 0.0006877 , 0.00424192,\n",
       "       0.00307816, 0.00167545, 0.00082432, 0.00077463, 0.00045917,\n",
       "       0.00065616, 0.00066917, 0.00084542, 0.00025436, 0.00208227,\n",
       "       0.00038752, 0.00273769, 0.00044491, 0.00134945, 0.0023394 ,\n",
       "       0.00269487, 0.00339019, 0.00101569, 0.00154722, 0.00057415,\n",
       "       0.00116672, 0.00095075, 0.00199425, 0.00241085, 0.0013695 ,\n",
       "       0.00106276, 0.00147437, 0.00148977, 0.00239441, 0.0006888 ,\n",
       "       0.00891248, 0.0003611 , 0.00068709, 0.00069611, 0.00036772,\n",
       "       0.00194125, 0.00164531, 0.00420409, 0.00063458, 0.00041462,\n",
       "       0.000689  , 0.00021805, 0.00244654, 0.00054487, 0.00111532,\n",
       "       0.00413392, 0.00063198, 0.00220092, 0.00142414, 0.00024459,\n",
       "       0.00117797, 0.00256422, 0.0012947 , 0.00065858, 0.00137657,\n",
       "       0.0005748 , 0.00056083, 0.00168503, 0.00120121, 0.00091593,\n",
       "       0.00082671, 0.00108469, 0.00085688, 0.0008909 , 0.00087093,\n",
       "       0.00032111, 0.00085796, 0.00021569, 0.00143944, 0.00027742,\n",
       "       0.0003089 , 0.00163305, 0.00122024, 0.00488824, 0.00042578,\n",
       "       0.00162441, 0.0005389 , 0.00028743, 0.00073288, 0.0005188 ,\n",
       "       0.00237475, 0.00076201, 0.00047954, 0.00577758, 0.00838328,\n",
       "       0.00100877, 0.00036935, 0.00126289, 0.001974  , 0.00079555,\n",
       "       0.00158147, 0.00118952, 0.0006143 , 0.00056484, 0.00029205,\n",
       "       0.00181143, 0.00030801, 0.0005496 , 0.00082446, 0.00079744,\n",
       "       0.00144612, 0.00024571, 0.00225912, 0.00081223, 0.0006999 ,\n",
       "       0.00110922, 0.00094909, 0.00115124, 0.00082941, 0.00087781,\n",
       "       0.0003585 , 0.00025385, 0.00074597, 0.00093697, 0.0014266 ,\n",
       "       0.00111156, 0.00078395, 0.00070163, 0.00092303, 0.00026096,\n",
       "       0.00387819, 0.00101602, 0.00080663, 0.00079298, 0.00112465,\n",
       "       0.00178329, 0.00083451, 0.00163911, 0.00081496, 0.00148372,\n",
       "       0.00098314, 0.0003159 , 0.00178096, 0.00208945, 0.00072003,\n",
       "       0.00138367, 0.00072286, 0.00058347, 0.00052239, 0.00151691,\n",
       "       0.00051487, 0.00105171, 0.0018359 , 0.00245905, 0.00252029,\n",
       "       0.00086322, 0.0011124 , 0.0005323 , 0.00224206, 0.00090803,\n",
       "       0.00119786, 0.00074357, 0.00094587, 0.00117573, 0.00067849,\n",
       "       0.00163543, 0.00120688, 0.00040188, 0.00066495, 0.00064465,\n",
       "       0.00061574, 0.00059363, 0.00114175, 0.00157684, 0.00316822,\n",
       "       0.00168412, 0.00132069, 0.0008337 , 0.00041386, 0.00171502,\n",
       "       0.00059639, 0.0011685 , 0.00160912, 0.00087413, 0.0005101 ,\n",
       "       0.0004163 , 0.00050066, 0.00061949, 0.00036394, 0.0012341 ,\n",
       "       0.00093126, 0.00070249, 0.00097955, 0.00058205, 0.00229092,\n",
       "       0.00046176, 0.0002963 , 0.00181373, 0.00177435, 0.00108046,\n",
       "       0.00059694, 0.00137939, 0.0003995 , 0.00044423, 0.00196723,\n",
       "       0.00084203, 0.00110342, 0.00092694, 0.00100881, 0.00033015,\n",
       "       0.00014838, 0.00197468, 0.00093645, 0.00158655, 0.00083611,\n",
       "       0.00078105, 0.00109045, 0.00083483, 0.00016769, 0.00079371,\n",
       "       0.00073259, 0.00115744, 0.00165336, 0.00101902, 0.00069598,\n",
       "       0.00156121, 0.00222795, 0.00274107, 0.00525939, 0.00109066,\n",
       "       0.00138599, 0.00101957, 0.0011632 , 0.0005676 , 0.00039047,\n",
       "       0.00079619, 0.00041009, 0.0030626 , 0.00049531, 0.00036155,\n",
       "       0.00115392, 0.0048724 , 0.00126186, 0.00286008, 0.00049507,\n",
       "       0.00081294, 0.00088043, 0.00067159, 0.00051938, 0.00041233,\n",
       "       0.0005212 , 0.00084813, 0.00088967, 0.00125715, 0.00024795,\n",
       "       0.00110514, 0.0002626 , 0.00023657, 0.00231094, 0.00133622,\n",
       "       0.00051289, 0.00053149, 0.00380331, 0.00194439, 0.00057296,\n",
       "       0.00016454, 0.001138  , 0.00032119, 0.00106684, 0.00094682,\n",
       "       0.00211666, 0.00046247, 0.00254521, 0.00300573, 0.00071205,\n",
       "       0.00269808, 0.00161356, 0.00147267, 0.00032867, 0.00090766,\n",
       "       0.00226665, 0.00102219, 0.0006289 , 0.00101173, 0.00226479,\n",
       "       0.00073744, 0.00058843, 0.00046138, 0.00053834, 0.00154637,\n",
       "       0.00096891, 0.00067495, 0.00167985, 0.0004735 , 0.00095513,\n",
       "       0.00090897, 0.00298344, 0.00094303, 0.00062568, 0.00056246,\n",
       "       0.00044453, 0.00025906, 0.00042629, 0.00053536, 0.00110848,\n",
       "       0.00035089, 0.00058977, 0.00041883, 0.00085621, 0.0007082 ,\n",
       "       0.00227974, 0.00252602, 0.00079055, 0.00094143, 0.00300012,\n",
       "       0.00069166, 0.00067549, 0.00204274, 0.00085753, 0.00126101,\n",
       "       0.00097221, 0.0006747 , 0.00039717, 0.00078344, 0.00086467,\n",
       "       0.00042727, 0.00079072, 0.00040505, 0.00110287, 0.00103668,\n",
       "       0.00186523, 0.00512457, 0.00094401, 0.00076019, 0.00225636,\n",
       "       0.00088667, 0.00081637, 0.00049976, 0.00072126, 0.00077627,\n",
       "       0.00111435, 0.00070633], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_valid_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a1a62ea-147b-436c-9c51-0c86748d9fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check ot see if the variances are inded correct\n",
    "np.mean(np.round(valid_bts_variance, 6) == np.round(alt_valid_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d839130c-9dd1-4862-a1d5-b5aca8ecc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bts_variance = equation_6_model_variance(all_test_preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d4dc91ab-548b-4052-bba4-a03f8e03fe6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebe42952-530c-4cca-a2ce-07067bfc654e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.3056750e-04, 5.4126582e-04, 7.3728670e-04, 2.3910771e-03,\n",
       "       1.2964582e-03, 2.0693133e-03, 1.7236081e-03, 4.0479396e-03,\n",
       "       7.3798490e-04, 9.4396499e-04, 4.6927575e-04, 1.1376116e-03,\n",
       "       1.1319038e-03, 1.0751548e-03, 4.0510131e-04, 9.1339211e-04,\n",
       "       4.4738321e-04, 4.1118104e-04, 6.1193370e-04, 1.0366106e-03,\n",
       "       1.6892438e-03, 1.4544725e-03, 5.7380856e-04, 2.0091825e-03,\n",
       "       1.3613458e-03, 1.3245464e-03, 4.0226689e-04, 3.4186421e-03,\n",
       "       9.3519076e-04, 3.3677850e-04, 3.9706362e-04, 7.3917158e-04,\n",
       "       1.1229625e-03, 3.3812751e-03, 5.4388738e-04, 4.8154328e-04,\n",
       "       1.2786197e-03, 8.0953725e-04, 1.6883966e-03, 1.5954311e-03,\n",
       "       1.3278313e-03, 7.0237753e-04, 7.6777092e-04, 4.6900054e-04,\n",
       "       2.9251128e-03, 4.3809963e-03, 1.0897731e-03, 3.3490313e-03,\n",
       "       1.2390179e-03, 1.3006560e-03, 5.3913714e-03, 9.3830645e-04,\n",
       "       1.6804704e-03, 1.3012582e-03, 8.3899620e-04, 7.0608687e-03,\n",
       "       1.9761466e-03, 1.0441262e-03, 3.0904261e-03, 6.0246279e-04,\n",
       "       4.9685471e-04, 8.8865019e-04, 1.6715023e-03, 5.1982200e-04,\n",
       "       1.5053996e-03, 3.2418750e-03, 1.7582248e-03, 8.0119271e-04,\n",
       "       7.3334645e-04, 9.7093009e-04, 2.3624459e-03, 7.9011574e-04,\n",
       "       1.2380185e-03, 3.4004804e-03, 8.9455530e-04, 4.0172050e-03,\n",
       "       1.2377334e-03, 1.3906370e-03, 2.0140598e-03, 6.6937116e-04,\n",
       "       3.4417113e-04, 2.6107584e-03, 6.5609103e-04, 6.0235104e-04,\n",
       "       6.7960366e-04, 1.3336198e-03, 8.1023807e-03, 5.8685325e-04,\n",
       "       1.3296694e-03, 5.1342871e-04, 1.0049741e-03, 1.1454229e-03,\n",
       "       1.7307616e-04, 1.4636129e-03, 1.3603717e-03, 6.1048884e-03,\n",
       "       1.3685980e-03, 8.8154909e-04, 1.2937079e-03, 4.8924086e-04,\n",
       "       3.4942089e-03, 1.7204365e-03, 2.4582441e-03, 5.1647064e-04,\n",
       "       5.3013989e-04, 3.4360886e-03, 1.0579745e-03, 4.9607025e-04,\n",
       "       1.4349368e-03, 7.6751027e-04, 6.3240482e-04, 5.9880997e-04,\n",
       "       9.5186726e-04, 6.5444602e-04, 1.0251409e-03, 3.2715136e-03,\n",
       "       3.1573598e-03, 3.7327099e-03, 9.9465577e-04, 4.1765702e-04,\n",
       "       3.1494012e-04, 3.9849101e-04, 4.9053977e-04, 6.0536916e-04,\n",
       "       1.4666026e-03, 1.9226917e-03, 1.7797335e-03, 6.4740115e-04,\n",
       "       3.7791734e-03, 4.1073938e-03, 9.8702009e-04, 3.2778545e-03,\n",
       "       7.0930389e-04, 8.1762590e-04, 4.8016044e-03, 8.3180773e-04,\n",
       "       1.3672267e-03, 1.1674004e-03, 3.5108751e-04, 8.5928640e-04,\n",
       "       4.1114163e-04, 4.6258824e-04, 3.5633903e-03, 7.0028001e-04,\n",
       "       2.5282770e-03, 1.6525488e-03, 1.0018266e-03, 8.1694435e-04,\n",
       "       2.8544469e-03, 4.5575420e-03, 1.3825705e-03, 8.7336119e-04,\n",
       "       2.5502529e-03, 6.7815883e-04, 5.9534906e-04, 1.0396200e-03,\n",
       "       8.4708352e-04, 1.6735484e-03, 1.1682432e-03, 1.0807370e-03,\n",
       "       9.5505110e-04, 3.6382832e-04, 6.5346214e-04, 1.3783234e-03,\n",
       "       6.8642508e-04, 6.4208911e-04, 2.7048464e-03, 7.6470646e-04,\n",
       "       1.0890064e-03, 5.6475337e-04, 7.0426194e-04, 7.5996865e-04,\n",
       "       8.5115730e-04, 5.7283469e-04, 1.1080208e-03, 1.2218752e-03,\n",
       "       1.6144542e-03, 7.2128250e-04, 8.5834542e-04, 6.0305721e-04,\n",
       "       5.3483131e-04, 1.6120209e-03, 7.8333076e-04, 1.8511439e-03,\n",
       "       3.4479902e-04, 1.1683548e-03, 1.7686677e-03, 1.3496795e-03,\n",
       "       1.8321427e-03, 2.5469379e-04, 3.7945280e-04, 1.6806292e-03,\n",
       "       4.5578682e-04, 1.6823565e-03, 2.1575701e-03, 7.2142109e-04,\n",
       "       4.4644563e-04, 6.7769724e-04, 7.3777302e-04, 6.3407701e-04,\n",
       "       1.4404899e-03, 3.5947848e-03, 1.5366999e-03, 2.4322567e-03,\n",
       "       1.3905925e-03, 6.9182884e-04, 2.3524868e-03, 9.3137170e-04,\n",
       "       7.4496795e-04, 5.3898100e-04, 6.1364670e-04, 2.6060343e-03,\n",
       "       4.5769056e-03, 1.1734723e-03, 2.2387919e-03, 2.6387312e-03,\n",
       "       1.7145878e-03, 1.1835005e-03, 3.9443788e-03, 3.7555518e-03,\n",
       "       1.3116912e-03, 1.3572626e-03, 1.6742421e-03, 1.2767843e-03,\n",
       "       4.2494605e-04, 5.0274865e-04, 1.1795514e-03, 2.3323072e-03,\n",
       "       3.4257250e-03, 6.3570132e-03, 2.2406583e-03, 1.0364656e-03,\n",
       "       5.9795845e-04, 5.9897808e-04, 2.5466473e-03, 1.3746129e-03,\n",
       "       1.5184603e-03, 2.4153220e-03, 3.6677191e-04, 2.5352167e-03,\n",
       "       5.3207920e-04, 5.7012698e-04, 5.4789329e-04, 1.4368077e-03,\n",
       "       3.0215727e-03, 1.0713838e-03, 6.6492683e-03, 1.0352276e-03,\n",
       "       3.4963412e-04, 1.5011422e-03, 4.3135480e-04, 7.8209181e-04,\n",
       "       1.0127402e-03, 1.9633318e-03, 1.5374505e-03, 4.7589103e-03,\n",
       "       1.3569479e-03, 1.1247521e-03, 9.5982128e-04, 5.4021360e-04,\n",
       "       1.0193090e-03, 2.1056002e-03, 1.2678097e-03, 1.3480864e-03,\n",
       "       1.8499647e-03, 1.4133817e-03, 8.6643384e-04, 3.7313730e-04,\n",
       "       3.0604124e-04, 3.5548440e-04, 1.9298022e-04, 8.6083106e-04,\n",
       "       6.9769635e-04, 9.3164056e-04, 2.0982279e-03, 5.3159567e-04,\n",
       "       3.1807788e-03, 2.0141460e-03, 6.4048084e-04, 5.9092528e-04,\n",
       "       1.0928611e-03, 9.4286102e-04, 1.8096049e-03, 5.0337205e-04,\n",
       "       9.8984549e-04, 5.6587346e-04, 4.7362901e-04, 5.3830474e-04,\n",
       "       1.0130765e-03, 6.0828961e-04, 1.0492080e-03, 6.2846066e-03,\n",
       "       2.7831143e-03, 5.1403505e-04, 1.7384721e-03, 9.0267241e-04,\n",
       "       1.1187415e-03, 2.6402175e-03, 1.4813783e-03, 1.0261819e-03,\n",
       "       3.0525019e-03, 1.8078387e-03, 1.4297938e-03, 6.8102987e-04,\n",
       "       1.6295646e-03, 1.3390466e-03, 1.7338381e-03, 2.2463573e-03,\n",
       "       1.1512538e-03, 3.2766671e-03, 8.7335799e-04, 9.6865860e-04,\n",
       "       3.4334458e-04, 3.1939347e-04, 2.6540371e-04, 9.8344183e-04,\n",
       "       7.4417354e-04, 1.3612441e-03, 1.2873310e-03, 1.5000901e-03,\n",
       "       1.2786593e-03, 6.0149874e-03, 1.2621437e-03, 6.1537902e-04,\n",
       "       1.6635644e-03, 7.5762597e-04, 5.7529012e-04, 1.6868574e-03,\n",
       "       6.2278775e-04, 6.0701300e-04, 4.2271198e-04, 1.8455815e-03,\n",
       "       4.1856265e-04, 2.2398038e-03, 1.5849392e-03, 2.8027720e-03,\n",
       "       7.2514929e-04, 1.9389575e-03, 1.3168867e-03, 9.5412071e-04,\n",
       "       1.5045262e-03, 6.0213188e-04, 4.5044409e-04, 7.1942271e-04,\n",
       "       3.0735536e-03, 8.0871506e-04, 9.3146961e-04, 6.3982606e-04,\n",
       "       1.0750415e-03, 1.0735237e-03, 1.3485842e-03, 7.0637703e-04,\n",
       "       2.0138845e-03, 3.6506329e-03, 4.8446344e-04, 4.5176269e-04,\n",
       "       3.6407262e-03, 3.1089943e-04, 3.6730978e-04, 5.7353452e-03,\n",
       "       1.5664907e-03, 3.2249903e-03, 6.2687445e-04, 9.3359075e-04,\n",
       "       9.0561179e-04, 1.2801199e-03, 1.0096943e-03, 1.5447955e-03,\n",
       "       1.6515027e-03, 1.3798838e-03, 3.3785563e-03, 1.1542849e-03,\n",
       "       4.2977146e-04, 2.3811432e-03, 3.5679387e-04, 5.8502227e-04,\n",
       "       8.3445333e-04, 9.8220049e-04, 1.3199640e-03, 5.8072573e-04,\n",
       "       1.1316028e-03, 3.7701284e-03, 3.7733221e-03, 2.5081190e-03,\n",
       "       1.8889699e-03, 1.4525768e-03, 1.8993521e-03, 2.1331734e-03,\n",
       "       7.4912899e-04, 1.0749600e-03, 5.5158604e-04, 1.5294336e-03,\n",
       "       2.6557555e-03, 4.6073189e-03, 1.6807414e-04, 1.1931136e-03,\n",
       "       1.3700648e-03, 6.3732504e-03, 6.7134149e-04, 1.5823195e-03,\n",
       "       5.8490795e-04, 2.1010851e-03, 1.1987853e-03, 7.1677711e-04,\n",
       "       2.5535440e-03, 1.3999124e-03, 1.8210078e-03, 3.4054043e-04,\n",
       "       3.1048155e-03, 1.2583970e-03, 1.0286660e-03, 5.8434560e-04,\n",
       "       6.1591342e-04, 1.0377666e-03, 1.5354237e-03, 1.0040351e-03,\n",
       "       9.3735050e-04, 6.2049530e-04, 9.5946185e-04, 2.3144977e-03,\n",
       "       2.4818934e-03, 2.7430903e-03, 8.3509106e-03, 5.4182108e-03,\n",
       "       1.4625891e-03, 8.8763307e-04, 1.4279665e-03, 6.7239651e-04,\n",
       "       3.7109202e-03, 1.4741167e-03, 3.7191296e-04, 8.7298558e-04,\n",
       "       6.0979516e-04, 4.5410695e-04, 8.9496293e-04, 1.8175329e-03,\n",
       "       1.3197822e-03, 8.8133343e-04, 1.7073337e-03, 2.3971053e-03,\n",
       "       8.3357678e-04, 1.6977950e-03, 2.3267760e-03, 1.0441075e-03,\n",
       "       4.6178361e-04, 7.3975348e-04, 2.0450572e-03, 1.8177887e-03,\n",
       "       1.5937870e-03, 9.4422098e-04, 3.2429607e-04, 8.4479345e-04,\n",
       "       2.8549396e-03, 7.0660730e-04, 1.3547356e-03, 7.3433615e-04,\n",
       "       1.3807372e-03, 1.0638636e-03, 1.0735650e-03, 4.4155600e-03,\n",
       "       1.3299478e-03, 6.7060301e-04, 9.5545850e-04, 1.5962279e-03,\n",
       "       1.2426031e-03, 5.1145717e-03, 1.5205266e-03, 7.3285418e-04,\n",
       "       8.3158491e-04, 1.2003125e-03, 1.5359194e-03, 4.9691676e-04,\n",
       "       3.3136702e-04, 1.4908196e-03, 3.4156739e-04, 1.0587844e-03,\n",
       "       9.8282285e-04, 7.8329624e-04, 8.6145441e-04, 9.0476766e-04,\n",
       "       7.5646880e-04, 9.0381253e-04, 1.5645091e-03, 2.8196655e-04,\n",
       "       3.5091199e-04, 6.1996793e-03, 6.3576452e-03, 1.8724313e-03,\n",
       "       1.0485206e-03, 9.5164974e-04, 4.6477555e-03, 5.3209881e-04,\n",
       "       4.9827024e-03, 2.2016868e-03, 9.2783035e-04, 1.0710438e-03,\n",
       "       1.5174523e-03, 1.6068830e-03, 7.7498466e-04, 1.4025761e-03,\n",
       "       5.5369129e-03, 2.7940802e-03, 2.0038246e-03, 1.6658992e-03,\n",
       "       2.4095304e-04, 7.1761559e-04, 8.5121207e-04, 1.3326330e-03,\n",
       "       9.5200934e-04, 6.3800794e-04, 8.5262157e-04, 2.5070244e-03,\n",
       "       2.8185109e-03, 8.4745907e-04, 3.1725867e-03, 8.9131296e-04,\n",
       "       9.2890230e-04, 2.3295437e-03, 1.2181965e-03, 1.0624406e-03,\n",
       "       9.6708140e-04, 2.7152715e-04, 7.0082152e-04, 1.4784848e-03,\n",
       "       6.2012149e-04, 7.2645937e-04, 4.1118421e-04, 1.1137010e-03,\n",
       "       1.5820547e-03, 8.1244274e-04, 9.7060692e-04, 1.0029112e-03,\n",
       "       2.6047468e-04, 7.5019331e-04, 2.3515793e-03, 8.6234621e-04,\n",
       "       1.3259260e-03, 9.5374708e-04, 1.3438046e-03, 5.9065979e-04,\n",
       "       3.7511203e-03, 5.4158265e-04, 4.7441744e-04, 2.2912980e-04,\n",
       "       1.9882135e-03, 1.6857266e-03, 1.0158459e-03, 5.6378066e-04,\n",
       "       1.1374662e-03, 1.2126839e-03, 1.0500973e-03, 4.2631747e-03,\n",
       "       7.4435794e-04, 3.5006511e-03, 4.3672923e-04, 1.9075214e-03,\n",
       "       3.8725795e-04, 6.4315734e-04, 2.7836347e-03, 7.0316688e-04,\n",
       "       8.3335821e-04, 5.0818257e-04, 2.7852631e-04, 1.6852967e-03,\n",
       "       2.1437977e-03, 1.8877549e-03, 4.5361579e-03, 2.7081177e-03,\n",
       "       1.3460210e-03, 2.0279193e-03, 1.5348023e-03, 1.2628173e-03,\n",
       "       1.4365583e-03, 6.1582896e-04, 1.4707688e-03, 3.6029721e-04,\n",
       "       5.8775721e-04, 1.9014563e-03, 8.7623047e-03, 4.0302286e-04,\n",
       "       3.7518980e-03, 1.9492302e-03, 4.1040713e-03, 2.3010804e-03,\n",
       "       2.0299326e-03, 2.7483634e-03, 3.3816912e-03, 8.1890181e-04,\n",
       "       3.1906930e-03, 2.2124121e-04, 3.0103239e-04, 1.3258947e-04,\n",
       "       5.3687062e-04, 2.0001370e-03, 2.4259492e-04, 4.2002476e-04,\n",
       "       2.7528028e-03, 1.2817816e-03, 3.4320509e-04, 4.4233605e-04,\n",
       "       7.0291304e-04, 5.3467671e-04, 1.0965007e-03, 7.6554180e-04,\n",
       "       9.6607179e-04, 8.6603721e-04, 7.9591642e-04, 8.1281055e-04,\n",
       "       2.7839036e-03, 2.7351864e-04, 7.1451842e-04, 5.0532556e-04,\n",
       "       8.2611450e-04, 5.8694754e-04, 2.7509904e-03, 4.7696568e-03,\n",
       "       2.2688890e-03, 1.2182239e-03, 1.2021548e-03, 4.2278669e-03,\n",
       "       1.9713484e-03, 1.7835521e-03, 7.0701912e-03, 5.0612050e-04,\n",
       "       1.2395714e-03, 3.5975338e-04, 5.6440849e-04, 9.4927463e-04,\n",
       "       2.0607209e-03, 7.5078587e-04, 1.0767072e-03, 8.7490835e-04,\n",
       "       1.5171836e-03, 2.6767389e-03, 1.1821222e-03, 6.7316921e-04,\n",
       "       6.0309225e-04, 3.3598924e-03, 2.2395800e-03, 1.5181028e-03,\n",
       "       1.4059573e-03, 1.0333711e-03, 6.3787424e-04, 2.8696808e-04,\n",
       "       8.9593086e-04, 1.6993659e-03, 2.4002083e-03, 1.6325670e-03,\n",
       "       9.5199491e-04, 2.7842885e-03, 1.2286125e-03, 5.9178972e-04,\n",
       "       1.3418274e-03, 9.8624825e-04, 1.4348077e-03, 6.4183288e-04,\n",
       "       7.9186325e-04, 1.1225411e-03, 9.0505194e-04, 4.0077651e-04,\n",
       "       4.0662647e-04, 5.7209289e-04, 3.1871004e-03, 5.0800329e-04,\n",
       "       9.3517016e-04, 1.8840529e-04, 6.7892397e-04, 6.8412942e-04,\n",
       "       8.0852589e-04, 3.9490056e-04, 6.4841355e-04, 2.2292235e-03,\n",
       "       4.7942391e-04, 1.5017488e-03, 4.3811981e-04, 5.0632772e-04,\n",
       "       6.0787547e-04, 4.2033225e-04, 2.3646507e-04, 7.1082212e-04,\n",
       "       3.5332036e-03, 1.2599614e-03, 6.4560102e-04, 3.8444702e-04,\n",
       "       8.1571011e-04, 1.2589173e-03, 6.9124158e-04, 1.6777334e-03,\n",
       "       2.6770213e-03, 6.3892524e-04, 1.6204049e-03, 1.9820677e-03,\n",
       "       3.6845764e-04, 1.6898278e-03, 3.9477984e-04, 3.4809008e-04,\n",
       "       3.1926887e-04, 1.2225449e-03, 1.9298527e-04, 9.0297763e-05,\n",
       "       3.5189092e-03, 9.0174749e-04, 4.3586839e-04, 1.2500547e-03,\n",
       "       7.3764502e-04, 4.7896378e-04, 6.5828726e-04, 2.3934580e-03,\n",
       "       8.5517683e-04, 1.4124927e-03, 5.0670619e-04, 3.6832402e-03,\n",
       "       1.2229250e-03, 8.6922117e-04, 5.5488560e-04, 1.0425332e-03,\n",
       "       5.6366692e-04, 1.4337116e-03, 8.8198681e-04, 3.3154670e-04,\n",
       "       1.1719716e-03, 5.4475991e-04, 9.6498511e-04, 8.5827190e-04,\n",
       "       2.7057095e-03, 1.8286933e-03, 6.9168000e-04, 2.5528780e-04,\n",
       "       3.6113730e-03, 1.4815114e-03, 1.5048847e-03, 1.2143364e-03,\n",
       "       1.5233273e-03, 1.7911678e-03, 1.8058188e-03, 6.3646003e-04,\n",
       "       8.0147426e-04, 5.5085955e-04, 4.6468308e-04, 1.4296084e-03,\n",
       "       8.8153582e-04, 2.5615495e-04, 1.1584577e-03, 8.1345573e-04,\n",
       "       1.4703596e-03, 6.7129120e-04, 1.5286619e-03, 8.7449211e-04,\n",
       "       1.3891327e-03, 2.3712395e-03, 1.8438055e-03, 5.8699801e-04,\n",
       "       4.9062853e-04, 2.3327051e-03, 1.3730939e-03, 4.8379475e-04,\n",
       "       7.4545073e-04, 1.7769432e-03, 9.1620587e-04, 4.6588818e-04,\n",
       "       3.0590568e-04, 1.2735748e-03, 4.9415475e-04, 5.5538135e-04,\n",
       "       1.9163764e-03, 1.2200236e-03, 2.9093813e-04, 5.6808081e-04,\n",
       "       5.3610094e-04, 4.4800821e-03, 4.0229857e-03, 3.5392211e-04,\n",
       "       3.0123589e-03, 1.8879896e-03, 8.2961493e-04, 2.5749083e-03,\n",
       "       7.2350859e-04, 4.6725658e-04, 4.5991177e-03, 1.1045048e-03,\n",
       "       8.1636786e-04, 5.4341382e-03, 2.4451632e-03, 1.8674686e-03,\n",
       "       3.7308575e-03, 4.8511749e-04, 1.4691298e-03, 2.1665904e-03,\n",
       "       8.3072606e-04, 6.2659080e-04, 7.2904309e-04, 8.7101595e-04,\n",
       "       4.3016570e-04, 1.1096726e-03, 1.3579772e-03, 1.4713199e-03,\n",
       "       3.7231384e-04, 1.1835045e-03, 1.7342768e-03, 6.7381263e-03,\n",
       "       1.0615562e-03, 1.6021230e-03, 7.1065046e-04, 5.3620635e-04,\n",
       "       2.2508819e-03, 3.8007912e-04, 8.8030222e-04, 2.5354687e-04,\n",
       "       2.7413897e-03, 1.4858081e-03, 3.5788247e-03, 1.0372134e-03,\n",
       "       4.6584263e-04, 1.6793230e-03, 5.5176619e-04, 1.1740304e-03,\n",
       "       6.3015166e-04, 1.8999750e-03, 8.6868770e-04, 7.3926186e-04,\n",
       "       3.3323802e-03, 3.3914754e-03, 2.9975362e-03, 1.1779227e-03,\n",
       "       1.8344750e-03, 1.1701499e-03, 9.4856072e-04, 1.9168752e-03,\n",
       "       1.8111817e-03, 2.4978083e-03, 1.6441460e-03, 2.0672376e-03,\n",
       "       1.1117801e-03, 1.0685374e-03, 1.1102092e-03, 3.5895477e-03,\n",
       "       5.3552492e-04, 7.0238914e-03, 1.3198957e-03, 7.4573304e-04,\n",
       "       1.1863629e-03, 2.0014567e-03, 8.7981886e-04, 1.3068783e-04,\n",
       "       6.8491563e-04, 6.3952268e-04, 9.0072234e-04, 1.1887442e-03,\n",
       "       9.5729955e-04, 6.9862622e-04, 9.1704156e-04, 2.0557265e-03,\n",
       "       1.4598804e-03, 1.0091028e-03, 3.7449904e-04, 1.1851848e-03,\n",
       "       1.4731359e-03, 6.0202270e-03, 2.7205388e-03, 1.1281809e-03,\n",
       "       9.2886266e-04, 1.1043027e-03, 4.3200867e-04, 1.9266908e-03,\n",
       "       1.0649480e-03, 4.6716692e-04, 2.7940182e-03, 3.7507620e-04,\n",
       "       8.2643091e-04, 6.2631600e-04, 4.7052624e-03, 1.4616319e-03,\n",
       "       1.2172445e-03, 2.3852193e-03, 3.6595936e-03, 5.7072594e-04,\n",
       "       7.7917235e-04, 4.2441741e-04, 6.5185205e-04, 8.7247894e-04,\n",
       "       7.2806742e-04, 1.3305238e-03, 7.6007727e-04, 9.1882673e-04,\n",
       "       1.6735672e-04, 4.4075560e-04, 8.1883057e-04, 3.4045835e-04,\n",
       "       7.0911803e-04, 1.7533246e-03, 7.3621655e-04, 1.0071443e-03,\n",
       "       1.4928090e-03, 6.4765173e-04, 4.2002387e-03, 1.1360295e-03,\n",
       "       4.2509948e-04, 1.0347251e-03, 4.9022696e-04, 1.1521843e-03,\n",
       "       1.3153460e-03, 7.8488950e-04, 3.2167353e-03, 4.5361617e-04,\n",
       "       1.1705518e-03, 6.9833139e-04, 1.2712282e-03, 1.2397579e-03,\n",
       "       3.7248395e-04, 3.0885276e-04, 4.5785907e-04, 1.1212640e-04,\n",
       "       1.4400422e-03, 9.2185859e-04, 8.0889958e-04, 1.0378810e-03,\n",
       "       6.2914367e-04, 9.0323121e-04, 2.7592370e-04, 4.0596226e-04,\n",
       "       2.2871022e-03, 7.3262374e-04, 5.7369529e-04, 2.9755009e-03,\n",
       "       5.5389717e-04, 7.5285160e-04, 3.6478387e-03, 2.5740682e-04,\n",
       "       1.5329381e-03, 8.5733912e-04, 3.6622426e-03, 7.9537398e-04,\n",
       "       8.6068228e-04, 1.1314600e-03, 1.0069946e-03, 6.4806012e-04,\n",
       "       6.9749792e-04, 2.3538014e-03, 1.8939365e-03, 3.6102723e-04,\n",
       "       1.7958637e-03, 8.7042444e-04, 1.8922181e-03, 7.2359509e-04,\n",
       "       1.6772829e-03, 8.0433412e-04, 1.7576911e-03, 2.3980290e-03,\n",
       "       3.2036235e-03, 6.2757015e-04, 5.6496821e-04], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c246c6e-5d46-41be-afda-68d6dafb4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to alternatively compute the variance in one line\n",
    "alt_test_bts_variance = np.var(all_test_preds_array, axis = 0, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5b0034f-6e56-471b-8815-7c744079d978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_test_bts_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18b9fda9-37d0-4fec-91b6-f731f55df205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.3056750e-04, 5.4126582e-04, 7.3728670e-04, 2.3910769e-03,\n",
       "       1.2964581e-03, 2.0693133e-03, 1.7236081e-03, 4.0479396e-03,\n",
       "       7.3798490e-04, 9.4396499e-04, 4.6927575e-04, 1.1376116e-03,\n",
       "       1.1319038e-03, 1.0751548e-03, 4.0510131e-04, 9.1339211e-04,\n",
       "       4.4738321e-04, 4.1118107e-04, 6.1193370e-04, 1.0366106e-03,\n",
       "       1.6892441e-03, 1.4544724e-03, 5.7380856e-04, 2.0091825e-03,\n",
       "       1.3613458e-03, 1.3245464e-03, 4.0226692e-04, 3.4186421e-03,\n",
       "       9.3519076e-04, 3.3677855e-04, 3.9706362e-04, 7.3917158e-04,\n",
       "       1.1229626e-03, 3.3812751e-03, 5.4388738e-04, 4.8154328e-04,\n",
       "       1.2786197e-03, 8.0953725e-04, 1.6883966e-03, 1.5954311e-03,\n",
       "       1.3278312e-03, 7.0237753e-04, 7.6777092e-04, 4.6900049e-04,\n",
       "       2.9251126e-03, 4.3809963e-03, 1.0897730e-03, 3.3490313e-03,\n",
       "       1.2390179e-03, 1.3006560e-03, 5.3913714e-03, 9.3830633e-04,\n",
       "       1.6804704e-03, 1.3012583e-03, 8.3899614e-04, 7.0608687e-03,\n",
       "       1.9761466e-03, 1.0441263e-03, 3.0904261e-03, 6.0246279e-04,\n",
       "       4.9685471e-04, 8.8865019e-04, 1.6715024e-03, 5.1982200e-04,\n",
       "       1.5053997e-03, 3.2418752e-03, 1.7582248e-03, 8.0119277e-04,\n",
       "       7.3334645e-04, 9.7093009e-04, 2.3624459e-03, 7.9011574e-04,\n",
       "       1.2380185e-03, 3.4004806e-03, 8.9455518e-04, 4.0172050e-03,\n",
       "       1.2377334e-03, 1.3906369e-03, 2.0140598e-03, 6.6937116e-04,\n",
       "       3.4417113e-04, 2.6107584e-03, 6.5609108e-04, 6.0235115e-04,\n",
       "       6.7960366e-04, 1.3336198e-03, 8.1023807e-03, 5.8685325e-04,\n",
       "       1.3296694e-03, 5.1342871e-04, 1.0049741e-03, 1.1454229e-03,\n",
       "       1.7307616e-04, 1.4636128e-03, 1.3603717e-03, 6.1048884e-03,\n",
       "       1.3685980e-03, 8.8154909e-04, 1.2937079e-03, 4.8924086e-04,\n",
       "       3.4942084e-03, 1.7204365e-03, 2.4582439e-03, 5.1647064e-04,\n",
       "       5.3013989e-04, 3.4360883e-03, 1.0579744e-03, 4.9607025e-04,\n",
       "       1.4349367e-03, 7.6751033e-04, 6.3240482e-04, 5.9880991e-04,\n",
       "       9.5186726e-04, 6.5444596e-04, 1.0251410e-03, 3.2715134e-03,\n",
       "       3.1573595e-03, 3.7327099e-03, 9.9465577e-04, 4.1765699e-04,\n",
       "       3.1494012e-04, 3.9849099e-04, 4.9053982e-04, 6.0536916e-04,\n",
       "       1.4666027e-03, 1.9226917e-03, 1.7797335e-03, 6.4740115e-04,\n",
       "       3.7791734e-03, 4.1073943e-03, 9.8702009e-04, 3.2778545e-03,\n",
       "       7.0930395e-04, 8.1762590e-04, 4.8016044e-03, 8.3180773e-04,\n",
       "       1.3672267e-03, 1.1674004e-03, 3.5108754e-04, 8.5928635e-04,\n",
       "       4.1114163e-04, 4.6258827e-04, 3.5633901e-03, 7.0028001e-04,\n",
       "       2.5282770e-03, 1.6525489e-03, 1.0018266e-03, 8.1694435e-04,\n",
       "       2.8544469e-03, 4.5575420e-03, 1.3825704e-03, 8.7336107e-04,\n",
       "       2.5502532e-03, 6.7815883e-04, 5.9534900e-04, 1.0396200e-03,\n",
       "       8.4708352e-04, 1.6735485e-03, 1.1682432e-03, 1.0807370e-03,\n",
       "       9.5505104e-04, 3.6382835e-04, 6.5346219e-04, 1.3783234e-03,\n",
       "       6.8642513e-04, 6.4208906e-04, 2.7048467e-03, 7.6470658e-04,\n",
       "       1.0890063e-03, 5.6475337e-04, 7.0426200e-04, 7.5996865e-04,\n",
       "       8.5115735e-04, 5.7283469e-04, 1.1080208e-03, 1.2218752e-03,\n",
       "       1.6144542e-03, 7.2128244e-04, 8.5834542e-04, 6.0305721e-04,\n",
       "       5.3483137e-04, 1.6120211e-03, 7.8333088e-04, 1.8511441e-03,\n",
       "       3.4479902e-04, 1.1683548e-03, 1.7686675e-03, 1.3496795e-03,\n",
       "       1.8321427e-03, 2.5469382e-04, 3.7945280e-04, 1.6806292e-03,\n",
       "       4.5578682e-04, 1.6823565e-03, 2.1575701e-03, 7.2142109e-04,\n",
       "       4.4644563e-04, 6.7769724e-04, 7.3777302e-04, 6.3407701e-04,\n",
       "       1.4404899e-03, 3.5947848e-03, 1.5366999e-03, 2.4322572e-03,\n",
       "       1.3905927e-03, 6.9182896e-04, 2.3524868e-03, 9.3137182e-04,\n",
       "       7.4496795e-04, 5.3898100e-04, 6.1364670e-04, 2.6060343e-03,\n",
       "       4.5769052e-03, 1.1734724e-03, 2.2387919e-03, 2.6387309e-03,\n",
       "       1.7145878e-03, 1.1835005e-03, 3.9443788e-03, 3.7555513e-03,\n",
       "       1.3116912e-03, 1.3572626e-03, 1.6742419e-03, 1.2767843e-03,\n",
       "       4.2494608e-04, 5.0274865e-04, 1.1795512e-03, 2.3323072e-03,\n",
       "       3.4257248e-03, 6.3570132e-03, 2.2406585e-03, 1.0364656e-03,\n",
       "       5.9795845e-04, 5.9897808e-04, 2.5466473e-03, 1.3746130e-03,\n",
       "       1.5184602e-03, 2.4153220e-03, 3.6677191e-04, 2.5352167e-03,\n",
       "       5.3207920e-04, 5.7012687e-04, 5.4789329e-04, 1.4368077e-03,\n",
       "       3.0215729e-03, 1.0713838e-03, 6.6492679e-03, 1.0352274e-03,\n",
       "       3.4963409e-04, 1.5011422e-03, 4.3135486e-04, 7.8209181e-04,\n",
       "       1.0127402e-03, 1.9633316e-03, 1.5374505e-03, 4.7589103e-03,\n",
       "       1.3569481e-03, 1.1247520e-03, 9.5982128e-04, 5.4021360e-04,\n",
       "       1.0193090e-03, 2.1056002e-03, 1.2678097e-03, 1.3480864e-03,\n",
       "       1.8499647e-03, 1.4133817e-03, 8.6643390e-04, 3.7313730e-04,\n",
       "       3.0604127e-04, 3.5548440e-04, 1.9298022e-04, 8.6083106e-04,\n",
       "       6.9769635e-04, 9.3164056e-04, 2.0982281e-03, 5.3159567e-04,\n",
       "       3.1807788e-03, 2.0141460e-03, 6.4048084e-04, 5.9092528e-04,\n",
       "       1.0928611e-03, 9.4286102e-04, 1.8096049e-03, 5.0337205e-04,\n",
       "       9.8984537e-04, 5.6587346e-04, 4.7362901e-04, 5.3830474e-04,\n",
       "       1.0130765e-03, 6.0828961e-04, 1.0492080e-03, 6.2846062e-03,\n",
       "       2.7831143e-03, 5.1403511e-04, 1.7384721e-03, 9.0267241e-04,\n",
       "       1.1187415e-03, 2.6402175e-03, 1.4813782e-03, 1.0261819e-03,\n",
       "       3.0525019e-03, 1.8078387e-03, 1.4297938e-03, 6.8102981e-04,\n",
       "       1.6295647e-03, 1.3390468e-03, 1.7338381e-03, 2.2463573e-03,\n",
       "       1.1512538e-03, 3.2766676e-03, 8.7335799e-04, 9.6865860e-04,\n",
       "       3.4334458e-04, 3.1939347e-04, 2.6540371e-04, 9.8344195e-04,\n",
       "       7.4417354e-04, 1.3612442e-03, 1.2873310e-03, 1.5000902e-03,\n",
       "       1.2786593e-03, 6.0149874e-03, 1.2621437e-03, 6.1537896e-04,\n",
       "       1.6635644e-03, 7.5762597e-04, 5.7529006e-04, 1.6868574e-03,\n",
       "       6.2278775e-04, 6.0701300e-04, 4.2271192e-04, 1.8455815e-03,\n",
       "       4.1856267e-04, 2.2398038e-03, 1.5849392e-03, 2.8027717e-03,\n",
       "       7.2514929e-04, 1.9389577e-03, 1.3168867e-03, 9.5412071e-04,\n",
       "       1.5045262e-03, 6.0213188e-04, 4.5044409e-04, 7.1942271e-04,\n",
       "       3.0735538e-03, 8.0871512e-04, 9.3146949e-04, 6.3982600e-04,\n",
       "       1.0750415e-03, 1.0735237e-03, 1.3485843e-03, 7.0637703e-04,\n",
       "       2.0138845e-03, 3.6506329e-03, 4.8446344e-04, 4.5176269e-04,\n",
       "       3.6407262e-03, 3.1089943e-04, 3.6730978e-04, 5.7353452e-03,\n",
       "       1.5664907e-03, 3.2249903e-03, 6.2687451e-04, 9.3359075e-04,\n",
       "       9.0561179e-04, 1.2801198e-03, 1.0096943e-03, 1.5447955e-03,\n",
       "       1.6515027e-03, 1.3798842e-03, 3.3785566e-03, 1.1542849e-03,\n",
       "       4.2977149e-04, 2.3811436e-03, 3.5679387e-04, 5.8502227e-04,\n",
       "       8.3445333e-04, 9.8220038e-04, 1.3199640e-03, 5.8072578e-04,\n",
       "       1.1316028e-03, 3.7701284e-03, 3.7733221e-03, 2.5081190e-03,\n",
       "       1.8889699e-03, 1.4525768e-03, 1.8993521e-03, 2.1331734e-03,\n",
       "       7.4912905e-04, 1.0749602e-03, 5.5158604e-04, 1.5294335e-03,\n",
       "       2.6557557e-03, 4.6073189e-03, 1.6807417e-04, 1.1931137e-03,\n",
       "       1.3700648e-03, 6.3732509e-03, 6.7134149e-04, 1.5823195e-03,\n",
       "       5.8490789e-04, 2.1010851e-03, 1.1987853e-03, 7.1677711e-04,\n",
       "       2.5535440e-03, 1.3999124e-03, 1.8210076e-03, 3.4054049e-04,\n",
       "       3.1048155e-03, 1.2583970e-03, 1.0286660e-03, 5.8434554e-04,\n",
       "       6.1591348e-04, 1.0377666e-03, 1.5354236e-03, 1.0040351e-03,\n",
       "       9.3735050e-04, 6.2049530e-04, 9.5946185e-04, 2.3144977e-03,\n",
       "       2.4818936e-03, 2.7430903e-03, 8.3509106e-03, 5.4182112e-03,\n",
       "       1.4625891e-03, 8.8763301e-04, 1.4279666e-03, 6.7239651e-04,\n",
       "       3.7109207e-03, 1.4741168e-03, 3.7191293e-04, 8.7298546e-04,\n",
       "       6.0979516e-04, 4.5410698e-04, 8.9496293e-04, 1.8175329e-03,\n",
       "       1.3197822e-03, 8.8133354e-04, 1.7073337e-03, 2.3971053e-03,\n",
       "       8.3357678e-04, 1.6977951e-03, 2.3267760e-03, 1.0441075e-03,\n",
       "       4.6178361e-04, 7.3975348e-04, 2.0450572e-03, 1.8177887e-03,\n",
       "       1.5937870e-03, 9.4422098e-04, 3.2429607e-04, 8.4479345e-04,\n",
       "       2.8549396e-03, 7.0660730e-04, 1.3547356e-03, 7.3433615e-04,\n",
       "       1.3807372e-03, 1.0638636e-03, 1.0735651e-03, 4.4155600e-03,\n",
       "       1.3299478e-03, 6.7060307e-04, 9.5545850e-04, 1.5962278e-03,\n",
       "       1.2426029e-03, 5.1145717e-03, 1.5205266e-03, 7.3285424e-04,\n",
       "       8.3158491e-04, 1.2003124e-03, 1.5359194e-03, 4.9691676e-04,\n",
       "       3.3136702e-04, 1.4908196e-03, 3.4156741e-04, 1.0587844e-03,\n",
       "       9.8282285e-04, 7.8329630e-04, 8.6145435e-04, 9.0476766e-04,\n",
       "       7.5646886e-04, 9.0381253e-04, 1.5645091e-03, 2.8196655e-04,\n",
       "       3.5091199e-04, 6.1996784e-03, 6.3576452e-03, 1.8724313e-03,\n",
       "       1.0485207e-03, 9.5164962e-04, 4.6477551e-03, 5.3209881e-04,\n",
       "       4.9827020e-03, 2.2016871e-03, 9.2783035e-04, 1.0710438e-03,\n",
       "       1.5174524e-03, 1.6068830e-03, 7.7498466e-04, 1.4025760e-03,\n",
       "       5.5369129e-03, 2.7940804e-03, 2.0038246e-03, 1.6658991e-03,\n",
       "       2.4095304e-04, 7.1761559e-04, 8.5121195e-04, 1.3326330e-03,\n",
       "       9.5200934e-04, 6.3800794e-04, 8.5262157e-04, 2.5070242e-03,\n",
       "       2.8185109e-03, 8.4745907e-04, 3.1725867e-03, 8.9131296e-04,\n",
       "       9.2890218e-04, 2.3295435e-03, 1.2181965e-03, 1.0624406e-03,\n",
       "       9.6708135e-04, 2.7152715e-04, 7.0082152e-04, 1.4784848e-03,\n",
       "       6.2012143e-04, 7.2645932e-04, 4.1118421e-04, 1.1137010e-03,\n",
       "       1.5820547e-03, 8.1244268e-04, 9.7060692e-04, 1.0029112e-03,\n",
       "       2.6047468e-04, 7.5019326e-04, 2.3515793e-03, 8.6234621e-04,\n",
       "       1.3259260e-03, 9.5374708e-04, 1.3438046e-03, 5.9065974e-04,\n",
       "       3.7511203e-03, 5.4158265e-04, 4.7441738e-04, 2.2912980e-04,\n",
       "       1.9882135e-03, 1.6857267e-03, 1.0158459e-03, 5.6378066e-04,\n",
       "       1.1374662e-03, 1.2126839e-03, 1.0500973e-03, 4.2631752e-03,\n",
       "       7.4435794e-04, 3.5006511e-03, 4.3672917e-04, 1.9075214e-03,\n",
       "       3.8725795e-04, 6.4315734e-04, 2.7836347e-03, 7.0316694e-04,\n",
       "       8.3335827e-04, 5.0818257e-04, 2.7852631e-04, 1.6852967e-03,\n",
       "       2.1437975e-03, 1.8877550e-03, 4.5361579e-03, 2.7081177e-03,\n",
       "       1.3460211e-03, 2.0279193e-03, 1.5348023e-03, 1.2628173e-03,\n",
       "       1.4365583e-03, 6.1582896e-04, 1.4707687e-03, 3.6029718e-04,\n",
       "       5.8775727e-04, 1.9014563e-03, 8.7623047e-03, 4.0302286e-04,\n",
       "       3.7518980e-03, 1.9492300e-03, 4.1040718e-03, 2.3010804e-03,\n",
       "       2.0299326e-03, 2.7483630e-03, 3.3816914e-03, 8.1890181e-04,\n",
       "       3.1906932e-03, 2.2124121e-04, 3.0103239e-04, 1.3258948e-04,\n",
       "       5.3687056e-04, 2.0001368e-03, 2.4259492e-04, 4.2002476e-04,\n",
       "       2.7528026e-03, 1.2817816e-03, 3.4320509e-04, 4.4233611e-04,\n",
       "       7.0291304e-04, 5.3467671e-04, 1.0965007e-03, 7.6554180e-04,\n",
       "       9.6607179e-04, 8.6603721e-04, 7.9591642e-04, 8.1281061e-04,\n",
       "       2.7839036e-03, 2.7351867e-04, 7.1451848e-04, 5.0532556e-04,\n",
       "       8.2611450e-04, 5.8694760e-04, 2.7509902e-03, 4.7696563e-03,\n",
       "       2.2688892e-03, 1.2182239e-03, 1.2021549e-03, 4.2278673e-03,\n",
       "       1.9713484e-03, 1.7835521e-03, 7.0701921e-03, 5.0612050e-04,\n",
       "       1.2395714e-03, 3.5975338e-04, 5.6440843e-04, 9.4927463e-04,\n",
       "       2.0607209e-03, 7.5078587e-04, 1.0767072e-03, 8.7490835e-04,\n",
       "       1.5171835e-03, 2.6767387e-03, 1.1821222e-03, 6.7316921e-04,\n",
       "       6.0309225e-04, 3.3598926e-03, 2.2395800e-03, 1.5181028e-03,\n",
       "       1.4059574e-03, 1.0333711e-03, 6.3787424e-04, 2.8696808e-04,\n",
       "       8.9593086e-04, 1.6993659e-03, 2.4002080e-03, 1.6325670e-03,\n",
       "       9.5199491e-04, 2.7842887e-03, 1.2286125e-03, 5.9178972e-04,\n",
       "       1.3418273e-03, 9.8624825e-04, 1.4348077e-03, 6.4183288e-04,\n",
       "       7.9186325e-04, 1.1225413e-03, 9.0505194e-04, 4.0077654e-04,\n",
       "       4.0662647e-04, 5.7209289e-04, 3.1871004e-03, 5.0800329e-04,\n",
       "       9.3517016e-04, 1.8840529e-04, 6.7892391e-04, 6.8412942e-04,\n",
       "       8.0852583e-04, 3.9490056e-04, 6.4841355e-04, 2.2292235e-03,\n",
       "       4.7942391e-04, 1.5017488e-03, 4.3811981e-04, 5.0632772e-04,\n",
       "       6.0787541e-04, 4.2033228e-04, 2.3646507e-04, 7.1082212e-04,\n",
       "       3.5332036e-03, 1.2599615e-03, 6.4560102e-04, 3.8444699e-04,\n",
       "       8.1571005e-04, 1.2589173e-03, 6.9124158e-04, 1.6777334e-03,\n",
       "       2.6770213e-03, 6.3892524e-04, 1.6204048e-03, 1.9820677e-03,\n",
       "       3.6845761e-04, 1.6898278e-03, 3.9477984e-04, 3.4809008e-04,\n",
       "       3.1926887e-04, 1.2225448e-03, 1.9298527e-04, 9.0297755e-05,\n",
       "       3.5189097e-03, 9.0174749e-04, 4.3586834e-04, 1.2500546e-03,\n",
       "       7.3764502e-04, 4.7896378e-04, 6.5828726e-04, 2.3934580e-03,\n",
       "       8.5517688e-04, 1.4124927e-03, 5.0670619e-04, 3.6832402e-03,\n",
       "       1.2229250e-03, 8.6922117e-04, 5.5488560e-04, 1.0425332e-03,\n",
       "       5.6366692e-04, 1.4337115e-03, 8.8198681e-04, 3.3154670e-04,\n",
       "       1.1719716e-03, 5.4475991e-04, 9.6498523e-04, 8.5827190e-04,\n",
       "       2.7057095e-03, 1.8286933e-03, 6.9168006e-04, 2.5528783e-04,\n",
       "       3.6113735e-03, 1.4815114e-03, 1.5048846e-03, 1.2143364e-03,\n",
       "       1.5233274e-03, 1.7911678e-03, 1.8058188e-03, 6.3646003e-04,\n",
       "       8.0147426e-04, 5.5085955e-04, 4.6468308e-04, 1.4296083e-03,\n",
       "       8.8153570e-04, 2.5615492e-04, 1.1584577e-03, 8.1345567e-04,\n",
       "       1.4703596e-03, 6.7129108e-04, 1.5286619e-03, 8.7449211e-04,\n",
       "       1.3891328e-03, 2.3712392e-03, 1.8438055e-03, 5.8699801e-04,\n",
       "       4.9062853e-04, 2.3327051e-03, 1.3730939e-03, 4.8379475e-04,\n",
       "       7.4545084e-04, 1.7769432e-03, 9.1620587e-04, 4.6588812e-04,\n",
       "       3.0590565e-04, 1.2735748e-03, 4.9415475e-04, 5.5538135e-04,\n",
       "       1.9163764e-03, 1.2200236e-03, 2.9093813e-04, 5.6808075e-04,\n",
       "       5.3610094e-04, 4.4800816e-03, 4.0229857e-03, 3.5392211e-04,\n",
       "       3.0123584e-03, 1.8879896e-03, 8.2961499e-04, 2.5749085e-03,\n",
       "       7.2350871e-04, 4.6725658e-04, 4.5991181e-03, 1.1045048e-03,\n",
       "       8.1636786e-04, 5.4341382e-03, 2.4451630e-03, 1.8674686e-03,\n",
       "       3.7308575e-03, 4.8511749e-04, 1.4691298e-03, 2.1665904e-03,\n",
       "       8.3072606e-04, 6.2659080e-04, 7.2904309e-04, 8.7101595e-04,\n",
       "       4.3016570e-04, 1.1096726e-03, 1.3579769e-03, 1.4713199e-03,\n",
       "       3.7231384e-04, 1.1835045e-03, 1.7342770e-03, 6.7381267e-03,\n",
       "       1.0615560e-03, 1.6021230e-03, 7.1065046e-04, 5.3620635e-04,\n",
       "       2.2508819e-03, 3.8007912e-04, 8.8030234e-04, 2.5354687e-04,\n",
       "       2.7413894e-03, 1.4858079e-03, 3.5788252e-03, 1.0372134e-03,\n",
       "       4.6584263e-04, 1.6793230e-03, 5.5176625e-04, 1.1740304e-03,\n",
       "       6.3015166e-04, 1.8999749e-03, 8.6868770e-04, 7.3926180e-04,\n",
       "       3.3323802e-03, 3.3914754e-03, 2.9975362e-03, 1.1779227e-03,\n",
       "       1.8344750e-03, 1.1701499e-03, 9.4856072e-04, 1.9168750e-03,\n",
       "       1.8111817e-03, 2.4978085e-03, 1.6441460e-03, 2.0672379e-03,\n",
       "       1.1117801e-03, 1.0685375e-03, 1.1102092e-03, 3.5895477e-03,\n",
       "       5.3552486e-04, 7.0238914e-03, 1.3198955e-03, 7.4573304e-04,\n",
       "       1.1863630e-03, 2.0014567e-03, 8.7981881e-04, 1.3068781e-04,\n",
       "       6.8491558e-04, 6.3952268e-04, 9.0072240e-04, 1.1887442e-03,\n",
       "       9.5729955e-04, 6.9862622e-04, 9.1704156e-04, 2.0557265e-03,\n",
       "       1.4598804e-03, 1.0091029e-03, 3.7449904e-04, 1.1851848e-03,\n",
       "       1.4731360e-03, 6.0202265e-03, 2.7205388e-03, 1.1281808e-03,\n",
       "       9.2886266e-04, 1.1043026e-03, 4.3200864e-04, 1.9266908e-03,\n",
       "       1.0649482e-03, 4.6716689e-04, 2.7940180e-03, 3.7507626e-04,\n",
       "       8.2643091e-04, 6.2631600e-04, 4.7052624e-03, 1.4616319e-03,\n",
       "       1.2172447e-03, 2.3852193e-03, 3.6595936e-03, 5.7072588e-04,\n",
       "       7.7917235e-04, 4.2441741e-04, 6.5185211e-04, 8.7247905e-04,\n",
       "       7.2806742e-04, 1.3305238e-03, 7.6007727e-04, 9.1882661e-04,\n",
       "       1.6735672e-04, 4.4075560e-04, 8.1883057e-04, 3.4045835e-04,\n",
       "       7.0911803e-04, 1.7533244e-03, 7.3621655e-04, 1.0071443e-03,\n",
       "       1.4928090e-03, 6.4765173e-04, 4.2002387e-03, 1.1360295e-03,\n",
       "       4.2509951e-04, 1.0347251e-03, 4.9022696e-04, 1.1521843e-03,\n",
       "       1.3153460e-03, 7.8488950e-04, 3.2167353e-03, 4.5361617e-04,\n",
       "       1.1705519e-03, 6.9833139e-04, 1.2712281e-03, 1.2397578e-03,\n",
       "       3.7248395e-04, 3.0885276e-04, 4.5785907e-04, 1.1212640e-04,\n",
       "       1.4400422e-03, 9.2185871e-04, 8.0889958e-04, 1.0378809e-03,\n",
       "       6.2914367e-04, 9.0323121e-04, 2.7592367e-04, 4.0596226e-04,\n",
       "       2.2871022e-03, 7.3262374e-04, 5.7369529e-04, 2.9755011e-03,\n",
       "       5.5389723e-04, 7.5285160e-04, 3.6478378e-03, 2.5740685e-04,\n",
       "       1.5329381e-03, 8.5733918e-04, 3.6622426e-03, 7.9537398e-04,\n",
       "       8.6068228e-04, 1.1314600e-03, 1.0069946e-03, 6.4806012e-04,\n",
       "       6.9749786e-04, 2.3538014e-03, 1.8939365e-03, 3.6102725e-04,\n",
       "       1.7958637e-03, 8.7042444e-04, 1.8922181e-03, 7.2359509e-04,\n",
       "       1.6772829e-03, 8.0433406e-04, 1.7576913e-03, 2.3980292e-03,\n",
       "       3.2036235e-03, 6.2757015e-04, 5.6496821e-04], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_test_bts_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8a52e8e4-6c0a-4b63-85d4-427c305e65a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.round(test_bts_variance, 6) == np.round(alt_test_bts_variance, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dc52cd39-5a56-447c-a175-6f3531bf0424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.029776e-05, 0.008762305)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the minimum and the maximum values here?\n",
    "np.min(test_bts_variance), np.max(test_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "715d2502-9ec6-4dd3-ab2b-c25b480e2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the means\n",
    "catch_train_mean = []\n",
    "for i in range(all_train_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_train_preds_array[:,i])\n",
    "    catch_train_mean.append(computed_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5f41e04-4957-4feb-9684-4faac1d51ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_train_means = np.array(catch_train_mean)\n",
    "np.mean(np.round(train_bts_mean,2) == np.round(sanity_check_train_means, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75d80d65-0758-4c7a-9dc7-8fb5a19a622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the validation means\n",
    "catch_valid_mean = []\n",
    "for i in range(all_valid_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_valid_preds_array[:,i])\n",
    "    catch_valid_mean.append(computed_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7440535e-e19c-4715-86a9-64f153a03af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_valid_means = np.array(catch_valid_mean)\n",
    "np.mean(np.round(valid_bts_mean, 3) == np.round(sanity_check_valid_means, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55ed1b47-6abd-4aa8-a1de-46ce492007eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the test means\n",
    "catch_test_mean = []\n",
    "for i in range(all_test_preds_array.shape[1]):\n",
    "    computed_mean = np.mean(all_test_preds_array[:,i])\n",
    "    catch_test_mean.append(computed_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9994b2dc-0cb3-423a-b558-1d4648dc223f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check_test_means = np.array(catch_test_mean)\n",
    "np.mean(np.round(test_bts_mean,3) == np.round(sanity_check_test_means, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b1a3c7ea-73dd-4210-865a-d3cc317e4fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all test bootstrap means\n",
    "test_bts_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68a0176a-3f4e-472b-a873-fa630aa36ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we have the variances and the means, now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e74b99c0-480f-4b13-9628-6b1c217be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute the rsquare value mentioned in equation 10 of the paper -  we will do this for both train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd6d722f-e67f-4355-a08b-639c9e1180d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e5a7c82-aed4-40d2-a3e4-6bf06cdf9e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fa2fb2f5-882e-4fa7-95fe-6a4662c11613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_keep[\"AUC\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f2d9822-cd2d-417c-a632-3c2821fc3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute r^2(x_i) for each bootstrap model\n",
    "def compute_r_squared(y_true, y_pred, model_variance):\n",
    "    residuals = (y_true - y_pred) ** 2 - model_variance\n",
    "    return np.maximum(residuals, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71373927-1f61-420f-a5ac-0ee57bddb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2_true_train = compute_r_squared(train_keep[\"AUC\"].values, train_bts_mean, train_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e013f824-2646-40f3-aef3-7545f965337e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7616,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6eded242-0eb1-48ef-a533-3aaa48f484e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00820237, 0.00362897, 0.00079942, ..., 0.07589278, 0.00014988,\n",
       "       0.01846294])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2347b2cc-a62d-47c6-acf1-51d1811e3a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there any zeros?\n",
    "np.min(r_2_true_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fcdc25da-e0c0-4401-97cd-900243997bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros\n",
    "num_zeros_train = np.count_nonzero(r_2_true_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56d2e010-bdb2-47d5-86ed-9069ee16d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3852"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0d67be6-7109-485b-b88d-a94dc83c6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is a lot of zeros, but let's role with this number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e1e404a-d97f-4b94-a4d8-e590116378a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about teh validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74ef3464-6cc5-4aa1-a3fd-cac346538a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_2_true_valid = compute_r_squared(valid_keep[\"AUC\"].values, valid_bts_mean, valid_bts_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5af42c92-f404-4b49-90ad-92fecec23fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_2_true_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39c97501-26a8-4b93-a3e5-a342a52d3f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there any zeros?\n",
    "np.min(r_2_true_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3200b39e-1421-42ae-8b4e-569a988b0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of zeros\n",
    "num_zeros_valid = np.count_nonzero(r_2_true_valid == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "466c9f2c-3cc2-4f7f-bb68-355522d91513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "026e8c3e-23b3-4cbc-9a39-99946e53f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm thinking, maybe for now, we should not worry too much about the custom loss function, and proceed with a regular loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "df203d28-4ffd-40ab-8b52-37e5eb9d297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dont think we need this r_2 for the test data, as we only need the predictions from the NNe of the test data set for computing the PIs for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83cb6543-c30f-4de2-b830-4bbfac3695e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function as described in equation 12\n",
    "def correct_custom_loss(r_true, r_pred):\n",
    "    # first term in equation 12\n",
    "    term_1 = tf.math.log(r_pred + 1)\n",
    "    # define the second term\n",
    "    term_2 = r_true/r_pred\n",
    "    # cost function\n",
    "    cost = 0.5 * tf.reduce_mean(term_1 + term_2)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f20de1eb-e5e0-4d93-8ab9-1cb87b74cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Define the noise variance estimation network (Phase II)\n",
    "# def create_noise_variance_nn(input_shape):\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(1, activation=tf.keras.activations.exponential)  # Output layer for variance prediction\n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce9eb6fc-4429-453d-b0a6-4febbb2766c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use the model above as we should be able to match all our features comin from the different omic types. Therefore we will use the same model that we used for training for training the NNe as well. The only difference will be, now instead of the original response y (AUC) we use the computed r_square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa7a6b61-c6d7-4068-b147-3b249f193e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's define the model as follows - with just one adjustment, make sure the last dense layer has an exponential activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba3187c5-b70b-4b5c-9e1d-cd327e3d1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "dropout1 = 0.10\n",
    "dropout2 = 0.20\n",
    "## get the model architecture\n",
    "def deepcdrgcn_NNe(dict_features, dict_adj_mat, samp_drug, samp_ach, cancer_dna_methy_model, cancer_gen_expr_model, cancer_gen_mut_model, training = training, dropout1 = dropout1, dropout2 = dropout2):\n",
    "    \n",
    "    input_gcn_features = tf.keras.layers.Input(shape = (dict_features[samp_drug].shape[0], 75))\n",
    "    input_norm_adj_mat = tf.keras.layers.Input(shape = (dict_adj_mat[samp_drug].shape[0], dict_adj_mat[samp_drug].shape[0]))\n",
    "    mult_1 = tf.keras.layers.Dot(1)([input_norm_adj_mat, input_gcn_features])\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(256, activation = \"relu\")\n",
    "    dense_out = dense_layer_gcn(mult_1)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "    mult_2 = tf.keras.layers.Dot(1)([input_norm_adj_mat, dense_out])\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(256, activation = \"relu\")\n",
    "    dense_out = dense_layer_gcn(mult_2)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "\n",
    "    dense_layer_gcn = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    mult_3 = tf.keras.layers.Dot(1)([input_norm_adj_mat, dense_out])\n",
    "    dense_out = dense_layer_gcn(mult_3)\n",
    "    dense_out = tf.keras.layers.BatchNormalization()(dense_out)\n",
    "    dense_out = tf.keras.layers.Dropout(dropout1)(dense_out, training = training)\n",
    "\n",
    "    dense_out = tf.keras.layers.GlobalAvgPool1D()(dense_out)\n",
    "    # All above code is for GCN for drugs\n",
    "\n",
    "    # methylation data\n",
    "    input_gen_methy1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_methy = cancer_dna_methy_model(input_gen_methy1)\n",
    "    input_gen_methy.trainable = False\n",
    "    gen_methy_layer = tf.keras.layers.Dense(256, activation = \"tanh\")\n",
    "    \n",
    "    gen_methy_emb = gen_methy_layer(input_gen_methy)\n",
    "    gen_methy_emb = tf.keras.layers.BatchNormalization()(gen_methy_emb)\n",
    "    gen_methy_emb = tf.keras.layers.Dropout(dropout1)(gen_methy_emb, training = training)\n",
    "    gen_methy_layer = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    gen_methy_emb = gen_methy_layer(gen_methy_emb)\n",
    "\n",
    "    # gene expression data\n",
    "    input_gen_expr1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_expr = cancer_gen_expr_model(input_gen_expr1)\n",
    "    input_gen_expr.trainable = False\n",
    "    gen_expr_layer = tf.keras.layers.Dense(256, activation = \"tanh\")\n",
    "    \n",
    "    gen_expr_emb = gen_expr_layer(input_gen_expr)\n",
    "    gen_expr_emb = tf.keras.layers.BatchNormalization()(gen_expr_emb)\n",
    "    gen_expr_emb = tf.keras.layers.Dropout(dropout1)(gen_expr_emb, training = training)\n",
    "    gen_expr_layer = tf.keras.layers.Dense(100, activation = \"relu\")\n",
    "    gen_expr_emb = gen_expr_layer(gen_expr_emb)\n",
    "    \n",
    "    \n",
    "    input_gen_mut1 = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "    input_gen_mut = cancer_gen_mut_model(input_gen_mut1)\n",
    "    input_gen_mut.trainable = False\n",
    "    \n",
    "    reshape_gen_mut = tf.keras.layers.Reshape((1, cancer_gen_mut_model(samp_ach).numpy().shape[0], 1))\n",
    "    reshape_gen_mut = reshape_gen_mut(input_gen_mut)\n",
    "    gen_mut_layer = tf.keras.layers.Conv2D(50, (1, 700), strides=5, activation = \"tanh\")\n",
    "    gen_mut_emb = gen_mut_layer(reshape_gen_mut)\n",
    "    pool_layer = tf.keras.layers.MaxPooling2D((1,5))\n",
    "    pool_out = pool_layer(gen_mut_emb)\n",
    "    gen_mut_layer = tf.keras.layers.Conv2D(30, (1, 5), strides=2, activation = \"relu\")\n",
    "    gen_mut_emb = gen_mut_layer(pool_out)\n",
    "    pool_layer = tf.keras.layers.MaxPooling2D((1,10))\n",
    "    pool_out = pool_layer(gen_mut_emb)\n",
    "    flatten_layer = tf.keras.layers.Flatten()\n",
    "    flatten_out = flatten_layer(pool_out)\n",
    "    x_mut = tf.keras.layers.Dense(100,activation = 'relu')(flatten_out)\n",
    "    x_mut = tf.keras.layers.Dropout(dropout1)(x_mut)\n",
    "    \n",
    "    all_omics = tf.keras.layers.Concatenate()([dense_out, gen_methy_emb, gen_expr_emb, x_mut])\n",
    "    x = tf.keras.layers.Dense(300,activation = 'tanh')(all_omics)\n",
    "    x = tf.keras.layers.Dropout(dropout1)(x, training = training)\n",
    "    x = tf.keras.layers.Lambda(lambda x: K.expand_dims(x,axis=-1))(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: K.expand_dims(x,axis=1))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=30, kernel_size=(1,150),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=10, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=5, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout1)(x, training = training)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout2)(x, training = training)\n",
    "    final_out_layer = tf.keras.layers.Dense(1, activation = tf.keras.activations.exponential)\n",
    "    final_out = final_out_layer(x)\n",
    "    simplecdr = tf.keras.models.Model([input_gcn_features, input_norm_adj_mat, input_gen_expr1,\n",
    "                                   input_gen_methy1, input_gen_mut1], final_out)\n",
    "    \n",
    "    return simplecdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "565c52d0-0100-445f-a072-3326c36be985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train function\n",
    "# def train_model_nne(X_train, y_train, bootstrap_predictions, model_variance, model):\n",
    "#     # Define custom loss inside model.compile using lambda\n",
    "    \n",
    "#     model.compile(optimizer='adam', \n",
    "#                   loss=lambda y_true, y_pred: correct_custom_loss(\n",
    "#                       y_true, y_pred))  # y_true corresponds to r_true, and y_pred corresponds to r_pred\n",
    "    \n",
    "#     bootstrap_mean_predictions = np.squeeze(np.mean(bootstrap_predictions, axis = 0))\n",
    "#     # The true residuals (r_true) can be computed outside and passed during training\n",
    "#     r_true = compute_r_squared(y_train, bootstrap_mean_predictions, model_variance)\n",
    "    \n",
    "#     # Fit the model with the true residuals\n",
    "#     model.fit(X_train, r_true, epochs=50, batch_size=32)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72e1202d-3356-4728-9cc2-784a981332f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's try and train this model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f38f83b-907d-4570-bedd-6b036b67c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "209ac81b-8740-4f20-860d-e06bf7790ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "dropout1 = 0.10\n",
    "dropout2 = 0.20\n",
    "NNE_model = deepcdrgcn_NNe(dict_features, dict_adj_mat, samp_drug, samp_ach, cancer_dna_methy_model, cancer_gen_expr_model, cancer_gen_mut_model,  training = training, dropout1 = dropout1, dropout2 = dropout2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f5e56159-f03e-4df8-9979-33fd564c8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 223, 223)]   0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 223, 75)]    0           []                               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 223, 75)      0           ['input_2[0][0]',                \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 223, 256)     19456       ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 223, 256)    1024        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 223, 256)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 223, 256)     0           ['input_2[0][0]',                \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 18739)        19226214    ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 223, 256)     65792       ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 18739, 1)  0           ['model_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 223, 256)    1024        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1, 3608, 50)  35050       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 223, 256)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 1, 721, 50)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 223, 256)     0           ['input_2[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " model_2 (Functional)           (None, 19606)        16194556    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 30805)        31082245    ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 1, 359, 30)   7530        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 223, 100)     25700       ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          5019392     ['model_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256)          7886336     ['model[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 1, 35, 30)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 223, 100)    400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256)         1024        ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1050)         0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 223, 100)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 100)          105100      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 100)         0           ['dropout_2[0][0]']              \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          25700       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 100)          25700       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 100)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 400)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'dense_4[0][0]',               \n",
      "                                                                  'dense_6[0][0]',                \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 300)          120300      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 300)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 300, 1)       0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1, 300, 1)    0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 1, 151, 30)   4530        ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 75, 30)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 1, 71, 10)    1510        ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 23, 10)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 1, 19, 5)     255         ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 6, 5)     0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 1, 6, 5)      0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 30)           0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 30)           0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            31          ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 79,849,893\n",
      "Trainable params: 13,344,630\n",
      "Non-trainable params: 66,505,263\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNE_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66309370-a831-4b3e-a3e4-14500a95fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the train and validation data generators, now with our output for the NNe and not the AUC value we have had earleir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56913f44-e1cc-4d53-a9a0-03f3ead5853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_NNe = DataGenerator(train_gcn_feats, train_adj_list, train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), train_keep[\"Cell_Line\"].values.reshape(-1,1), r_2_true_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ec27361-7c82-4cdd-aa85-2e06abe0a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen_NNe = DataGenerator(valid_gcn_feats, valid_adj_list, valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), valid_keep[\"Cell_Line\"].values.reshape(-1,1), r_2_true_valid, batch_size=32,  shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4b570723-3abf-434f-8fe2-05fc8b79861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "lr = 0.001\n",
    "NNE_model.compile(loss = tf.keras.losses.MeanSquaredError(), \n",
    "                      # optimizer = tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False), \n",
    "                    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0aeb4b11-56dc-4cfd-bed3-5b12d72f266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model   \n",
    "batch_size = 32\n",
    "generator_batch_size = 32\n",
    "epoch_num = 150\n",
    "patience_val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49cefa42-d77e-4e20-b2f7-c0c321ad85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNE_model.fit(train_gen_NNe, validation_data = val_gen_NNe, epochs = epoch_num, \n",
    "#          callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_val, restore_best_weights=True, \n",
    "#                                                      mode = \"min\") ,validation_batch_size = generator_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "78779e36-58d2-4bf2-9e3b-4496831de1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might need to copy and paste all these in a python script as we run into a graph execution error - Also we are currently rolling with the MSE loss -  we will adjust the loss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "49089018-0494-4086-9ea1-eb83ce4ed977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we can now work on the custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ae7f845d-2168-46a4-92de-0ee5bab22f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we do the custom loss function? We have done this earlier, so we will refer that code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eaf0a708-1052-4434-8938-4ee933ce09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function as described in equation 12\n",
    "def correct_custom_loss(r_true, r_pred):\n",
    "    # first term in equation 12\n",
    "    term_1 = tf.math.log(r_pred + 1)\n",
    "    # define the second term\n",
    "    term_2 = r_true/r_pred\n",
    "    # cost function\n",
    "    cost = 0.5 * tf.reduce_mean(term_1 + term_2)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96ea17-8724-4d7a-b596-3065958a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, need to include this in the model compile, inorder to use this as the loss when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4e36aecf-fa80-47f2-b456-ef8bee4e3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model, and use the new custom loss function\n",
    "NNE_model.compile(loss = lambda y_true, y_pred: correct_custom_loss(\n",
    "                      y_true, y_pred), \n",
    "                    optimizer = tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "                    metrics = [tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e343edbe-c763-414f-a5c5-f57a92212470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we use this model next in fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5508754d-05cb-4236-9a56-08f81b30d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNE_model.fit(train_gen_NNe, validation_data = val_gen_NNe, epochs = epoch_num, \n",
    "#          callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_val, restore_best_weights=True, \n",
    "#                                                      mode = \"min\") ,validation_batch_size = generator_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b57c03-4dd1-4a0a-85e2-e511d71ac74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, moving on to getting the PIs - for this we need the preds for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39f4897-e6b8-443b-aa36-ca8a4f211c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, what do we need for the test data to compute the PIs? we need the preds from all the bootstrap models, and their means and variances. And now using the trained NNe model, we need the predicted error variance for the test data inputs. Once we have all those, we can compute the coverages and widths of the PIs using bootstraps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04677a9f-1aef-4caa-9d4d-6e674e3da9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting these quantities is pretty straight forward, we can edit the code above to capture the bootstrap preds for the test set for all bootstrap models. Once we have those, we can get the means and the variance as the first step. Then using the trained NNe model, we can try and get the predicted error variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56547e1b-197d-4a0b-999c-85f9f1d32256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted variance from NNE for the test dataset here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf3a52b2-f4c4-4d72-89f4-6ebac4905258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNe_model = tf.keras.models.load_model(\"NNe_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84ce78-1011-4395-bffe-1eec521078f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53a76f-0934-4d5e-ad8a-38fa21532246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a2642-5137-4211-a3e4-02f4c29464c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c71457-1592-4a52-bdd9-e9690494b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed0fca-bd8e-4387-b1fe-95c9d975f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804fdf4-274e-4449-a5fe-a14414487de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a3db7-db52-46bd-8576-ec65e665f320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45fbf9-ce0d-4244-8b14-2b3921a59cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcdr_improve_env)",
   "language": "python",
   "name": "deepcdr_improve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
