{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c865078-1597-4bd6-a9cb-62a2696b7b31",
   "metadata": {},
   "source": [
    "##### Training DeepCDR with the new data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66130d-f335-4223-b3d9-4626c0e3a695",
   "metadata": {},
   "source": [
    "So far we have written down a new generator function with tf.keras.Sequence, and we have trained the DeepCDR model with this function. We however still need to write an improve-compliant code if we are to share the work with people from ANL. Therefore, let's try doing that with the new generator function for both the train and inference scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521ce92-598f-48f4-bcb9-3e91e43c078d",
   "metadata": {},
   "source": [
    "The generator function is available in the py script \"New_data_generator_with_tf.py\", and the train script with this function is available at \"DeepCDR_train_with_new_generator.py\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed4262-d791-4105-89cd-fe189f2bcd2f",
   "metadata": {},
   "source": [
    "Okay, we have made the two scripts improve compliant: The codes are stored as; train script - \"deepcdr_train_improve_with_new_generator.py\" and infer script - \"deepcdr_infer_improve_with_new_data_generator.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa980b85-d6da-4e9f-bdb5-ed6e52e571fc",
   "metadata": {},
   "source": [
    "One more thing to do here before we get to the bootstrapping method and storing the predictions. See if the train and validation steps are necessary, or if the written generator will automatically complete training at the end of the required number of batches in each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d349777-7d48-43c8-8f09-084c391a052f",
   "metadata": {},
   "source": [
    "#### Bootstrap CIs for DeepCDR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7880d98-757c-4a40-9bb5-d44483144e80",
   "metadata": {},
   "source": [
    "##### Training bootsrap samples for DeepCDR for UQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68dd15-41a0-43ef-b0d8-2cfae8113ca2",
   "metadata": {},
   "source": [
    "Okay, we can now move to bootstrap sample training. We should adjust our code in a way that the trained model and the validation scores from the exercise can be stored, so that later we can use these models to compute the CIs and talk about uncertainity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acca300-9f76-493f-904d-eaff7cb8fa04",
   "metadata": {},
   "source": [
    "Let's use the bootstrap generator, and compare it against the original generator, to make sure the bootstrap sampling generator is in fact doing what it is supposed to do. This comparison is done in the 'Examine_bootstrap_samples.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552031cb-0434-41d5-957d-efa886fd081e",
   "metadata": {},
   "source": [
    "We ensured that bootstrap samples indeed give different results than the regular training batches, so I think we are finally ready to implement the bootstrap training for DeepCDR for the UQ project - To get the predictions for the rrain data for the different botstrap samples, and then train the NNe, and then get the CIs, and thereby talk about the width and the coverages of the confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1a3a60-4543-400d-b8a3-b6a91a6aa060",
   "metadata": {},
   "source": [
    "Notice that, we add the bootstrap data generator class to the python script \"New_data_generator_with_tf.py\" so that it would seem less complex to someone going through the codes (else too many python scripts, eventhough they are just simple fucntion r class definitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7c573-a786-431a-9379-c9b617418820",
   "metadata": {},
   "source": [
    "The bootstrap train code is in the python script \"DeepCDR_train_with_new_generator_bootstraps.py\". This is a generic code, and is still not the improve-compliant one. This code defines the validation data generator (that is created by the regular generator construct and not the bootstrap), and the folder to store the bootstrap results outside the for loop for training the bootstrap models. Inside the for loop, it first defines the model, compile the model, train the model, save the model, evaluate the model, and also stores the validation predictions and the associated scores. However, we cannot store the predictions or the scores for the train data for the bootstrap work due to an assertion error that we get in the improve library (it basically checks if the train order of the predicted values is the same, but since in bootstraps we have some samples repeated, we will not be able to use the improve methods to store the predictions for the train data. Therefore, we will do this step in the next model train - NNe work)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915260d0-a2da-4d8c-8162-207c23754adf",
   "metadata": {},
   "source": [
    "Our next step will be to write improve-compliant code for training the bootrap models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e7866-51ac-43b9-a826-0d7ee567d004",
   "metadata": {},
   "source": [
    "The improve-compliant bootstrap training of the DeepCDR model is in the py script \"deepcdr_train_bootstrap_improve_with_new_generator.py\". Note that the build model path [Req] in the original code is now defined inside the for loop. We still need to make sure that this code works, and then we should be good to get started with developing the code for the model NNe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1a5bd-404b-4b83-a34d-5bb7d3809039",
   "metadata": {},
   "source": [
    "##### Training NNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f088b3-2bbb-4102-bc41-410771f1bdf7",
   "metadata": {},
   "source": [
    "For this, we need the predictions from the trained model, and the respective quantities computed. The model we can use for the training will have to be the same model we used for DeepCDR model, as we have several sets of inputs for the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e83220-4dcd-4f7c-a0a0-f507bcb52001",
   "metadata": {},
   "source": [
    "We will initially develop some work in the ipynb \"Train_NNe_and_PIs.ipynb\", and later put all the codes together, and also write an improve-compliant code later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d51f7-61a4-47db-ab7c-b85f14b04d6e",
   "metadata": {},
   "source": [
    "\"Train_NNe_and_PIs.py\" has the end to end code for training the NNe, and the trained NNe model is saved in the folder \"NNe_model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186db69-8d9a-4ecf-95a1-6f9d08bffeda",
   "metadata": {},
   "source": [
    "We still need to make this train script improve-compliant, if we are to sahre with them, and mark what all changes to be made - for both main train scripts of the models, and train script of the NNe model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e2a1c-8567-4590-8daf-a0bef8974853",
   "metadata": {},
   "source": [
    "##### Getting the PIs for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88fdff-3161-43bb-812b-21429ded85db",
   "metadata": {},
   "source": [
    "We have done this in the same python scripts as above, the Train_NNe_and_PIs.py script and also the rough work in the corresponding notebook. And I think we can use this script as the infer script for improve, we just have to make it improve-compliant now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31630097-1b8b-4271-8701-de3d284463b7",
   "metadata": {},
   "source": [
    "The improve compliant code is written in \"deepcdr_infer_bootstrap_imporve_with_new_generator.py\" script. Note that this also stores the coverage and width in a json file in the output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b977c-cdb2-4757-a295-148a6e19867d",
   "metadata": {},
   "source": [
    "Okay, so training and infer is there, what abobut the preprocessing part? I think we never made changes in there, so wew use the same old script?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca27531-c23d-4113-b19c-1e842b384dba",
   "metadata": {},
   "source": [
    "##### Steps for running the py scripts on the command line for getting the bootstrap predictions and the corresponding predicton intervals aong with their coverages and widths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfafbef-735d-4e1c-8cef-9d0d5b79b82a",
   "metadata": {},
   "source": [
    "1. For the preprocessing script: python deepcdr_preprocess_improve.py --input_dir ./csa_data/raw_data --output_dir exp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b100cf-0497-4fbf-9253-014cd10ecd93",
   "metadata": {},
   "source": [
    "This will store all preprocessed data in the location(folder) exp_result. The contents here at this point will be:\n",
    "\n",
    "exp_result\n",
    "\n",
    " ├── param_log_file.txt\n",
    " \n",
    " ├── cancer_dna_methy_model\n",
    " \n",
    " ├── cancer_gen_expr_model\n",
    " \n",
    " ├── cancer_gen_mut_model\n",
    " \n",
    " ├── test_y_data.csv\n",
    " \n",
    " ├── train_y_data.csv\n",
    " \n",
    " ├── val_y_data.csv\n",
    " \n",
    " ├── drug_features.pickle\n",
    " \n",
    " └── norm_adj_mat.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce47ff-38fd-4a73-b23a-fb10e421b93a",
   "metadata": {},
   "source": [
    "2. For the train script: python deepcdr_train_bootstrap_improve_with_new_generator.py --input_dir exp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5fd31-7fba-4a57-9707-a6ccc145403e",
   "metadata": {},
   "source": [
    "The contents of this folder will look like below.\n",
    "\n",
    "bootstrap_results_all\n",
    "\n",
    " ├── bootstrap_1\n",
    " \n",
    "      └── DeepCDR_model\n",
    "\n",
    "      └── val_scores.json\n",
    "    \n",
    "      └── val_y_data_predicted.csv\n",
    " \n",
    " ├── bootstrap_2\n",
    " \n",
    "      └── DeepCDR_model\n",
    "\n",
    "      └── val_scores.json\n",
    "\n",
    "      └── val_y_data_predicted.csv\n",
    " \n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    " \n",
    " ├── bootstrap_10\n",
    " \n",
    "      └── DeepCDR_model\n",
    "\n",
    "      └── val_scores.json\n",
    "\n",
    "      └── val_y_data_predicted.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4378d13-ef6e-4373-844c-a2259048140a",
   "metadata": {},
   "source": [
    "3. python deepcdr_infer_bootstrap_improve_with_new_generator.py --input_data_dir exp_result --input_model_dir bootstrap_results_all --output_dir bootstrap_inference --calc_infer_score true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1c0b3-4e89-4ece-8465-ee7884cac0e0",
   "metadata": {},
   "source": [
    "The contents from this step will be as follows\n",
    "\n",
    "bootstrap_results_all\n",
    "\n",
    "    └── param_log_file.txt\n",
    "\n",
    "    └── test_scores.json\n",
    "    \n",
    "    └── CI_information_bootstraps.json\n",
    "\n",
    "    └── test_y_data_predicted.csv\n",
    "\n",
    "NNe_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ffb67-43a8-44f9-8aed-d53aa85c9e84",
   "metadata": {},
   "source": [
    "We also need a submit file for execution of the train and infer functions of very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d18882-0393-4f3d-814d-47ca2aa692c6",
   "metadata": {},
   "source": [
    "For the large datasets, it takes so much time to train and also it requires a lot of memory, so we must submit them with a higher RAM as a job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cd68d-4d17-49ba-a85a-49ae73a341c4",
   "metadata": {},
   "source": [
    "#### Dropout induced CIs for DeepCDR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca9ce8-cb12-422d-99bc-e7387ffacbdf",
   "metadata": {},
   "source": [
    "For this work, we do nobt have to change the preprocess and the train script. We only need to adjust the inference script - should not be too bad. However, we still want to do the generator in the train script with the new generator we defined with the tf.keras.utils.Sequence class stored in the py script \"New_data_generator_with_tf\" (DataGenerator class for this to be exact)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dfdd5-f839-4c49-a76e-41f7c1637dbe",
   "metadata": {},
   "source": [
    "Let's just try it for the CCLE dataset maybe -  we know that this method is not the best to do the CI analysis, so we should be okay just doing it for one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b3b87-1883-4861-8bdb-94f58e336340",
   "metadata": {},
   "source": [
    "We do have the preprocessed data for this CCLE split 0 in the directories about CCLE, but let's store this work separately? - So that we have clarity when we get back here. Let's call this folder mc_dropout_exp_results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10d386-f64b-4b6d-80ea-ff7e37dfd302",
   "metadata": {},
   "source": [
    "So no changes to the preprocess and the train scripts. We do have to adjust the test script to make it improve compliant. Infer_with_uq_exp_CCLE.ipynb notebook has the complete inference code, let's first take it on to a python script and later make it improve-compliant. The corresponding python script is Infer_with_uq_exp_CCLE.py in the folder. We can now work on making it improve-compliant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11103d45-a848-4a81-9c6c-31e6b7765f0d",
   "metadata": {},
   "source": [
    "The improve compliant inference script is named \"deepcdr_infer_mcdropout_improve.py\". Note that this file also stores the width and the coverage information in a json file that get stored into the mc_dropout_exp_results folder. The lines below can be used to run the work on a terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d10d8a-37d7-4989-83f4-1bc6033c83b5",
   "metadata": {},
   "source": [
    "Running code on HCC - go to a terminal, go inside the location where the scripts are stored.\n",
    "\n",
    "on terminal:\n",
    "\n",
    "module load anaconda\n",
    "\n",
    "conda activate $COMMON/deepcdr_improve_env\n",
    "\n",
    "source setup_improve.sh\n",
    "\n",
    "\n",
    "1. Run the preprocess script - no changes to the script\n",
    "\n",
    "python deepcdr_preprocess_improve.py --input_dir ./csa_data/raw_data --output_dir mc_dropout_exp_results\n",
    "\n",
    "2. Run the train script - just change where teh generator functions are coming from - old train script with yield generator should work fine as well, but let's for now go with the new generator function - this is the same exact script as deepcdr_train_improve_with_new_generator.py. We named it deepcdr_train_mcdropout_improve.py\n",
    "\n",
    "python deepcdr_train_mcdropout_improve.py --input_dir mc_dropout_exp_results --output_dir mc_dropout_exp_results\n",
    "\n",
    "Running the above two scripts should be a no-brainer as there aren't any adjustments to be done\n",
    "\n",
    "3. What about the inference script?\n",
    "\n",
    "\n",
    "python deepcdr_infer_mcdropout_improve.py --input_data_dir mc_dropout_exp_results --input_model_dir mc_dropout_exp_results --output_dir mc_dropout_exp_results --calc_infer_score true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd73125-11e9-4605-9f1c-98dec0537898",
   "metadata": {},
   "source": [
    "The files that get saved in these steps are smilar to the ones we get in our original work. We also get an additional json file that stores the CI information (coverage and width) in the output directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcdr_improve_env)",
   "language": "python",
   "name": "deepcdr_improve_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
